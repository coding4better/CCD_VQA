{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8976a26",
   "metadata": {},
   "source": [
    "# åäº‹å®å¤šé€‰é¡¹ç”Ÿæˆç³»ç»Ÿ\n",
    "## Counterfactual Multiple-Choice Options Generator for Accident Videos\n",
    "\n",
    "ç”¨äºä»äº‹æ•…è§†é¢‘å’ŒQAå¯¹ç”Ÿæˆé«˜è´¨é‡çš„åäº‹å®é€‰é¡¹ï¼ˆé”™è¯¯é€‰é¡¹ï¼‰ï¼Œæ„å»ºæ ‡å‡†åŒ–çš„å¤šé€‰é¢˜ã€‚\n",
    "\n",
    "**å…³é”®ç‰¹æ€§:**\n",
    "- è°ƒç”¨ Qwen VL API ç”Ÿæˆåäº‹å®é€‰é¡¹\n",
    "- è‡ªåŠ¨è®¡ç®—ä¸æ­£ç¡®ç­”æ¡ˆçš„ç›¸ä¼¼åº¦\n",
    "- æ”¯æŒå¯å˜é€‰é¡¹æ•°ï¼ˆ3, 4, 5ï¼‰ç”¨äºæ¶ˆèå®éªŒ\n",
    "- è¾“å‡ºç›¸ä¼¼åº¦ç»Ÿè®¡å’Œè´¨é‡è¯„åˆ†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84495ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# éƒ¨åˆ† 1: å®‰è£…ä¾èµ–å’Œå¯¼å…¥\n",
    "# ============================================================================\n",
    "\n",
    "# å®‰è£…å¿…éœ€çš„åŒ…\n",
    "%pip install -q pandas numpy scikit-learn matplotlib seaborn tqdm openai torch torchvision\n",
    "%pip install -q nltk python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ab3d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥åº“\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ç›¸ä¼¼åº¦è®¡ç®—\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# è¿›åº¦æ¡\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ç»˜å›¾\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"âœ… æ‰€æœ‰ä¾èµ–å¯¼å…¥æˆåŠŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6652723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# éƒ¨åˆ† 2: é…ç½®å’Œå¸¸é‡\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"å…¨å±€é…ç½®\"\"\"\n",
    "    # API é…ç½®\n",
    "    qwen_api_key: str = None\n",
    "    qwen_api_url: str = \"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\"\n",
    "    \n",
    "    # æ–‡ä»¶è·¯å¾„\n",
    "    qa_csv_path: str = \"/home/24068286g/UString/VRU/src/option_generate/data/QA_pair.csv\"\n",
    "    video_dir: str = \"/home/24068286g/UString/data/crash/videos/Crash-1500\"\n",
    "    output_dir: str = \"/home/24068286g/UString/VRU/src/option_generate/outputs\"\n",
    "    \n",
    "    # å¤„ç†é…ç½®\n",
    "    num_options_list: List[int] = None  # [3, 4, 5] ç”¨äºæ¶ˆèå®éªŒ\n",
    "    max_frames_per_video: int = 8\n",
    "    target_fps: int = 2\n",
    "    \n",
    "    # ç›¸ä¼¼åº¦é…ç½®\n",
    "    min_similarity: float = 0.2  # é€‰é¡¹ä¸åº”è¿‡äºç›¸ä¼¼\n",
    "    max_similarity: float = 0.8  # é€‰é¡¹ä¸åº”è¿‡äºç›¸ä¼¼\n",
    "    similarity_threshold: float = 0.5  # ç”¨äºåˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆé€‰é¡¹\n",
    "    \n",
    "    # API å‚æ•°\n",
    "    api_timeout: int = 60\n",
    "    max_retries: int = 3\n",
    "    batch_size: int = 1\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # ä» Colab Secrets åŠ è½½ API Key\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            if self.qwen_api_key is None:\n",
    "                self.qwen_api_key = userdata.get('ALI_INTERNATIONAL_KEY')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # ä»ç¯å¢ƒå˜é‡åŠ è½½\n",
    "        if self.qwen_api_key is None:\n",
    "            self.qwen_api_key = os.getenv('ALI_INTERNATIONAL_KEY')\n",
    "        \n",
    "        # åˆå§‹åŒ–é»˜è®¤é€‰é¡¹æ•°\n",
    "        if self.num_options_list is None:\n",
    "            self.num_options_list = [3, 4, 5]\n",
    "        \n",
    "        # åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "        Path(self.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# åˆ›å»ºå…¨å±€é…ç½®å®ä¾‹\n",
    "config = Config()\n",
    "\n",
    "# éªŒè¯é…ç½®\n",
    "print(\"ğŸ”§ ç³»ç»Ÿé…ç½®\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"âœ“ QA CSV: {config.qa_csv_path}\")\n",
    "print(f\"âœ“ è§†é¢‘ç›®å½•: {config.video_dir}\")\n",
    "print(f\"âœ“ è¾“å‡ºç›®å½•: {config.output_dir}\")\n",
    "print(f\"âœ“ API Key: {'âœ… å·²åŠ è½½' if config.qwen_api_key else 'âŒ æœªåŠ è½½'}\")\n",
    "print(f\"âœ“ é€‰é¡¹æ•°é‡: {config.num_options_list}\")\n",
    "print(f\"âœ“ ç›¸ä¼¼åº¦èŒƒå›´: [{config.min_similarity}, {config.max_similarity}]\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93cc2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# éƒ¨åˆ† 3: åŠ è½½ QA æ•°æ®\n",
    "# ============================================================================\n",
    "\n",
    "def load_qa_data(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    åŠ è½½ QA_pair.csv æ–‡ä»¶\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“‚ åŠ è½½ QA æ•°æ®: {csv_path}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"  âœ“ æˆåŠŸåŠ è½½ {len(df)} ä¸ªè§†é¢‘çš„ QA å¯¹\")\n",
    "        print(f\"  âœ“ åˆ—æ•°: {len(df.columns)}\")\n",
    "        print(f\"\\n  å­—æ®µç¤ºä¾‹: {df.columns[:10].tolist()}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ åŠ è½½å¤±è´¥: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_qa_pairs(df: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    ä» DataFrame ä¸­æå– QA å¯¹\n",
    "    è¿”å›æ ¼å¼: [\n",
    "        {\n",
    "            \"video_id\": \"000003\",\n",
    "            \"questions\": [\n",
    "                {\n",
    "                    \"q_id\": 1,\n",
    "                    \"text\": \"é—®é¢˜æ–‡æœ¬\",\n",
    "                    \"category\": \"åˆ†ç±»\",\n",
    "                    \"correct_answer\": \"æ­£ç¡®ç­”æ¡ˆ\",\n",
    "                    \"wrong_options\": [\"é”™é€‰1\", \"é”™é€‰2\", \"é”™é€‰3\"]\n",
    "                },\n",
    "                ...\n",
    "            ]\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    qa_data = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        video_id = str(row.get('video_number', idx)).zfill(6)\n",
    "        \n",
    "        # æå–é—®é¢˜ (q1, q2, q3, q4, q5, q6)\n",
    "        questions = []\n",
    "        for q_num in range(1, 7):\n",
    "            q_col = f'q{q_num}_text'\n",
    "            ans_col = f'q{q_num}_ans_correct'\n",
    "            cat_col = f'q{q_num}_category'\n",
    "            \n",
    "            if q_col in df.columns and pd.notna(row.get(q_col)):\n",
    "                question = {\n",
    "                    'q_id': q_num,\n",
    "                    'text': str(row[q_col]).strip(),\n",
    "                    'category': str(row.get(cat_col, 'unknown')).strip(),\n",
    "                    'correct_answer': str(row.get(ans_col, '')).strip(),\n",
    "                    'wrong_options': []\n",
    "                }\n",
    "                \n",
    "                # æ”¶é›†é”™è¯¯é€‰é¡¹\n",
    "                for w_num in range(1, 4):\n",
    "                    wrong_col = f'q{q_num}_ans_wrong{w_num}'\n",
    "                    if wrong_col in df.columns and pd.notna(row.get(wrong_col)):\n",
    "                        wrong_ans = str(row[wrong_col]).strip()\n",
    "                        if wrong_ans and wrong_ans.lower() != 'nan':\n",
    "                            question['wrong_options'].append(wrong_ans)\n",
    "                \n",
    "                if question['text'] and question['correct_answer']:\n",
    "                    questions.append(question)\n",
    "        \n",
    "        if questions:\n",
    "            qa_data.append({\n",
    "                'video_id': video_id,\n",
    "                'questions': questions\n",
    "            })\n",
    "    \n",
    "    return qa_data\n",
    "\n",
    "# åŠ è½½æ•°æ®\n",
    "print(\"\\nğŸ“‹ åŠ è½½ QA æ•°æ®é›†\")\n",
    "print(\"=\" * 70)\n",
    "qa_df = load_qa_data(config.qa_csv_path)\n",
    "\n",
    "if qa_df is not None:\n",
    "    qa_pairs = extract_qa_pairs(qa_df)\n",
    "    print(f\"\\nâœ… æˆåŠŸæå– {len(qa_pairs)} ä¸ªè§†é¢‘çš„ QA å¯¹\")\n",
    "    print(f\"   æ€»é—®é¢˜æ•°: {sum(len(v['questions']) for v in qa_pairs)}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºæ ·ä¾‹\n",
    "    if qa_pairs:\n",
    "        print(f\"\\nğŸ“Œ ç¤ºä¾‹ (è§†é¢‘ {qa_pairs[0]['video_id']})\")\n",
    "        for q in qa_pairs[0]['questions'][:1]:\n",
    "            print(f\"  é—®: {q['text'][:60]}...\")\n",
    "            print(f\"  ç­”: {q['correct_answer'][:60]}...\")\n",
    "            print(f\"  åˆ†ç±»: {q['category']}\")\n",
    "else:\n",
    "    qa_pairs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8404aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# éƒ¨åˆ† 4: è§†é¢‘å¸§åŠ è½½\n",
    "# ============================================================================\n",
    "\n",
    "def load_video_frames(video_path: str, max_frames: int = 8, target_fps: int = 2) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    åŠ è½½è§†é¢‘å¸§ (å‡åŒ€é‡‡æ ·)\n",
    "    è¿”å›: (num_frames, H, W, 3) çš„ numpy æ•°ç»„æˆ– None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            print(f\"  âŒ æ— æ³•æ‰“å¼€: {Path(video_path).name}\")\n",
    "            return None\n",
    "        \n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if fps <= 0 or total_frames <= 0:\n",
    "            cap.release()\n",
    "            return None\n",
    "        \n",
    "        # è®¡ç®—é‡‡æ ·é—´éš”\n",
    "        frame_interval = max(1, int(fps / target_fps))\n",
    "        frames = []\n",
    "        frame_count = 0\n",
    "        \n",
    "        while cap.isOpened() and len(frames) < max_frames:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            if frame_count % frame_interval == 0:\n",
    "                try:\n",
    "                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    frames.append(frame_rgb)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            frame_count += 1\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        if frames:\n",
    "            return np.array(frames, dtype=np.uint8)\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ åŠ è½½è§†é¢‘å‡ºé”™: {e}\")\n",
    "        return None\n",
    "\n",
    "def find_video_file(video_id: str, video_dir: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    æ ¹æ®è§†é¢‘ ID æŸ¥æ‰¾è§†é¢‘æ–‡ä»¶\n",
    "    \"\"\"\n",
    "    video_dir = Path(video_dir)\n",
    "    \n",
    "    # å°è¯•å¤šç§æ‰©å±•å\n",
    "    for ext in ['.mp4', '.avi', '.mov', '.mkv']:\n",
    "        video_path = video_dir / f\"{video_id}{ext}\"\n",
    "        if video_path.exists():\n",
    "            return str(video_path)\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"âœ… è§†é¢‘åŠ è½½å‡½æ•°å®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd3c497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# éƒ¨åˆ† 5: æç¤ºè¯æ¨¡æ¿\n",
    "# ============================================================================\n",
    "\n",
    "def create_option_generation_prompt(\n",
    "    question: str,\n",
    "    correct_answer: str,\n",
    "    num_options: int,\n",
    "    video_context: str = \"\",\n",
    "    question_category: str = \"\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    åˆ›å»ºç”¨äºç”Ÿæˆåäº‹å®é€‰é¡¹çš„æç¤ºè¯\n",
    "    \n",
    "    å…³é”®ç­–ç•¥:\n",
    "    1. è¦æ±‚é€‰é¡¹ä¸æ­£ç¡®ç­”æ¡ˆè¯­ä¹‰ç›¸å…³ä½†ä¸åŒ\n",
    "    2. è¦æ±‚é€‰é¡¹ä¸é—®é¢˜ä¸Šä¸‹æ–‡ä¸€è‡´\n",
    "    3. è¦æ±‚é€‰é¡¹å…·æœ‰åˆç†çš„éš¾åº¦\n",
    "    4. é¿å…ç”Ÿæˆè¿‡äºæ˜æ˜¾çš„é”™è¯¯é€‰é¡¹\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"ä½ æ˜¯ä¸€ä¸ªå¤šé€‰é¢˜å‡ºé¢˜ä¸“å®¶ï¼Œä¸“é—¨ä¸ºè§†é¢‘ç†è§£ä»»åŠ¡ç”Ÿæˆé«˜è´¨é‡çš„é”™è¯¯é€‰é¡¹ï¼ˆå¹²æ‰°é¡¹ï¼‰ã€‚\n",
    "\n",
    "ã€ä»»åŠ¡ã€‘ä¸ºä»¥ä¸‹é—®é¢˜ç”Ÿæˆ {num_options - 1} ä¸ªé«˜è´¨é‡çš„é”™è¯¯é€‰é¡¹ã€‚\n",
    "\n",
    "ã€é—®é¢˜ã€‘\n",
    "{question}\n",
    "\n",
    "ã€æ­£ç¡®ç­”æ¡ˆã€‘\n",
    "{correct_answer}\n",
    "\n",
    "ã€è§†é¢‘èƒŒæ™¯ã€‘ï¼ˆå¦‚æœæœ‰ï¼‰\n",
    "{video_context if video_context else \"äº‹æ•…é©¾é©¶è§†é¢‘\"}\n",
    "\n",
    "ã€é—®é¢˜åˆ†ç±»ã€‘\n",
    "{question_category if question_category else \"é€šç”¨\"}\n",
    "\n",
    "ã€è¦æ±‚ã€‘\n",
    "\n",
    "1. **ç›¸ä¼¼åº¦å¹³è¡¡**ï¼š\n",
    "   - é”™è¯¯é€‰é¡¹åº”è¯¥ä¸æ­£ç¡®ç­”æ¡ˆåœ¨è¯­ä¹‰ä¸Šç›¸å…³ï¼ˆæ¶‰åŠç›¸åŒçš„ä¸»é¢˜æˆ–æ¦‚å¿µï¼‰\n",
    "   - ä½†è¡¨è¿°å’Œå†…å®¹åº”è¯¥æœ‰æ˜æ˜¾åŒºåˆ«ï¼ˆä¸èƒ½è¿‡äºæ¥è¿‘ï¼‰\n",
    "   - ä¾‹å¦‚ï¼šå¦‚æœæ­£ç¡®ç­”æ¡ˆæ¶‰åŠ\"Tå‹ç¢°æ’\"ï¼Œé€‰é¡¹å¯ä»¥æ¶‰åŠ\"ä¾§æ»‘ç¢°æ’\"ã€\"è¿½å°¾ç¢°æ’\"ç­‰\n",
    "\n",
    "2. **ä¸é—®é¢˜çš„ä¸€è‡´æ€§**ï¼š\n",
    "   - æ‰€æœ‰é€‰é¡¹ï¼ˆåŒ…æ‹¬é”™è¯¯é€‰é¡¹ï¼‰éƒ½åº”è¯¥ç›´æ¥å›ç­”é—®é¢˜\n",
    "   - é€‰é¡¹åº”è¯¥ç¬¦åˆé—®é¢˜çš„è¯­å¢ƒå’Œä¸Šä¸‹æ–‡\n",
    "   - é¿å…ç”Ÿæˆæ— å…³çš„æˆ–è¯­æ³•é”™è¯¯çš„é€‰é¡¹\n",
    "\n",
    "3. **åˆç†çš„éš¾åº¦**ï¼š\n",
    "   - é€‰é¡¹åº”è¯¥å¯¹å­¦ç”Ÿæœ‰ä¸€å®šçš„è¿·æƒ‘æ€§ï¼Œä½†ä¸èƒ½æ˜¯è’è°¬çš„\n",
    "   - åº”è¯¥åæ˜ å¸¸è§çš„é”™è¯¯ç†è§£æˆ–æ··æ·†ç‚¹\n",
    "   - é¿å…è¿‡äºç›¸ä¼¼æˆ–è¿‡äºå·®å¼‚\n",
    "\n",
    "4. **æ ¼å¼è¦æ±‚**ï¼š\n",
    "   - ä½¿ç”¨æ¸…æ™°ã€ç®€æ´çš„è¯­è¨€\n",
    "   - é•¿åº¦ä¸æ­£ç¡®ç­”æ¡ˆç›¸è¿‘ï¼ˆÂ±20%ï¼‰\n",
    "   - é¿å…ä½¿ç”¨æç«¯è¯æ±‡æˆ–è¿‡åº¦ä¿®é¥°\n",
    "\n",
    "ã€ç”ŸæˆæŒ‡å—ã€‘\n",
    "\n",
    "æ ¹æ®ä»¥ä¸‹ç­–ç•¥ç”Ÿæˆ {num_options - 1} ä¸ªé”™è¯¯é€‰é¡¹ï¼š\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    if num_options == 3:\n",
    "        prompt += \"\"\"é€‰é¡¹1ï¼šä¸»è¦é”™è¯¯è§’åº¦ï¼ˆä¸æ­£ç¡®ç­”æ¡ˆæ¶‰åŠä¸åŒçš„åœºæ™¯å…ƒç´ ï¼‰\n",
    "é€‰é¡¹2ï¼šå¸¸è§æ··æ·†ç‚¹ï¼ˆå­¦ç”Ÿå®¹æ˜“æ··æ·†çš„ç›¸å…³æ¦‚å¿µï¼‰\n",
    "\"\"\"\n",
    "    elif num_options == 4:\n",
    "        prompt += \"\"\"é€‰é¡¹1ï¼šç›¸ä¼¼ä½†ä¸åŒçš„åœºæ™¯ï¼ˆæ¶‰åŠç±»ä¼¼ç±»å‹ä½†ç»†èŠ‚ä¸åŒçš„ç¢°æ’/äº‹ä»¶ï¼‰\n",
    "é€‰é¡¹2ï¼šå¸¸è§æ··æ·†ç‚¹ï¼ˆå­¦ç”Ÿå®¹æ˜“æ··æ·†çš„æ¦‚å¿µï¼‰\n",
    "é€‰é¡¹3ï¼šå±€éƒ¨æ­£ç¡®ä½†æ•´ä½“é”™è¯¯ï¼ˆç­”æ¡ˆä¸­æŸäº›éƒ¨åˆ†æ­£ç¡®ï¼Œä½†æ•´ä½“ç†è§£æœ‰è¯¯ï¼‰\n",
    "\"\"\"\n",
    "    elif num_options == 5:\n",
    "        prompt += \"\"\"é€‰é¡¹1ï¼šç›¸ä¼¼ç±»å‹çš„ç¢°æ’/äº‹ä»¶ï¼ˆä¸åŒäºæ­£ç¡®ç­”æ¡ˆä½†ç›¸å…³ï¼‰\n",
    "é€‰é¡¹2ï¼šé¢ å€’å› æœå…³ç³»ï¼ˆæ··æ·†åŸå› å’Œç»“æœï¼‰\n",
    "é€‰é¡¹3ï¼šéƒ¨åˆ†æ­£ç¡®çš„ç­”æ¡ˆï¼ˆæ··åˆæ­£ç¡®å’Œé”™è¯¯ä¿¡æ¯ï¼‰\n",
    "é€‰é¡¹4ï¼šå¸¸è§å­¦ç”Ÿé”™è¯¯ï¼ˆåŸºäºå®é™…æ•™å­¦ç»éªŒçš„é”™è¯¯ç†è§£ï¼‰\n",
    "\"\"\"\n",
    "    \n",
    "    prompt += f\"\"\"\n",
    "\n",
    "ã€è¾“å‡ºæ ¼å¼ã€‘\n",
    "\n",
    "åªè¾“å‡º {num_options - 1} ä¸ªé”™è¯¯é€‰é¡¹ï¼Œæ¯ä¸ªé€‰é¡¹å•ç‹¬ä¸€è¡Œï¼Œä¸éœ€è¦ç¼–å·æˆ–é¢å¤–æ ‡è®°ã€‚\n",
    "ä¾‹å¦‚ï¼š\n",
    "ç¬¬ä¸€ä¸ªé€‰é¡¹å†…å®¹\n",
    "ç¬¬äºŒä¸ªé€‰é¡¹å†…å®¹\n",
    "ç¬¬ä¸‰ä¸ªé€‰é¡¹å†…å®¹ï¼ˆå¦‚æœé€‚ç”¨ï¼‰\n",
    "\n",
    "æ¯ä¸ªé€‰é¡¹åº”è¯¥æ˜¯ä¸€ä¸ªå®Œæ•´çš„å¥å­æˆ–çŸ­æ®µè½ï¼Œèƒ½å¤Ÿä½œä¸ºå¤šé€‰é¢˜çš„ç‹¬ç«‹é€‰é¡¹ã€‚\n",
    "\n",
    "ã€æœ€åæé†’ã€‘\n",
    "\n",
    "- ç¡®ä¿ç”Ÿæˆçš„é€‰é¡¹ä¸æ­£ç¡®ç­”æ¡ˆæœ‰è¯­ä¹‰å…³è”ï¼ˆè‡³å°‘50%ç›¸ä¼¼åº¦ï¼‰\n",
    "- ä½†åˆæœ‰è¶³å¤Ÿçš„åŒºåˆ«ï¼ˆä¸è¶…è¿‡80%ç›¸ä¼¼åº¦ï¼‰\n",
    "- æ‰€æœ‰é€‰é¡¹åº”è¯¥çœ‹èµ·æ¥éƒ½æ˜¯åˆç†çš„ç­”æ¡ˆï¼Œä½†åªæœ‰æ­£ç¡®ç­”æ¡ˆæ˜¯çœŸæ­£æ­£ç¡®çš„\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# æµ‹è¯•æç¤ºè¯\n",
    "print(\"ğŸ“ æç¤ºè¯ç”Ÿæˆå‡½æ•°å·²å®šä¹‰\")\n",
    "print(\"\\nğŸ“Œ æ ·ä¾‹æç¤ºè¯ (num_options=4):\")\n",
    "print(\"=\" * 70)\n",
    "sample_prompt = create_option_generation_prompt(\n",
    "    question=\"What type of collision occurs between the two vehicles?\",\n",
    "    correct_answer=\"A T-bone (side-impact) collision where one vehicle hits the side of another\",\n",
    "    num_options=4,\n",
    "    video_context=\"Urban traffic intersection with traffic lights\",\n",
    "    question_category=\"Accident Type\"\n",
    ")\n",
    "print(sample_prompt[:500] + \"\\n...(çœç•¥)...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307fbf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# éƒ¨åˆ† 6: Qwen VL API è°ƒç”¨\n",
    "# ============================================================================\n",
    "\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import re\n",
    "\n",
    "class QwenVLOptionGenerator:\n",
    "    \"\"\"ä½¿ç”¨ Qwen VL API ç”Ÿæˆåäº‹å®é€‰é¡¹\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, api_url: str = \"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\"):\n",
    "        self.api_key = api_key\n",
    "        self.api_url = api_url\n",
    "        self.client = OpenAI(api_key=api_key, base_url=api_url)\n",
    "        self.retry_count = 0\n",
    "    \n",
    "    def frames_to_base64(self, frames: np.ndarray, max_frames: int = 4) -> List[str]:\n",
    "        \"\"\"\n",
    "        å°†è§†é¢‘å¸§è½¬æ¢ä¸º base64 ç¼–ç ï¼ˆç”¨äº API è°ƒç”¨ï¼‰\n",
    "        \"\"\"\n",
    "        b64_list = []\n",
    "        \n",
    "        # å‡åŒ€é‡‡æ ·å¸§\n",
    "        if len(frames) > max_frames:\n",
    "            indices = np.linspace(0, len(frames) - 1, max_frames, dtype=int)\n",
    "            selected_frames = frames[indices]\n",
    "        else:\n",
    "            selected_frames = frames\n",
    "        \n",
    "        for frame in selected_frames:\n",
    "            try:\n",
    "                img = Image.fromarray(frame)\n",
    "                buf = BytesIO()\n",
    "                img.save(buf, format=\"JPEG\", quality=85)\n",
    "                b64 = base64.b64encode(buf.getvalue()).decode()\n",
    "                b64_list.append(b64)\n",
    "            except Exception as e:\n",
    "                print(f\"    âš ï¸ å¸§è½¬æ¢å¤±è´¥: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return b64_list\n",
    "    \n",
    "    def generate_options(\n",
    "        self,\n",
    "        question: str,\n",
    "        correct_answer: str,\n",
    "        num_options: int,\n",
    "        video_frames: Optional[np.ndarray] = None,\n",
    "        video_context: str = \"\",\n",
    "        question_category: str = \"\"\n",
    "    ) -> Tuple[List[str], float, Optional[str]]:\n",
    "        \"\"\"\n",
    "        è°ƒç”¨ Qwen VL API ç”Ÿæˆåäº‹å®é€‰é¡¹\n",
    "        \n",
    "        è¿”å›: (é€‰é¡¹åˆ—è¡¨, å¤„ç†æ—¶é—´, é”™è¯¯ä¿¡æ¯)\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # åˆ›å»ºæç¤ºè¯\n",
    "            prompt_text = create_option_generation_prompt(\n",
    "                question=question,\n",
    "                correct_answer=correct_answer,\n",
    "                num_options=num_options,\n",
    "                video_context=video_context,\n",
    "                question_category=question_category\n",
    "            )\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # æ„å»ºæ¶ˆæ¯å†…å®¹\n",
    "            content = [{\"type\": \"text\", \"text\": prompt_text}]\n",
    "            \n",
    "            # å¦‚æœæœ‰è§†é¢‘å¸§ï¼Œæ·»åŠ å›¾åƒ\n",
    "            if video_frames is not None and len(video_frames) > 0:\n",
    "                b64_frames = self.frames_to_base64(video_frames, max_frames=4)\n",
    "                for b64 in b64_frames:\n",
    "                    content.append({\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{b64}\"\n",
    "                        }\n",
    "                    })\n",
    "            \n",
    "            # è°ƒç”¨ API\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"qwen-vl-max\",\n",
    "                messages=[{\"role\": \"user\", \"content\": content}],\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                max_tokens=1000,\n",
    "                timeout=config.api_timeout\n",
    "            )\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            \n",
    "            # æå–å“åº”\n",
    "            response_text = response.choices[0].message.content\n",
    "            \n",
    "            # è§£æé€‰é¡¹\n",
    "            options = self._parse_options(response_text, num_options - 1)\n",
    "            \n",
    "            return options, elapsed_time, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            elapsed_time = time.time() - start_time if 'start_time' in locals() else 0\n",
    "            print(f\"    âŒ API é”™è¯¯: {error_msg[:100]}\")\n",
    "            return [], elapsed_time, error_msg\n",
    "    \n",
    "    def _parse_options(self, response_text: str, expected_count: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        ä» API å“åº”ä¸­è§£æé€‰é¡¹ï¼ˆæ”¹è¿›ç‰ˆ - é¿å…æˆªæ–­å¼€å¤´ï¼‰\n",
    "        \"\"\"\n",
    "        lines = response_text.strip().split('\\n')\n",
    "        \n",
    "        # è¿‡æ»¤ç©ºè¡Œ\n",
    "        options = [line.strip() for line in lines if line.strip()]\n",
    "        \n",
    "        # ç§»é™¤å¯èƒ½çš„ç¼–å·å‰ç¼€ï¼ˆæ›´ç²¾ç¡®çš„æ–¹å¼ï¼‰\n",
    "        cleaned_options = []\n",
    "        for opt in options:\n",
    "            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åªç§»é™¤æ˜ç¡®çš„ç¼–å·æ¨¡å¼\n",
    "            # åŒ¹é…: \"1.\", \"1)\", \"A.\", \"A)\", \"(1)\", \"[1]\" ç­‰\n",
    "            opt_cleaned = re.sub(r'^[\\[\\(]?\\d+[\\]\\)]?[\\.\\)\\:\\-\\s]+|^[A-Za-z][\\.\\)\\:\\-\\s]+', '', opt)\n",
    "            \n",
    "            # å¦‚æœæ¸…ç†åä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬\n",
    "            if len(opt_cleaned.strip()) < 10:\n",
    "                opt_cleaned = opt\n",
    "            \n",
    "            # è¿‡æ»¤è¿‡çŸ­çš„æ–‡æœ¬ï¼ˆä½†ä¸è¦å¤ªä¸¥æ ¼ï¼‰\n",
    "            if len(opt_cleaned.strip()) > 5:\n",
    "                cleaned_options.append(opt_cleaned.strip())\n",
    "        \n",
    "        return cleaned_options[:expected_count]\n",
    "\n",
    "# åˆå§‹åŒ–ç”Ÿæˆå™¨\n",
    "if config.qwen_api_key:\n",
    "    option_generator = QwenVLOptionGenerator(config.qwen_api_key, config.qwen_api_url)\n",
    "    print(\"âœ… Qwen VL é€‰é¡¹ç”Ÿæˆå™¨å·²åˆå§‹åŒ–\")\n",
    "else:\n",
    "    option_generator = None\n",
    "    print(\"âŒ æœªæ‰¾åˆ° API Keyï¼Œæ— æ³•åˆå§‹åŒ–ç”Ÿæˆå™¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e203221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# éƒ¨åˆ† 7: ç›¸ä¼¼åº¦è®¡ç®—\n",
    "# ============================================================================\n",
    "\n",
    "class SimilarityCalculator:\n",
    "    \"\"\"è®¡ç®—é€‰é¡¹ä¸æ­£ç¡®ç­”æ¡ˆä¹‹é—´çš„ç›¸ä¼¼åº¦\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=500,\n",
    "            lowercase=True,\n",
    "            stop_words='english'\n",
    "        )\n",
    "    \n",
    "    def compute_similarity(self, correct_answer: str, options: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        è®¡ç®—æ¯ä¸ªé€‰é¡¹ä¸æ­£ç¡®ç­”æ¡ˆçš„ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "        \n",
    "        è¿”å›: {\"option_text\": similarity_score, ...}\n",
    "        \"\"\"\n",
    "        \n",
    "        if not options:\n",
    "            return {}\n",
    "        \n",
    "        # åˆ›å»ºæ–‡æœ¬åˆ—è¡¨\n",
    "        texts = [correct_answer] + options\n",
    "        \n",
    "        try:\n",
    "            # TF-IDF å‘é‡åŒ–\n",
    "            tfidf_matrix = self.vectorizer.fit_transform(texts)\n",
    "            \n",
    "            # è®¡ç®—ç¬¬ä¸€ä¸ªï¼ˆæ­£ç¡®ç­”æ¡ˆï¼‰ä¸å…¶ä»–çš„ç›¸ä¼¼åº¦\n",
    "            similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])[0]\n",
    "            \n",
    "            # åˆ›å»ºç»“æœå­—å…¸\n",
    "            result = {}\n",
    "            for i, opt in enumerate(options):\n",
    "                result[opt] = float(similarities[i])\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    âš ï¸ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def evaluate_option_quality(\n",
    "        self,\n",
    "        correct_answer: str,\n",
    "        generated_options: List[str],\n",
    "        min_sim: float = 0.2,\n",
    "        max_sim: float = 0.8\n",
    "    ) -> Tuple[List[str], Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        æ ¹æ®ç›¸ä¼¼åº¦é˜ˆå€¼è¯„ä¼°å’Œè¿‡æ»¤é€‰é¡¹\n",
    "        \n",
    "        è¿”å›: (æœ‰æ•ˆé€‰é¡¹, ç›¸ä¼¼åº¦å­—å…¸)\n",
    "        \"\"\"\n",
    "        \n",
    "        similarities = self.compute_similarity(correct_answer, generated_options)\n",
    "        \n",
    "        # è¿‡æ»¤é€‰é¡¹\n",
    "        valid_options = []\n",
    "        for opt, sim in similarities.items():\n",
    "            if min_sim <= sim <= max_sim:\n",
    "                valid_options.append(opt)\n",
    "        \n",
    "        return valid_options, similarities\n",
    "\n",
    "# åˆå§‹åŒ–ç›¸ä¼¼åº¦è®¡ç®—å™¨\n",
    "similarity_calculator = SimilarityCalculator()\n",
    "print(\"âœ… ç›¸ä¼¼åº¦è®¡ç®—å™¨å·²åˆå§‹åŒ–\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72925894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# éƒ¨åˆ† 8: ä¸»å¤„ç†ç®¡é“\n",
    "# ============================================================================\n",
    "\n",
    "def process_single_question(\n",
    "    video_id: str,\n",
    "    question_data: Dict,\n",
    "    video_frames: Optional[np.ndarray],\n",
    "    num_options_list: List[int],\n",
    "    dense_caption: str = \"\"\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    å¤„ç†å•ä¸ªé—®é¢˜ï¼Œä¸ºå…¶ç”Ÿæˆåäº‹å®é€‰é¡¹\n",
    "    \n",
    "    è¿”å›åŒ…å«æ‰€æœ‰é€‰é¡¹æ•°é‡é…ç½®çš„ç»“æœ\n",
    "    \"\"\"\n",
    "    \n",
    "    result = {\n",
    "        'video_id': video_id,\n",
    "        'q_id': question_data['q_id'],\n",
    "        'question': question_data['text'],\n",
    "        'category': question_data['category'],\n",
    "        'correct_answer': question_data['correct_answer'],\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'results_by_num_options': {}\n",
    "    }\n",
    "    \n",
    "    # ä¸ºæ¯ä¸ªé€‰é¡¹æ•°é‡é…ç½®ç”Ÿæˆé€‰é¡¹\n",
    "    for num_options in num_options_list:\n",
    "        num_options_key = f\"{num_options}_options\"\n",
    "        \n",
    "        print(f\"      ç”Ÿæˆ {num_options} ä¸ªé€‰é¡¹... \", end=\"\", flush=True)\n",
    "        \n",
    "        try:\n",
    "            # è°ƒç”¨ Qwen VL API\n",
    "            generated_options, elapsed_time, error = option_generator.generate_options(\n",
    "                question=question_data['text'],\n",
    "                correct_answer=question_data['correct_answer'],\n",
    "                num_options=num_options,\n",
    "                video_frames=video_frames,\n",
    "                video_context=dense_caption,\n",
    "                question_category=question_data['category']\n",
    "            )\n",
    "            \n",
    "            if error:\n",
    "                print(f\"âŒ (API é”™è¯¯)\")\n",
    "                result['results_by_num_options'][num_options_key] = {\n",
    "                    'status': 'failed',\n",
    "                    'error': error,\n",
    "                    'elapsed_time': elapsed_time\n",
    "                }\n",
    "                continue\n",
    "            \n",
    "            # è®¡ç®—ç›¸ä¼¼åº¦\n",
    "            valid_options, similarities = similarity_calculator.evaluate_option_quality(\n",
    "                correct_answer=question_data['correct_answer'],\n",
    "                generated_options=generated_options,\n",
    "                min_sim=config.min_similarity,\n",
    "                max_sim=config.max_similarity\n",
    "            )\n",
    "            \n",
    "            # å¦‚æœæœ‰æ•ˆé€‰é¡¹ä¸è¶³ï¼Œä½¿ç”¨æ‰€æœ‰ç”Ÿæˆçš„é€‰é¡¹\n",
    "            final_options = valid_options if len(valid_options) >= num_options - 1 else generated_options[:num_options - 1]\n",
    "            \n",
    "            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯\n",
    "            sims_values = [similarities.get(opt, 0.5) for opt in final_options]\n",
    "            \n",
    "            result['results_by_num_options'][num_options_key] = {\n",
    "                'status': 'success',\n",
    "                'generated_options': final_options,\n",
    "                'similarities': {opt: similarities.get(opt, 0.5) for opt in final_options},\n",
    "                'similarity_stats': {\n",
    "                    'min': float(np.min(sims_values)) if sims_values else 0.0,\n",
    "                    'max': float(np.max(sims_values)) if sims_values else 0.0,\n",
    "                    'mean': float(np.mean(sims_values)) if sims_values else 0.0,\n",
    "                    'std': float(np.std(sims_values)) if len(sims_values) > 1 else 0.0\n",
    "                },\n",
    "                'elapsed_time': elapsed_time\n",
    "            }\n",
    "            \n",
    "            print(f\"âœ… ({len(final_options)} é€‰é¡¹, mean_sim={np.mean(sims_values):.3f})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ (å¼‚å¸¸: {str(e)[:50]})\")\n",
    "            result['results_by_num_options'][num_options_key] = {\n",
    "                'status': 'failed',\n",
    "                'error': str(e),\n",
    "                'elapsed_time': 0\n",
    "            }\n",
    "    \n",
    "    return result\n",
    "\n",
    "def process_video_batch(qa_data_list: List[Dict], num_options_list: List[int]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    æ‰¹é‡å¤„ç†æ‰€æœ‰è§†é¢‘çš„æ‰€æœ‰é—®é¢˜\n",
    "    \"\"\"\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for video_data in tqdm(qa_data_list, desc=\"å¤„ç†è§†é¢‘\"):\n",
    "        video_id = video_data['video_id']\n",
    "        questions = video_data['questions']\n",
    "        \n",
    "        print(f\"\\nğŸ¬ è§†é¢‘ {video_id}\")\n",
    "        \n",
    "        # åŠ è½½è§†é¢‘å¸§\n",
    "        video_path = find_video_file(video_id, config.video_dir)\n",
    "        if not video_path:\n",
    "            print(f\"  âŒ è§†é¢‘æ–‡ä»¶æœªæ‰¾åˆ°\")\n",
    "            continue\n",
    "        \n",
    "        video_frames = load_video_frames(video_path, config.max_frames_per_video, config.target_fps)\n",
    "        if video_frames is None:\n",
    "            print(f\"  âŒ æ— æ³•åŠ è½½è§†é¢‘å¸§\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  âœ“ åŠ è½½ {len(video_frames)} å¸§\")\n",
    "        \n",
    "        # å¤„ç†æ¯ä¸ªé—®é¢˜\n",
    "        for q_idx, question in enumerate(questions, 1):\n",
    "            print(f\"  é—®é¢˜ {q_idx}/{len(questions)}: {question['text'][:50]}...\")\n",
    "            \n",
    "            q_result = process_single_question(\n",
    "                video_id=video_id,\n",
    "                question_data=question,\n",
    "                video_frames=video_frames,\n",
    "                num_options_list=num_options_list\n",
    "            )\n",
    "            \n",
    "            all_results.append(q_result)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "print(\"âœ… å¤„ç†ç®¡é“å‡½æ•°å·²å®šä¹‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68881500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# éƒ¨åˆ† 9: è¿è¡Œå’Œä¿å­˜ç»“æœ\n",
    "# ============================================================================\n",
    "\n",
    "# å¿«é€Ÿæµ‹è¯• (ä»…å¤„ç†å‰ 2 ä¸ªè§†é¢‘, æ¯ä¸ª 2 ä¸ªé—®é¢˜)\n",
    "print(\"\\nğŸš€ å¼€å§‹å¤„ç†\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if option_generator is None:\n",
    "    print(\"âŒ é”™è¯¯: Qwen VL ç”Ÿæˆå™¨æœªåˆå§‹åŒ–\")\n",
    "    print(\"   è¯·ç¡®ä¿ ALI_INTERNATIONAL_KEY å·²åœ¨ Colab Secrets ä¸­è®¾ç½®\")\n",
    "else:\n",
    "    # é™åˆ¶æ•°æ®è§„æ¨¡ç”¨äºæµ‹è¯•\n",
    "    test_qa_data = []\n",
    "    for video_data in qa_pairs[:2]:  # ä»…å‰ 2 ä¸ªè§†é¢‘\n",
    "        limited_questions = video_data.copy()\n",
    "        limited_questions['questions'] = video_data['questions'][:2]  # æ¯ä¸ªè§†é¢‘ä»… 2 ä¸ªé—®é¢˜\n",
    "        test_qa_data.append(limited_questions)\n",
    "    \n",
    "    print(f\"ğŸ“ è®¡åˆ’å¤„ç†:\")\n",
    "    print(f\"   è§†é¢‘æ•°: {len(test_qa_data)}\")\n",
    "    print(f\"   æ€»é—®é¢˜æ•°: {sum(len(v['questions']) for v in test_qa_data)}\")\n",
    "    print(f\"   é€‰é¡¹æ•°é…ç½®: {config.num_options_list}\")\n",
    "    print(f\"   æ€» API è°ƒç”¨æ¬¡æ•°: {sum(len(v['questions']) for v in test_qa_data) * len(config.num_options_list)}\")\n",
    "    print()\n",
    "    \n",
    "    # å¤„ç†\n",
    "    results = process_video_batch(test_qa_data, config.num_options_list)\n",
    "    \n",
    "    print(f\"\\nâœ… å¤„ç†å®Œæˆ\")\n",
    "    print(f\"   ç”Ÿæˆ {len(results)} ä¸ªé—®é¢˜çš„ç»“æœ\")\n",
    "    \n",
    "    # ä¿å­˜ç»“æœ\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # JSON æ ¼å¼\n",
    "    json_output_path = Path(config.output_dir) / f\"generated_options_{timestamp}.json\"\n",
    "    with open(json_output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"   âœ“ JSON ç»“æœä¿å­˜: {json_output_path}\")\n",
    "    \n",
    "    # CSV æ ¼å¼ï¼ˆæ”¹è¿›ç‰ˆ - æ¯ä¸ªé€‰é¡¹å•ç‹¬ä¸€åˆ—ï¼‰\n",
    "    csv_data = []\n",
    "    for result in results:\n",
    "        for num_opts_key, num_opts_result in result['results_by_num_options'].items():\n",
    "            if num_opts_result['status'] == 'success':\n",
    "                num_options = int(num_opts_key.split('_')[0])\n",
    "                generated_opts = num_opts_result['generated_options']\n",
    "                \n",
    "                # æ„å»ºè¡Œæ•°æ®ï¼Œæ¯ä¸ªé€‰é¡¹å•ç‹¬ä¸€åˆ—\n",
    "                row_data = {\n",
    "                    'video_id': result['video_id'],\n",
    "                    'question_id': result['q_id'],\n",
    "                    'question': result['question'],\n",
    "                    'category': result['category'],\n",
    "                    'correct_answer': result['correct_answer'],\n",
    "                    'num_options': num_options,\n",
    "                }\n",
    "                \n",
    "                # ä¸ºæ¯ä¸ªé€‰é¡¹åˆ›å»ºå•ç‹¬çš„åˆ—\n",
    "                for i in range(num_options - 1):\n",
    "                    if i < len(generated_opts):\n",
    "                        row_data[f'option_{i+1}'] = generated_opts[i]\n",
    "                        row_data[f'option_{i+1}_similarity'] = num_opts_result['similarities'].get(generated_opts[i], 0.0)\n",
    "                    else:\n",
    "                        row_data[f'option_{i+1}'] = None\n",
    "                        row_data[f'option_{i+1}_similarity'] = None\n",
    "                \n",
    "                # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯\n",
    "                row_data.update({\n",
    "                    'min_similarity': num_opts_result['similarity_stats']['min'],\n",
    "                    'max_similarity': num_opts_result['similarity_stats']['max'],\n",
    "                    'mean_similarity': num_opts_result['similarity_stats']['mean'],\n",
    "                    'std_similarity': num_opts_result['similarity_stats']['std'],\n",
    "                    'elapsed_time': num_opts_result['elapsed_time']\n",
    "                })\n",
    "                \n",
    "                csv_data.append(row_data)\n",
    "    \n",
    "    if csv_data:\n",
    "        csv_output_path = Path(config.output_dir) / f\"generated_options_{timestamp}.csv\"\n",
    "        csv_df = pd.DataFrame(csv_data)\n",
    "        csv_df.to_csv(csv_output_path, index=False, encoding='utf-8', quoting=1, line_terminator='\\n')\n",
    "        print(f\"   âœ“ CSV ç»“æœä¿å­˜: {csv_output_path}\")\n",
    "        print(f\"     æ¯ä¸ªé€‰é¡¹å•ç‹¬ä¸€åˆ—ï¼ŒåŒ…å«ç›¸ä¼¼åº¦åˆ†æ•°\")\n",
    "        print(f\"\\nğŸ“‹ CSV åˆ—ç»“æ„:\")\n",
    "        print(f\"   åŸºç¡€åˆ—: video_id, question_id, question, category, correct_answer, num_options\")\n",
    "        print(f\"   é€‰é¡¹åˆ—: option_1, option_1_similarity, option_2, option_2_similarity, ...\")\n",
    "        print(f\"   ç»Ÿè®¡åˆ—: min_similarity, max_similarity, mean_similarity, std_similarity, elapsed_time\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š è¾“å‡ºç›®å½•: {config.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18027262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# éƒ¨åˆ† 10: æ¶ˆèå®éªŒåˆ†æ\n",
    "# ============================================================================\n",
    "# import timestamp\n",
    "results_json_path = \"/content/drive/MyDrive/data/VQA_outputs/full_generated_options_20251204_214859.json\"\n",
    "\n",
    "# å¯é€‰ï¼šä»å·²ç”Ÿæˆçš„ JSON ç»“æœæ¢å¤ï¼ˆé˜²æ­¢æ–­å¼€åå˜é‡ä¸¢å¤±ï¼‰\n",
    "# results_json_path = None\n",
    "if 'results' not in locals() and results_json_path:\n",
    "    with open(results_json_path, 'r', encoding='utf-8') as f:\n",
    "        results = json.load(f)\n",
    "    print(f\"ğŸ” å·²ä» JSON åŠ è½½ results: {results_json_path}\")\n",
    "\n",
    "def analyze_ablation_results(results: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    åˆ†ææ¶ˆèå®éªŒç»“æœ (3 vs 4 vs 5 ä¸ªé€‰é¡¹)\n",
    "    \"\"\"\n",
    "\n",
    "    analysis = {\n",
    "        'by_num_options': {},\n",
    "        'by_category': {},\n",
    "        'overall_stats': {}\n",
    "    }\n",
    "\n",
    "    # æŒ‰é€‰é¡¹æ•°ç»Ÿè®¡\n",
    "    for num_opts in [3, 4, 5]:\n",
    "        num_opts_key = f\"{num_opts}_options\"\n",
    "        stats = {\n",
    "            'total_questions': 0,\n",
    "            'successful': 0,\n",
    "            'failed': 0,\n",
    "            'mean_similarity': [],\n",
    "            'mean_elapsed_time': 0,\n",
    "            'difficulty_distribution': {\n",
    "                'easy': 0,    # mean_sim > 0.6\n",
    "                'medium': 0,  # 0.4 < mean_sim <= 0.6\n",
    "                'hard': 0     # mean_sim <= 0.4\n",
    "            }\n",
    "        }\n",
    "\n",
    "        elapsed_times = []\n",
    "\n",
    "        for result in results:\n",
    "            if num_opts_key in result['results_by_num_options']:\n",
    "                num_opts_result = result['results_by_num_options'][num_opts_key]\n",
    "                stats['total_questions'] += 1\n",
    "\n",
    "                if num_opts_result['status'] == 'success':\n",
    "                    stats['successful'] += 1\n",
    "                    mean_sim = num_opts_result['similarity_stats']['mean']\n",
    "                    stats['mean_similarity'].append(mean_sim)\n",
    "\n",
    "                    # éš¾åº¦åˆ†ç±»\n",
    "                    if mean_sim > 0.6:\n",
    "                        stats['difficulty_distribution']['easy'] += 1\n",
    "                    elif mean_sim > 0.4:\n",
    "                        stats['difficulty_distribution']['medium'] += 1\n",
    "                    else:\n",
    "                        stats['difficulty_distribution']['hard'] += 1\n",
    "\n",
    "                    elapsed_times.append(num_opts_result['elapsed_time'])\n",
    "                else:\n",
    "                    stats['failed'] += 1\n",
    "\n",
    "        if stats['mean_similarity']:\n",
    "            stats['mean_similarity_avg'] = float(np.mean(stats['mean_similarity']))\n",
    "            stats['mean_similarity_std'] = float(np.std(stats['mean_similarity']))\n",
    "\n",
    "        if elapsed_times:\n",
    "            stats['mean_elapsed_time'] = float(np.mean(elapsed_times))\n",
    "\n",
    "        analysis['by_num_options'][num_opts_key] = stats\n",
    "\n",
    "    # æŒ‰åˆ†ç±»ç»Ÿè®¡\n",
    "    categories = set()\n",
    "    for result in results:\n",
    "        categories.add(result['category'])\n",
    "\n",
    "    for category in categories:\n",
    "        analysis['by_category'][category] = {}\n",
    "        for num_opts in [3, 4, 5]:\n",
    "            num_opts_key = f\"{num_opts}_options\"\n",
    "            cat_results = [r for r in results if r['category'] == category\n",
    "                         and num_opts_key in r['results_by_num_options']]\n",
    "\n",
    "            if cat_results:\n",
    "                successful = sum(1 for r in cat_results\n",
    "                               if r['results_by_num_options'][num_opts_key]['status'] == 'success')\n",
    "                mean_sims = [r['results_by_num_options'][num_opts_key]['similarity_stats']['mean']\n",
    "                           for r in cat_results\n",
    "                           if r['results_by_num_options'][num_opts_key]['status'] == 'success']\n",
    "\n",
    "                analysis['by_category'][category][num_opts_key] = {\n",
    "                    'total': len(cat_results),\n",
    "                    'successful': successful,\n",
    "                    'success_rate': successful / len(cat_results),\n",
    "                    'mean_similarity': float(np.mean(mean_sims)) if mean_sims else 0.0\n",
    "                }\n",
    "\n",
    "    # æ€»ä½“ç»Ÿè®¡\n",
    "    all_successful = sum(r['results_by_num_options'][f\"{n}_options\"]['status'] == 'success'\n",
    "                        for r in results\n",
    "                        for n in [3, 4, 5]\n",
    "                        if f\"{n}_options\" in r['results_by_num_options'])\n",
    "\n",
    "    all_total = sum(1 for r in results\n",
    "                   for n in [3, 4, 5]\n",
    "                   if f\"{n}_options\" in r['results_by_num_options'])\n",
    "\n",
    "    analysis['overall_stats'] = {\n",
    "        'total_experiments': all_total,\n",
    "        'successful_experiments': all_successful,\n",
    "        'success_rate': all_successful / all_total if all_total > 0 else 0,\n",
    "        'total_unique_questions': len(set(r['question'] for r in results))\n",
    "    }\n",
    "\n",
    "    return analysis\n",
    "\n",
    "# ç¡®ä¿å¯è§†åŒ–æ–‡ä»¶åæœ‰æ—¶é—´æˆ³\n",
    "if 'timestamp' not in locals():\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# æ‰§è¡Œåˆ†æï¼ˆæ”¯æŒä» results_json_path æ¢å¤ï¼‰\n",
    "if 'results' in locals() and results:\n",
    "    ablation_analysis = analyze_ablation_results(results)\n",
    "\n",
    "    print(\"\\nğŸ“ˆ æ¶ˆèå®éªŒåˆ†æ\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # æ€»ä½“ç»Ÿè®¡\n",
    "    print(\"\\næ€»ä½“ç»Ÿè®¡:\")\n",
    "    overall = ablation_analysis['overall_stats']\n",
    "    print(f\"  â€¢ æ€»å®éªŒæ•°: {overall['total_experiments']}\")\n",
    "    print(f\"  â€¢ æˆåŠŸå®éªŒ: {overall['successful_experiments']}\")\n",
    "    print(f\"  â€¢ æˆåŠŸç‡: {100 * overall['success_rate']:.1f}%\")\n",
    "    print(f\"  â€¢ ç‹¬ç‰¹é—®é¢˜æ•°: {overall['total_unique_questions']}\")\n",
    "\n",
    "    # æŒ‰é€‰é¡¹æ•°ç»Ÿè®¡\n",
    "    print(\"\\næŒ‰é€‰é¡¹æ•°ç»Ÿè®¡:\")\n",
    "    for num_opts in [3, 4, 5]:\n",
    "        num_opts_key = f\"{num_opts}_options\"\n",
    "        stats = ablation_analysis['by_num_options'][num_opts_key]\n",
    "        print(f\"\\n  {num_opts} ä¸ªé€‰é¡¹:\")\n",
    "        print(f\"    â€¢ é—®é¢˜æ•°: {stats['total_questions']}\")\n",
    "        print(f\"    â€¢ æˆåŠŸ: {stats['successful']}/{stats['total_questions']}\")\n",
    "        if stats['successful'] > 0:\n",
    "            print(f\"    â€¢ å¹³å‡ç›¸ä¼¼åº¦: {stats.get('mean_similarity_avg', 0):.3f} Â± {stats.get('mean_similarity_std', 0):.3f}\")\n",
    "            print(f\"    â€¢ å¹³å‡è€—æ—¶: {stats['mean_elapsed_time']:.1f}s\")\n",
    "            print(f\"    â€¢ éš¾åº¦åˆ†å¸ƒ: Easy={stats['difficulty_distribution']['easy']}, Medium={stats['difficulty_distribution']['medium']}, Hard={stats['difficulty_distribution']['hard']}\")\n",
    "\n",
    "    # ä¿å­˜åˆ†æç»“æœ\n",
    "    analysis_output_path = Path(config.output_dir) / f\"ablation_analysis_{timestamp}.json\"\n",
    "    with open(analysis_output_path, 'w', encoding='utf-8') as f:\n",
    "        # Convert to JSON serializable format\n",
    "        json_analysis = json.loads(json.dumps(ablation_analysis, default=str))\n",
    "        json.dump(json_analysis, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nâœ“ åˆ†æç»“æœä¿å­˜: {analysis_output_path}\")\n",
    "else:\n",
    "    print(\"âš ï¸ æœªæ‰¾åˆ° resultsï¼Œè¯·å…ˆè¿è¡Œç”Ÿæˆæ­¥éª¤æˆ–è®¾ç½® results_json_pathã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8232dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# éƒ¨åˆ† 11: å¯è§†åŒ–\n",
    "# ============================================================================\n",
    "\n",
    "# å¯é€‰ï¼šå¦‚æœæ–­å¼€å ablation_analysis ä¸¢å¤±ï¼Œå¯ä» results é‡æ–°è®¡ç®—ï¼Œæˆ–ä» JSON é‡æ–°åŠ è½½\n",
    "# è‹¥éœ€è¦ä» JSON åŠ è½½ï¼Œè¯·è®¾ç½®ä¸‹æ–¹è·¯å¾„\n",
    "analysis_json_path = None  # ä¾‹å¦‚ Path(config.output_dir)/\"ablation_analysis_20250101_120000.json\"\n",
    "results_json_path_for_plot = None  # è‹¥ ablation_analysis ä¸åœ¨ï¼Œä¸”éœ€è¦å…ˆåŠ è½½ results å†è®¡ç®—\n",
    "\n",
    "if 'ablation_analysis' not in locals():\n",
    "    if analysis_json_path:\n",
    "        with open(analysis_json_path, 'r', encoding='utf-8') as f:\n",
    "            ablation_analysis = json.load(f)\n",
    "        print(f\"ğŸ” å·²ä» JSON åŠ è½½ ablation_analysis: {analysis_json_path}\")\n",
    "    elif 'results' in locals() and results:\n",
    "        ablation_analysis = analyze_ablation_results(results)\n",
    "        print(\"ğŸ” å·²æ ¹æ®ç°æœ‰ results é‡æ–°è®¡ç®— ablation_analysis\")\n",
    "    elif results_json_path_for_plot:\n",
    "        with open(results_json_path_for_plot, 'r', encoding='utf-8') as f:\n",
    "            results = json.load(f)\n",
    "        ablation_analysis = analyze_ablation_results(results)\n",
    "        print(f\"ğŸ” å·²ä» JSON åŠ è½½ results å¹¶é‡æ–°è®¡ç®— ablation_analysis: {results_json_path_for_plot}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ æ—  ablation_analysisï¼Œä¹Ÿæœªæä¾› JSON è·¯å¾„ã€‚è¯·å…ˆè¿è¡Œåˆ†ææˆ–æä¾›è·¯å¾„ã€‚\")\n",
    "\n",
    "# ç¡®ä¿å¯è§†åŒ–æ–‡ä»¶åæœ‰æ—¶é—´æˆ³\n",
    "if 'timestamp' not in locals():\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "\n",
    "def visualize_results(results: List[Dict], analysis: Dict, output_dir: str):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆæ¶ˆèå®éªŒçš„å¯è§†åŒ–å›¾è¡¨\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Counterfactual Options Generation - Ablation Study Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. æˆåŠŸç‡å¯¹æ¯”\n",
    "    ax = axes[0, 0]\n",
    "    num_options = [3, 4, 5]\n",
    "    success_rates = []\n",
    "    for num_opts in num_options:\n",
    "        key = f\"{num_opts}_options\"\n",
    "        stats = analysis['by_num_options'][key]\n",
    "        rate = (stats['successful'] / stats['total_questions'] * 100) if stats['total_questions'] > 0 else 0\n",
    "        success_rates.append(rate)\n",
    "    \n",
    "    ax.bar(num_options, success_rates, color=['#3498db', '#2ecc71', '#e74c3c'], alpha=0.7, edgecolor='black')\n",
    "    ax.set_xlabel('Number of Options', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Success Rate (%)', fontsize=11, fontweight='bold')\n",
    "    ax.set_title('Success Rate by Number of Options', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylim([0, 105])\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "    for i, v in enumerate(success_rates):\n",
    "        ax.text(num_options[i], v + 2, f'{v:.1f}%', ha='center', fontweight='bold')\n",
    "    \n",
    "    # 2. å¹³å‡ç›¸ä¼¼åº¦å¯¹æ¯”\n",
    "    ax = axes[0, 1]\n",
    "    mean_sims = []\n",
    "    for num_opts in num_options:\n",
    "        key = f\"{num_opts}_options\"\n",
    "        stats = analysis['by_num_options'][key]\n",
    "        mean_sims.append(stats.get('mean_similarity_avg', 0))\n",
    "    \n",
    "    ax.bar(num_options, mean_sims, color=['#3498db', '#2ecc71', '#e74c3c'], alpha=0.7, edgecolor='black')\n",
    "    ax.set_xlabel('Number of Options', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Mean Similarity', fontsize=11, fontweight='bold')\n",
    "    ax.set_title('Average Similarity Score by Number of Options', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "    ax.axhline(y=config.min_similarity, color='r', linestyle='--', label=f'Min threshold ({config.min_similarity})', alpha=0.5)\n",
    "    ax.axhline(y=config.max_similarity, color='b', linestyle='--', label=f'Max threshold ({config.max_similarity})', alpha=0.5)\n",
    "    ax.legend()\n",
    "    for i, v in enumerate(mean_sims):\n",
    "        ax.text(num_options[i], v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "    \n",
    "    # 3. éš¾åº¦åˆ†å¸ƒ\n",
    "    ax = axes[1, 0]\n",
    "    difficulty_labels = ['Easy\\n(>0.6)', 'Medium\\n(0.4-0.6)', 'Hard\\n(<0.4)']\n",
    "    x = np.arange(len(num_options))\n",
    "    width = 0.25\n",
    "    \n",
    "    easy_counts = []\n",
    "    medium_counts = []\n",
    "    hard_counts = []\n",
    "    \n",
    "    for num_opts in num_options:\n",
    "        key = f\"{num_opts}_options\"\n",
    "        diff = analysis['by_num_options'][key]['difficulty_distribution']\n",
    "        easy_counts.append(diff['easy'])\n",
    "        medium_counts.append(diff['medium'])\n",
    "        hard_counts.append(diff['hard'])\n",
    "    \n",
    "    ax.bar(x - width, easy_counts, width, label='Easy', color='#2ecc71', alpha=0.8)\n",
    "    ax.bar(x, medium_counts, width, label='Medium', color='#f39c12', alpha=0.8)\n",
    "    ax.bar(x + width, hard_counts, width, label='Hard', color='#e74c3c', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Number of Options', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Count', fontsize=11, fontweight='bold')\n",
    "    ax.set_title('Difficulty Distribution', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(num_options)\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    # 4. è€—æ—¶å¯¹æ¯”\n",
    "    ax = axes[1, 1]\n",
    "    elapsed_times = []\n",
    "    for num_opts in num_options:\n",
    "        key = f\"{num_opts}_options\"\n",
    "        stats = analysis['by_num_options'][key]\n",
    "        elapsed_times.append(stats['mean_elapsed_time'])\n",
    "    \n",
    "    ax.bar(num_options, elapsed_times, color=['#3498db', '#2ecc71', '#e74c3c'], alpha=0.7, edgecolor='black')\n",
    "    ax.set_xlabel('Number of Options', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Elapsed Time (seconds)', fontsize=11, fontweight='bold')\n",
    "    ax.set_title('Average Processing Time by Number of Options', fontsize=12, fontweight='bold')\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "    for i, v in enumerate(elapsed_times):\n",
    "        ax.text(num_options[i], v + 1, f'{v:.1f}s', ha='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # ä¿å­˜å›¾è¡¨\n",
    "    plot_path = Path(output_dir) / f\"ablation_visualization_{timestamp}.png\"\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"âœ“ å¯è§†åŒ–å·²ä¿å­˜: {plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# æ‰§è¡Œå¯è§†åŒ–ï¼ˆæ”¯æŒä» JSON æ¢å¤ï¼‰\n",
    "if 'ablation_analysis' in locals():\n",
    "    visualize_results(results if 'results' in locals() else [], ablation_analysis, config.output_dir)\n",
    "else:\n",
    "    print(\"âš ï¸ æ—  ablation_analysisï¼Œæ— æ³•ç»˜å›¾ï¼Œè¯·å…ˆæä¾› JSON æˆ–é‡æ–°è®¡ç®—ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bfab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# éƒ¨åˆ† 12: å…¨æ•°æ®é›†å¤„ç†ï¼ˆå¯é€‰ï¼‰\n",
    "# ============================================================================\n",
    "\n",
    "def process_full_dataset(qa_data_list: List[Dict], num_options_list: List[int]):\n",
    "    \"\"\"\n",
    "    å¤„ç†å®Œæ•´æ•°æ®é›†\n",
    "    \n",
    "    æ³¨æ„: è¿™ä¼šæ¶ˆè€—å¤§é‡ API é…é¢å’Œæ—¶é—´\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nâš ï¸ è­¦å‘Š: è¿™å°†å¤„ç†æ‰€æœ‰è§†é¢‘å’Œé—®é¢˜\")\n",
    "    print(f\"   é¢„è®¡ API è°ƒç”¨æ¬¡æ•°: {sum(len(v['questions']) for v in qa_data_list) * len(num_options_list)}\")\n",
    "    \n",
    "    total_questions = sum(len(v['questions']) for v in qa_data_list)\n",
    "    print(f\"   æ€»é—®é¢˜æ•°: {total_questions}\")\n",
    "    \n",
    "    # æ‰§è¡Œå¤„ç†\n",
    "    all_results = process_video_batch(qa_data_list, num_options_list)\n",
    "    \n",
    "    # ä¿å­˜ç»“æœ\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # JSON\n",
    "    json_output_path = Path(config.output_dir) / f\"full_generated_options_{timestamp}.json\"\n",
    "    with open(json_output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"âœ“ å®Œæ•´ç»“æœå·²ä¿å­˜: {json_output_path}\")\n",
    "    \n",
    "    # CSVï¼ˆæ”¹è¿›ç‰ˆ - æ¯ä¸ªé€‰é¡¹å•ç‹¬ä¸€åˆ—ï¼‰\n",
    "    csv_data = []\n",
    "    for result in all_results:\n",
    "        for num_opts_key, num_opts_result in result['results_by_num_options'].items():\n",
    "            if num_opts_result['status'] == 'success':\n",
    "                num_options = int(num_opts_key.split('_')[0])\n",
    "                generated_opts = num_opts_result['generated_options']\n",
    "                \n",
    "                # æ„å»ºè¡Œæ•°æ®\n",
    "                row_data = {\n",
    "                    'video_id': result['video_id'],\n",
    "                    'question_id': result['q_id'],\n",
    "                    'question': result['question'],\n",
    "                    'category': result['category'],\n",
    "                    'correct_answer': result['correct_answer'],\n",
    "                    'num_options': num_options,\n",
    "                }\n",
    "                \n",
    "                # ä¸ºæ¯ä¸ªé€‰é¡¹åˆ›å»ºå•ç‹¬çš„åˆ—ï¼ˆæ”¯æŒæœ€å¤š4ä¸ªé”™è¯¯é€‰é¡¹ï¼‰\n",
    "                for i in range(4):\n",
    "                    if i < len(generated_opts):\n",
    "                        row_data[f'option_{i+1}'] = generated_opts[i]\n",
    "                        row_data[f'option_{i+1}_similarity'] = num_opts_result['similarities'].get(generated_opts[i], 0.0)\n",
    "                    else:\n",
    "                        row_data[f'option_{i+1}'] = None\n",
    "                        row_data[f'option_{i+1}_similarity'] = None\n",
    "                \n",
    "                # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯\n",
    "                row_data.update({\n",
    "                    'min_similarity': num_opts_result['similarity_stats']['min'],\n",
    "                    'max_similarity': num_opts_result['similarity_stats']['max'],\n",
    "                    'mean_similarity': num_opts_result['similarity_stats']['mean'],\n",
    "                    'std_similarity': num_opts_result['similarity_stats']['std'],\n",
    "                    'elapsed_time': num_opts_result['elapsed_time']\n",
    "                })\n",
    "                \n",
    "                csv_data.append(row_data)\n",
    "    \n",
    "    if csv_data:\n",
    "        csv_output_path = Path(config.output_dir) / f\"full_generated_options_{timestamp}.csv\"\n",
    "        csv_df = pd.DataFrame(csv_data)\n",
    "        csv_df.to_csv(csv_output_path, index=False, encoding='utf-8', quoting=1, line_terminator='\\n')\n",
    "        print(f\"âœ“ CSV å·²ä¿å­˜: {csv_output_path}\")\n",
    "        print(f\"  æ¯ä¸ªé€‰é¡¹å•ç‹¬ä¸€åˆ—ï¼ŒåŒ…å«ç›¸ä¼¼åº¦åˆ†æ•°\")\n",
    "        print(f\"\\nğŸ“‹ CSV åˆ—ç»“æ„:\")\n",
    "        print(f\"   åŸºç¡€åˆ—: video_id, question_id, question, category, correct_answer, num_options\")\n",
    "        print(f\"   é€‰é¡¹åˆ—: option_1, option_1_similarity, option_2, option_2_similarity, ...\")\n",
    "        print(f\"   ç»Ÿè®¡åˆ—: min_similarity, max_similarity, mean_similarity, std_similarity, elapsed_time\")\n",
    "    \n",
    "    # åˆ†æ\n",
    "    full_analysis = analyze_ablation_results(all_results)\n",
    "    analysis_output_path = Path(config.output_dir) / f\"full_ablation_analysis_{timestamp}.json\"\n",
    "    with open(analysis_output_path, 'w', encoding='utf-8') as f:\n",
    "        json_analysis = json.loads(json.dumps(full_analysis, default=str))\n",
    "        json.dump(json_analysis, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"âœ“ åˆ†æå·²ä¿å­˜: {analysis_output_path}\")\n",
    "    \n",
    "    # å¯è§†åŒ–\n",
    "    visualize_results(all_results, full_analysis, config.output_dir)\n",
    "    \n",
    "    return all_results, full_analysis\n",
    "\n",
    "# å‡½æ•°å·²å®šä¹‰ï¼Œå¯ä»¥ç”¨ä»¥ä¸‹æ–¹å¼è°ƒç”¨:\n",
    "# full_results, full_analysis = process_full_dataset(qa_pairs, config.num_options_list)\n",
    "\n",
    "print(\"âœ… å…¨æ•°æ®é›†å¤„ç†å‡½æ•°å·²å®šä¹‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17b415e",
   "metadata": {},
   "source": [
    "# å¿«é€Ÿå‚è€ƒæŒ‡å—\n",
    "\n",
    "## ä½¿ç”¨æ–¹æ³•\n",
    "\n",
    "### åŸºæœ¬æµç¨‹\n",
    "\n",
    "1. **é…ç½®** (éƒ¨åˆ† 2)\n",
    "   - API Key ä¼šè‡ªåŠ¨ä» Colab Secrets åŠ è½½\n",
    "   - å¯ä¿®æ”¹ `config` å¯¹è±¡æ¥è°ƒæ•´å‚æ•°\n",
    "\n",
    "2. **åŠ è½½æ•°æ®** (éƒ¨åˆ† 3-4)\n",
    "   - è‡ªåŠ¨åŠ è½½ `QA_pair.csv` å’Œè§†é¢‘æ–‡ä»¶\n",
    "   - éªŒè¯æ•°æ®å®Œæ•´æ€§\n",
    "\n",
    "3. **ç”Ÿæˆé€‰é¡¹** (éƒ¨åˆ† 9)\n",
    "   - è°ƒç”¨ Qwen VL API\n",
    "   - ä¸ºæ¯ä¸ªé—®é¢˜ç”Ÿæˆ 3, 4, 5 ä¸ªé€‰é¡¹\n",
    "   - è‡ªåŠ¨è®¡ç®—ç›¸ä¼¼åº¦\n",
    "\n",
    "4. **åˆ†æç»“æœ** (éƒ¨åˆ† 10-11)\n",
    "   - æ¶ˆèå®éªŒåˆ†æ\n",
    "   - å¯è§†åŒ–å¯¹æ¯”å›¾è¡¨\n",
    "\n",
    "### ä¸»è¦å‚æ•°\n",
    "\n",
    "```python\n",
    "config.num_options_list = [3, 4, 5]      # é€‰é¡¹æ•°ï¼ˆç”¨äºæ¶ˆèå®éªŒï¼‰\n",
    "config.min_similarity = 0.2               # æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼\n",
    "config.max_similarity = 0.8               # æœ€å¤§ç›¸ä¼¼åº¦é˜ˆå€¼\n",
    "config.max_frames_per_video = 8           # æ¯ä¸ªè§†é¢‘æœ€å¤šæå–çš„å¸§æ•°\n",
    "config.target_fps = 2                     # å¸§é‡‡æ ·ç›®æ ‡é€Ÿç‡\n",
    "```\n",
    "\n",
    "### è°ƒæ•´æç¤ºè¯\n",
    "\n",
    "ç¼–è¾‘éƒ¨åˆ† 5 çš„ `create_option_generation_prompt()` å‡½æ•°æ¥è‡ªå®šä¹‰é€‰é¡¹ç”Ÿæˆç­–ç•¥ã€‚\n",
    "\n",
    "## è¾“å‡ºæ–‡ä»¶\n",
    "\n",
    "- `generated_options_YYYYMMDD_HHMMSS.json` - è¯¦ç»†çš„ç”Ÿæˆç»“æœ\n",
    "- `generated_options_YYYYMMDD_HHMMSS.csv` - ç»“æ„åŒ–çš„é€‰é¡¹æ•°æ®\n",
    "- `ablation_analysis_YYYYMMDD_HHMMSS.json` - æ¶ˆèå®éªŒåˆ†æ\n",
    "- `ablation_visualization_YYYYMMDD_HHMMSS.png` - å¯è§†åŒ–å›¾è¡¨\n",
    "\n",
    "## æ³¨æ„äº‹é¡¹\n",
    "\n",
    "- é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½ç›¸å…³æ¨¡å‹å’Œèµ„æº\n",
    "- API è°ƒç”¨æœ‰é…é¢é™åˆ¶ï¼Œè¯·æ³¨æ„æˆæœ¬\n",
    "- å»ºè®®å…ˆç”¨å°æ•°æ®é›†æµ‹è¯•ï¼Œå†è¿è¡Œå®Œæ•´æ•°æ®é›†\n",
    "- ç›¸ä¼¼åº¦è®¡ç®—åŸºäº TF-IDF å‘é‡åŒ–ï¼Œå¯æ ¹æ®éœ€è¦ä¿®æ”¹"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
