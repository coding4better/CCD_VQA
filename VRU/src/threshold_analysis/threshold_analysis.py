#!/usr/bin/env python3
"""
é˜ˆå€¼åˆ†æä¸ä¼˜åŒ–è„šæœ¬

åŠŸèƒ½ï¼š
1. è®¡ç®—å…¨é‡æ•°æ®çš„æŒ‡æ ‡åˆ†å¸ƒ
2. åŸºäºåˆ†ä½æ•°ç»™å‡ºé˜ˆå€¼å»ºè®®
3. é˜ˆå€¼æ‰«æï¼Œè®¡ç®—å„é˜ˆå€¼ç»„åˆçš„å¬å›/ç²¾åº¦/F1
4. æ‰¾åˆ°æœ€ä¼˜é˜ˆå€¼å¹¶å¯¼å‡ºå¯¹æ¯”ç»“æœ
"""

import os
import json
import numpy as np
import pandas as pd
from tqdm import tqdm

# ============================================================================
# é…ç½®ä¸è·¯å¾„
# ============================================================================

ROOT_DIR = "/home/24068286g/UString"
CCD_ROOT = os.path.join(ROOT_DIR, 'data', 'crash')
VRU_ROOT = "/home/24068286g/CCD_VQA/VRU"

ANNOTATION_FILE = os.path.join(CCD_ROOT, 'videos', 'Crash-1500.txt')
NPZ_DIR = os.path.join(CCD_ROOT, 'yolo_features', 'positive')
OUTPUT_DIR = os.path.join(VRU_ROOT, 'output')
ANALYSIS_OUTPUT = os.path.join(VRU_ROOT, 'src', 'threshold_analysis', 'threshold_analysis')

os.makedirs(ANALYSIS_OUTPUT, exist_ok=True)

# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†: æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# ============================================================================

def load_annotations(file_path):
    """åŠ è½½äº‹æ•…æ ‡æ³¨"""
    annotations = {}
    try:
        with open(file_path, 'r') as f:
            for line in f:
                try:
                    vid_id = line[:6]
                    start = line.find('[')
                    end = line.find(']')
                    if start != -1 and end != -1:
                        labels = [int(x.strip()) for x in line[start+1:end].split(',')]
                        accident_frame = labels.index(1)
                        annotations[f"{vid_id}.mp4"] = accident_frame
                except:
                    continue
    except FileNotFoundError:
        print(f"âœ— æ ‡æ³¨æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
    return annotations


def calculate_metrics(frame_features, global_max_dist=None):
    """è®¡ç®—åŠ¨æ€å˜åŒ–è¯„åˆ†ï¼ˆå…¨å±€å½’ä¸€åŒ–ï¼‰
    
    Args:
        frame_features: çª—å£å†…çš„ç‰¹å¾åºåˆ—
        global_max_dist: å…¨å±€å‚è€ƒæœ€å¤§è·ç¦»ï¼ˆç”¨äºå½’ä¸€åŒ–ï¼‰
    
    Returns:
        å½’ä¸€åŒ–åçš„æœ€å¤§è·ç¦»å€¼
    """
    if frame_features.shape[0] < 2:
        return 0.0
    distances = np.linalg.norm(frame_features[:-1] - frame_features[1:], axis=1)
    max_dist = np.max(distances) if np.max(distances) > 0 else 1e-6
    
    # å¦‚æœæœ‰å…¨å±€å‚è€ƒå€¼ï¼Œç”¨å…¨å±€å‚è€ƒå€¼å½’ä¸€åŒ–ï¼›å¦åˆ™ç”¨å±€éƒ¨æœ€å¤§å€¼
    if global_max_dist is not None and global_max_dist > 0:
        distances_norm = distances / global_max_dist
    else:
        distances_norm = distances / (max_dist + 1e-6)
    
    return float(np.max(distances_norm))


def calculate_complexity(detections_list):
    """è®¡ç®—åœºæ™¯å¤æ‚åº¦"""
    max_objs = 0
    for frame_dets in detections_list:
        if frame_dets.size > 0:
            max_objs = max(max_objs, frame_dets.shape[0])
    return max_objs


# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†: å…¨é‡æ•°æ®åˆ†æ
# ============================================================================

def compute_all_metrics(max_videos=None):
    """è®¡ç®—æ‰€æœ‰è§†é¢‘çš„æŒ‡æ ‡åˆ†å¸ƒï¼ˆä¸¤é˜¶æ®µï¼šå…ˆè®¡ç®—å…¨å±€å‚è€ƒå€¼ï¼Œå†å½’ä¸€åŒ–ï¼‰"""
    
    print("\n" + "="*70)
    print("ç¬¬ä¸€é˜¶æ®µ: è®¡ç®—å…¨é‡è§†é¢‘æŒ‡æ ‡åˆ†å¸ƒï¼ˆå…¨å±€å½’ä¸€åŒ–ï¼‰")
    print("="*70)
    
    annotations = load_annotations(ANNOTATION_FILE)
    
    if not annotations:
        print("âœ— æ— æ³•åŠ è½½æ ‡æ³¨")
        return None, None
    
    # é…ç½®
    CONF_THRESHOLD = 0.5
    TIME_WINDOW = 30
    
    npz_files = sorted([f for f in os.listdir(NPZ_DIR) if f.endswith('.npz')])
    if max_videos:
        npz_files = npz_files[:max_videos]
    
    # ========== ç¬¬ä¸€éï¼šæ”¶é›†æ‰€æœ‰max_distå’Œçª—å£é•¿åº¦ç»Ÿè®¡ ==========
    print("\nğŸ“Š ç¬¬1/2é: æ‰«ææ‰€æœ‰è§†é¢‘è·å–å…¨å±€å‚è€ƒå€¼...")
    max_dists = []
    window_lengths = []
    
    for npz_file in tqdm(npz_files, desc="æ‰«æè§†é¢‘"):
        video_name = npz_file.replace('.npz', '.mp4')
        
        if video_name not in annotations:
            continue
        
        npz_path = os.path.join(NPZ_DIR, npz_file)
        try:
            data = np.load(npz_path)
            detections = data['det']
            features = data['data']
        except:
            continue
        
        accident_frame = annotations[video_name]
        start_frame = max(0, accident_frame - TIME_WINDOW)
        end_frame = min(detections.shape[0], accident_frame + TIME_WINDOW)
        
        # è®°å½•çª—å£é•¿åº¦
        window_len = end_frame - start_frame
        window_lengths.append(window_len)
        
        # å¸§å†…ï¼šå¯¹é«˜ç½®ä¿¡åº¦æ£€æµ‹å–å¹³å‡ç‰¹å¾ï¼ˆä¸ä¸‹æ¸¸ pipeline ä¸€è‡´ï¼‰
        frame_avg_features = []
        for t in range(start_frame, end_frame):
            frame_dets = detections[t]
            if frame_dets.size > 0:
                high_conf_mask = frame_dets[:, 4] > CONF_THRESHOLD
                high_conf_indices = np.where(high_conf_mask)[0]
                if len(high_conf_indices) > 0:
                    frame_feat = np.mean(features[t, high_conf_indices, :], axis=0)
                else:
                    frame_feat = features[t, 0, :]
            else:
                frame_feat = features[t, 0, :]
            frame_avg_features.append(frame_feat)
        feats_window = np.array(frame_avg_features) if len(frame_avg_features) > 0 else np.array([])
        
        # è®¡ç®—è¯¥è§†é¢‘çš„æœ€å¤§è·ç¦»
        if feats_window.shape[0] >= 2:
            distances = np.linalg.norm(feats_window[:-1] - feats_window[1:], axis=1)
            max_dist = np.max(distances)
            if max_dist > 0:
                max_dists.append(max_dist)
    
    # è®¡ç®—å…¨å±€å‚è€ƒå€¼ï¼ˆä½¿ç”¨95åˆ†ä½æ•°é¿å…æç«¯å€¼ï¼‰
    global_max_dist = np.percentile(max_dists, 95) if max_dists else 1.0
    
    print(f"\nğŸ“ˆ çª—å£é•¿åº¦ç»Ÿè®¡:")
    print(f"   æœ€å°: {np.min(window_lengths)}, æœ€å¤§: {np.max(window_lengths)}, å¹³å‡: {np.mean(window_lengths):.1f}")
    print(f"   ä¸­ä½æ•°: {np.median(window_lengths):.0f}, æ ‡å‡†å·®: {np.std(window_lengths):.1f}")
    print(f"\nğŸ¯ åŠ¨æ€å˜åŒ–å…¨å±€å‚è€ƒå€¼:")
    print(f"   95åˆ†ä½æ•°: {global_max_dist:.6f}")
    print(f"   ç”¨äºå½’ä¸€åŒ–çš„å‚è€ƒå€¼: {global_max_dist:.6f}")
    
    # ========== ç¬¬äºŒéï¼šç”¨å…¨å±€å‚è€ƒå€¼é‡æ–°è®¡ç®—æ‰€æœ‰æŒ‡æ ‡ ==========
    print(f"\nğŸ“Š ç¬¬2/2é: è®¡ç®—æ‰€æœ‰æŒ‡æ ‡ï¼ˆä½¿ç”¨å…¨å±€å½’ä¸€åŒ–ï¼‰...")
    results = []
    
    for npz_file in tqdm(npz_files, desc="å¤„ç†è§†é¢‘"):
        video_name = npz_file.replace('.npz', '.mp4')
        
        if video_name not in annotations:
            continue
        
        npz_path = os.path.join(NPZ_DIR, npz_file)
        try:
            data = np.load(npz_path)
            detections = data['det']
            features = data['data']
        except:
            continue
        
        accident_frame = annotations[video_name]
        start_frame = max(0, accident_frame - TIME_WINDOW)
        end_frame = min(detections.shape[0], accident_frame + TIME_WINDOW)
        
        dets_window = [detections[i] for i in range(start_frame, end_frame)]
        
        # å¸§å†…å¹³å‡ç‰¹å¾ï¼ˆä¸å…¨å±€å‚è€ƒä¸€è‡´ï¼‰
        frame_avg_features = []
        for t, frame_dets in enumerate(dets_window):
            idx = start_frame + t
            if frame_dets.size > 0:
                high_conf_mask = frame_dets[:, 4] > CONF_THRESHOLD
                high_conf_indices = np.where(high_conf_mask)[0]
                if len(high_conf_indices) > 0:
                    frame_feat = np.mean(features[idx, high_conf_indices, :], axis=0)
                else:
                    frame_feat = features[idx, 0, :]
            else:
                frame_feat = features[idx, 0, :]
            frame_avg_features.append(frame_feat)
        feats_window = np.array(frame_avg_features) if len(frame_avg_features) > 0 else np.array([])
        
        # ç½®ä¿¡åº¦è¿‡æ»¤ç”¨äºå¤æ‚åº¦
        dets_filtered = []
        for frame_dets in dets_window:
            if frame_dets.size > 0:
                filtered = frame_dets[frame_dets[:, 4] > CONF_THRESHOLD]
                dets_filtered.append(filtered)
            else:
                dets_filtered.append(np.array([]))
        
        dynamic = calculate_metrics(feats_window, global_max_dist=global_max_dist)
        complexity = calculate_complexity(dets_filtered)
        
        results.append({
            'video_name': video_name,
            'accident_frame': accident_frame,
            'dynamic_change': dynamic,
            'scene_complexity': complexity,
            'window_length': end_frame - start_frame
        })
    
    df = pd.DataFrame(results)
    print(f"\nâœ“ æˆåŠŸå¤„ç† {len(df)} ä¸ªè§†é¢‘")
    
    return df, global_max_dist


def analyze_distribution(df):
    """åˆ†ææŒ‡æ ‡åˆ†å¸ƒ"""
    
    print("\n" + "="*70)
    print("ç¬¬äºŒé˜¶æ®µ: æŒ‡æ ‡åˆ†å¸ƒåˆ†æ")
    print("="*70)
    
    # åŠ¨æ€å˜åŒ–åˆ†æ
    print("\nã€åŠ¨æ€å˜åŒ–è¯„åˆ† (Dynamic Change)ã€‘")
    dyn = df['dynamic_change']
    print(f"  ç»Ÿè®¡å€¼:")
    print(f"    æœ€å°å€¼: {dyn.min():.4f}")
    print(f"    æœ€å¤§å€¼: {dyn.max():.4f}")
    print(f"    å¹³å‡å€¼: {dyn.mean():.4f}")
    print(f"    ä¸­ä½æ•°: {dyn.median():.4f}")
    print(f"  åˆ†ä½æ•°:")
    for q in [0.25, 0.5,0.6, 0.7, 0.8, 0.9, 0.95]:
        val = dyn.quantile(q)
        print(f"    P{int(q*100)}: {val:.4f}")
    
    # åœºæ™¯å¤æ‚åº¦åˆ†æ
    print("\nã€åœºæ™¯å¤æ‚åº¦ (Scene Complexity)ã€‘")
    cplx = df['scene_complexity']
    print(f"  ç»Ÿè®¡å€¼:")
    print(f"    æœ€å°å€¼: {cplx.min()}")
    print(f"    æœ€å¤§å€¼: {cplx.max()}")
    print(f"    å¹³å‡å€¼: {cplx.mean():.2f}")
    print(f"    ä¸­ä½æ•°: {cplx.median():.2f}")
    print(f"  åˆ†ä½æ•°:")
    for q in [0.25, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]:
        val = cplx.quantile(q)
        print(f"    P{int(q*100)}: {val:.2f}")
    
    return df


def suggest_thresholds(df):
    """åŸºäºåˆ†ä½æ•°ç»™å‡ºé˜ˆå€¼å»ºè®®"""
    
    print("\n" + "="*70)
    print("ç¬¬ä¸‰é˜¶æ®µ: åŸºäºåˆ†ä½æ•°çš„é˜ˆå€¼å»ºè®®")
    print("="*70)
    
    dyn = df['dynamic_change']
    cplx = df['scene_complexity']
    
    print("\nã€ä¿å®ˆç­–ç•¥ (é«˜ç²¾åº¦ï¼Œä½å¬å›)ã€‘")
    print("  ç›®çš„: åªç­›é€‰é«˜è´¨é‡æ ·æœ¬ï¼Œå®¹è®¸æ¼æ‰éƒ¨åˆ†")
    dynamic_thresh_cons = dyn.quantile(0.8)
    complexity_thresh_cons = cplx.quantile(0.75)
    print(f"    Dynamic >= {dynamic_thresh_cons:.4f} (P80)")
    print(f"    Complexity >= {complexity_thresh_cons:.1f} (P75)")
    
    print("\nã€å¹³è¡¡ç­–ç•¥ (ä¸­ç­‰ç²¾åº¦ï¼Œä¸­ç­‰å¬å›)ã€‘")
    print("  ç›®çš„: å¹³è¡¡å¬å›ä¸ç²¾åº¦ï¼Œæ˜¯æ¨èæ–¹æ¡ˆ")
    dynamic_thresh_bal = dyn.quantile(0.6)
    complexity_thresh_bal = cplx.quantile(0.5)
    print(f"    Dynamic >= {dynamic_thresh_bal:.4f} (P60)")
    print(f"    Complexity >= {complexity_thresh_bal:.1f} (P50)")
    
    print("\nã€æ¿€è¿›ç­–ç•¥ (ä½ç²¾åº¦ï¼Œé«˜å¬å›)ã€‘")
    print("  ç›®çš„: å°½é‡ä¿ç•™æ‰€æœ‰æ½œåœ¨æ ·æœ¬ï¼Œæ¥å—å™ªå£°")
    dynamic_thresh_aggr = dyn.quantile(0.4)
    complexity_thresh_aggr = cplx.quantile(0.25)
    print(f"    Dynamic >= {dynamic_thresh_aggr:.4f} (P40)")
    print(f"    Complexity >= {complexity_thresh_aggr:.1f} (P25)")
    
    suggestions = {
        'conservative': {
            'dynamic_change_threshold': dynamic_thresh_cons,
            'complexity_threshold': complexity_thresh_cons,
        },
        'balanced': {
            'dynamic_change_threshold': dynamic_thresh_bal,
            'complexity_threshold': complexity_thresh_bal,
        },
        'aggressive': {
            'dynamic_change_threshold': dynamic_thresh_aggr,
            'complexity_threshold': complexity_thresh_aggr,
        }
    }
    
    return suggestions


def export_basic_reports(df, suggestions):
    """å¯¼å‡ºåŸºç¡€åˆ†å¸ƒä¸åˆ†ä½æ•°å»ºè®®ï¼Œä¾¿äº pipeline å¤ç”¨"""
    os.makedirs(ANALYSIS_OUTPUT, exist_ok=True)

    dist_report = {
        'dynamic_change': {
            'min': float(df['dynamic_change'].min()),
            'max': float(df['dynamic_change'].max()),
            'mean': float(df['dynamic_change'].mean()),
            'median': float(df['dynamic_change'].median()),
            'std': float(df['dynamic_change'].std()),
            'quantiles': {
                f'p{int(q*100)}': float(df['dynamic_change'].quantile(q))
                for q in [0.25, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]
            }
        },
        'scene_complexity': {
            'min': int(df['scene_complexity'].min()),
            'max': int(df['scene_complexity'].max()),
            'mean': float(df['scene_complexity'].mean()),
            'median': float(df['scene_complexity'].median()),
            'std': float(df['scene_complexity'].std()),
            'quantiles': {
                f'p{int(q*100)}': float(df['scene_complexity'].quantile(q))
                for q in [0.25, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]
            }
        }
    }

    with open(os.path.join(ANALYSIS_OUTPUT, '01_distribution_analysis.json'), 'w') as f:
        json.dump(dist_report, f, indent=2)

    with open(os.path.join(ANALYSIS_OUTPUT, '02_threshold_suggestions.json'), 'w') as f:
        json.dump(suggestions, f, indent=2)

    df.to_csv(os.path.join(ANALYSIS_OUTPUT, '00_raw_metrics.csv'), index=False)


def main():
    print("\n" + "â–ˆ"*70)
    print("â–ˆ" + " "*68 + "â–ˆ")
    print("â–ˆ" + "  ç­›é€‰é˜ˆå€¼åˆ†æï¼ˆDynamic + Complexityï¼‰".center(68) + "â–ˆ")
    print("â–ˆ" + " "*68 + "â–ˆ")
    print("â–ˆ"*70)

    result = compute_all_metrics()
    if result is None or result[0] is None:
        return
    df, _ = result

    analyze_distribution(df)
    suggestions = suggest_thresholds(df)
    export_basic_reports(df, suggestions)

    print("\n" + "="*70)
    print("âœ“ åˆ†æå®Œæˆï¼Œç»“æœå·²å¯¼å‡ºè‡³ threshold_analysis ç›®å½•")
    print("="*70)


if __name__ == '__main__':
    main()
