#!/usr/bin/env python3
"""
é˜ˆå€¼åˆ†æä¸ä¼˜åŒ–è„šæœ¬

åŠŸèƒ½ï¼š
1. è®¡ç®—å…¨é‡æ•°æ®çš„æŒ‡æ ‡åˆ†å¸ƒ
2. åŸºäºåˆ†ä½æ•°ç»™å‡ºé˜ˆå€¼å»ºè®®
3. é˜ˆå€¼æ‰«æï¼Œè®¡ç®—å„é˜ˆå€¼ç»„åˆçš„å¬å›/ç²¾åº¦/F1
4. æ‰¾åˆ°æœ€ä¼˜é˜ˆå€¼å¹¶å¯¼å‡ºå¯¹æ¯”ç»“æœ
"""

import os
import json
import numpy as np
import pandas as pd
from collections import defaultdict
from itertools import product
import sys
from tqdm import tqdm

# ============================================================================
# é…ç½®ä¸è·¯å¾„
# ============================================================================

ROOT_DIR = "/home/24068286g/UString"
CCD_ROOT = os.path.join(ROOT_DIR, 'data', 'crash')
VRU_ROOT = os.path.join(ROOT_DIR, 'VRU')

ANNOTATION_FILE = os.path.join(CCD_ROOT, 'videos', 'Crash-1500.txt')
NPZ_DIR = os.path.join(CCD_ROOT, 'yolo_features', 'positive')
OUTPUT_DIR = os.path.join(VRU_ROOT, 'output')
ANALYSIS_OUTPUT = os.path.join(VRU_ROOT, 'threshold_analysis')

os.makedirs(ANALYSIS_OUTPUT, exist_ok=True)

# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†: æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# ============================================================================

def load_annotations(file_path):
    """åŠ è½½äº‹æ•…æ ‡æ³¨"""
    annotations = {}
    try:
        with open(file_path, 'r') as f:
            for line in f:
                try:
                    vid_id = line[:6]
                    start = line.find('[')
                    end = line.find(']')
                    if start != -1 and end != -1:
                        labels = [int(x.strip()) for x in line[start+1:end].split(',')]
                        accident_frame = labels.index(1)
                        annotations[f"{vid_id}.mp4"] = accident_frame
                except:
                    continue
    except FileNotFoundError:
        print(f"âœ— æ ‡æ³¨æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
    return annotations


def load_ground_truth():
    """åŠ è½½äººå·¥æ ‡æ³¨çš„çœŸå€¼æ ‡ç­¾"""
    ground_truth = {}
    json_path = os.path.join(OUTPUT_DIR, 'filtered_videos_analysis.json')
    try:
        with open(json_path, 'r') as f:
            data = json.load(f)
        for video in data:
            ground_truth[video['video_name']] = video.get('human_judgement', 0)
    except FileNotFoundError:
        print(f"âœ— åˆ†ææ–‡ä»¶æœªæ‰¾åˆ°: {json_path}")
    return ground_truth


def calculate_metrics(frame_features, global_max_dist=None):
    """è®¡ç®—åŠ¨æ€å˜åŒ–è¯„åˆ†ï¼ˆå…¨å±€å½’ä¸€åŒ–ï¼‰
    
    Args:
        frame_features: çª—å£å†…çš„ç‰¹å¾åºåˆ—
        global_max_dist: å…¨å±€å‚è€ƒæœ€å¤§è·ç¦»ï¼ˆç”¨äºå½’ä¸€åŒ–ï¼‰
    
    Returns:
        å½’ä¸€åŒ–åçš„æœ€å¤§è·ç¦»å€¼
    """
    if frame_features.shape[0] < 2:
        return 0.0
    distances = np.linalg.norm(frame_features[:-1] - frame_features[1:], axis=1)
    max_dist = np.max(distances) if np.max(distances) > 0 else 1e-6
    
    # å¦‚æœæœ‰å…¨å±€å‚è€ƒå€¼ï¼Œç”¨å…¨å±€å‚è€ƒå€¼å½’ä¸€åŒ–ï¼›å¦åˆ™ç”¨å±€éƒ¨æœ€å¤§å€¼
    if global_max_dist is not None and global_max_dist > 0:
        distances_norm = distances / global_max_dist
    else:
        distances_norm = distances / (max_dist + 1e-6)
    
    return float(np.max(distances_norm))


def calculate_complexity(detections_list):
    """è®¡ç®—åœºæ™¯å¤æ‚åº¦"""
    max_objs = 0
    for frame_dets in detections_list:
        if frame_dets.size > 0:
            max_objs = max(max_objs, frame_dets.shape[0])
    return max_objs


def check_vru_interaction(detections_list, vru_ids, car_ids):
    """æ£€æµ‹VRUäº¤äº’"""
    for frame_dets in detections_list:
        if frame_dets.size > 0:
            classes = set(int(obj[5]) for obj in frame_dets)
            if (classes & vru_ids) and (classes & car_ids):
                return True
    return False


# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†: å…¨é‡æ•°æ®åˆ†æ
# ============================================================================

def compute_all_metrics(max_videos=None):
    """è®¡ç®—æ‰€æœ‰è§†é¢‘çš„æŒ‡æ ‡åˆ†å¸ƒï¼ˆä¸¤é˜¶æ®µï¼šå…ˆè®¡ç®—å…¨å±€å‚è€ƒå€¼ï¼Œå†å½’ä¸€åŒ–ï¼‰"""
    
    print("\n" + "="*70)
    print("ç¬¬ä¸€é˜¶æ®µ: è®¡ç®—å…¨é‡è§†é¢‘æŒ‡æ ‡åˆ†å¸ƒï¼ˆå…¨å±€å½’ä¸€åŒ–ï¼‰")
    print("="*70)
    
    annotations = load_annotations(ANNOTATION_FILE)
    ground_truth = load_ground_truth()
    
    if not annotations:
        print("âœ— æ— æ³•åŠ è½½æ ‡æ³¨")
        return None, None
    
    # é…ç½®
    VRU_IDS = {0, 1, 3}
    CAR_IDS = {2, 5, 7}
    CONF_THRESHOLD = 0.5
    TIME_WINDOW = 30
    
    npz_files = sorted([f for f in os.listdir(NPZ_DIR) if f.endswith('.npz')])
    if max_videos:
        npz_files = npz_files[:max_videos]
    
    # ========== ç¬¬ä¸€éï¼šæ”¶é›†æ‰€æœ‰max_distå’Œçª—å£é•¿åº¦ç»Ÿè®¡ ==========
    print("\nğŸ“Š ç¬¬1/2é: æ‰«ææ‰€æœ‰è§†é¢‘è·å–å…¨å±€å‚è€ƒå€¼...")
    max_dists = []
    window_lengths = []
    
    for npz_file in tqdm(npz_files, desc="æ‰«æè§†é¢‘"):
        video_name = npz_file.replace('.npz', '.mp4')
        
        if video_name not in annotations:
            continue
        
        npz_path = os.path.join(NPZ_DIR, npz_file)
        try:
            data = np.load(npz_path)
            detections = data['det']
            features = data['data']
        except:
            continue
        
        accident_frame = annotations[video_name]
        start_frame = max(0, accident_frame - TIME_WINDOW)
        end_frame = min(detections.shape[0], accident_frame + TIME_WINDOW)
        
        # è®°å½•çª—å£é•¿åº¦
        window_len = end_frame - start_frame
        window_lengths.append(window_len)
        
        # æå–ç‰¹å¾çª—å£
        feats_window = features[start_frame:end_frame, 0, :]
        
        # è®¡ç®—è¯¥è§†é¢‘çš„æœ€å¤§è·ç¦»
        if feats_window.shape[0] >= 2:
            distances = np.linalg.norm(feats_window[:-1] - feats_window[1:], axis=1)
            max_dist = np.max(distances)
            if max_dist > 0:
                max_dists.append(max_dist)
    
    # è®¡ç®—å…¨å±€å‚è€ƒå€¼ï¼ˆä½¿ç”¨95åˆ†ä½æ•°é¿å…æç«¯å€¼ï¼‰
    global_max_dist = np.percentile(max_dists, 95) if max_dists else 1.0
    
    print(f"\nğŸ“ˆ çª—å£é•¿åº¦ç»Ÿè®¡:")
    print(f"   æœ€å°: {np.min(window_lengths)}, æœ€å¤§: {np.max(window_lengths)}, å¹³å‡: {np.mean(window_lengths):.1f}")
    print(f"   ä¸­ä½æ•°: {np.median(window_lengths):.0f}, æ ‡å‡†å·®: {np.std(window_lengths):.1f}")
    print(f"\nğŸ¯ åŠ¨æ€å˜åŒ–å…¨å±€å‚è€ƒå€¼:")
    print(f"   95åˆ†ä½æ•°: {global_max_dist:.6f}")
    print(f"   ç”¨äºå½’ä¸€åŒ–çš„å‚è€ƒå€¼: {global_max_dist:.6f}")
    
    # ========== ç¬¬äºŒéï¼šç”¨å…¨å±€å‚è€ƒå€¼é‡æ–°è®¡ç®—æ‰€æœ‰æŒ‡æ ‡ ==========
    print(f"\nğŸ“Š ç¬¬2/2é: è®¡ç®—æ‰€æœ‰æŒ‡æ ‡ï¼ˆä½¿ç”¨å…¨å±€å½’ä¸€åŒ–ï¼‰...")
    results = []
    
    for npz_file in tqdm(npz_files, desc="å¤„ç†è§†é¢‘"):
        video_name = npz_file.replace('.npz', '.mp4')
        
        if video_name not in annotations:
            continue
        
        npz_path = os.path.join(NPZ_DIR, npz_file)
        try:
            data = np.load(npz_path)
            detections = data['det']
            features = data['data']
        except:
            continue
        
        accident_frame = annotations[video_name]
        start_frame = max(0, accident_frame - TIME_WINDOW)
        end_frame = min(detections.shape[0], accident_frame + TIME_WINDOW)
        
        # æå–çª—å£æ•°æ®
        dets_window = [detections[i] for i in range(start_frame, end_frame)]
        feats_window = features[start_frame:end_frame, 0, :]
        
        # ç½®ä¿¡åº¦è¿‡æ»¤
        dets_filtered = []
        for frame_dets in dets_window:
            if frame_dets.size > 0:
                filtered = frame_dets[frame_dets[:, 4] > CONF_THRESHOLD]
                dets_filtered.append(filtered)
            else:
                dets_filtered.append(np.array([]))
        
        # è®¡ç®—æŒ‡æ ‡ï¼ˆä½¿ç”¨å…¨å±€å‚è€ƒå€¼ï¼‰
        dynamic = calculate_metrics(feats_window, global_max_dist=global_max_dist)
        complexity = calculate_complexity(dets_filtered)
        has_vru = check_vru_interaction(dets_filtered, VRU_IDS, CAR_IDS)
        
        # è·å–çœŸå€¼æ ‡ç­¾
        label = ground_truth.get(video_name, 0)
        
        results.append({
            'video_name': video_name,
            'accident_frame': accident_frame,
            'dynamic_change': dynamic,
            'scene_complexity': complexity,
            'has_vru_interaction': int(has_vru),
            'human_judgement': label,
            'window_length': end_frame - start_frame
        })
    
    df = pd.DataFrame(results)
    print(f"\nâœ“ æˆåŠŸå¤„ç† {len(df)} ä¸ªè§†é¢‘")
    
    return df, global_max_dist


def analyze_distribution(df):
    """åˆ†ææŒ‡æ ‡åˆ†å¸ƒ"""
    
    print("\n" + "="*70)
    print("ç¬¬äºŒé˜¶æ®µ: æŒ‡æ ‡åˆ†å¸ƒåˆ†æ")
    print("="*70)
    
    # åŠ¨æ€å˜åŒ–åˆ†æ
    print("\nã€åŠ¨æ€å˜åŒ–è¯„åˆ† (Dynamic Change)ã€‘")
    dyn = df['dynamic_change']
    print(f"  ç»Ÿè®¡å€¼:")
    print(f"    æœ€å°å€¼: {dyn.min():.4f}")
    print(f"    æœ€å¤§å€¼: {dyn.max():.4f}")
    print(f"    å¹³å‡å€¼: {dyn.mean():.4f}")
    print(f"    ä¸­ä½æ•°: {dyn.median():.4f}")
    print(f"  åˆ†ä½æ•°:")
    for q in [0.25, 0.5, 0.7, 0.8, 0.9, 0.95]:
        val = dyn.quantile(q)
        print(f"    P{int(q*100)}: {val:.4f}")
    
    # åœºæ™¯å¤æ‚åº¦åˆ†æ
    print("\nã€åœºæ™¯å¤æ‚åº¦ (Scene Complexity)ã€‘")
    cplx = df['scene_complexity']
    print(f"  ç»Ÿè®¡å€¼:")
    print(f"    æœ€å°å€¼: {cplx.min()}")
    print(f"    æœ€å¤§å€¼: {cplx.max()}")
    print(f"    å¹³å‡å€¼: {cplx.mean():.2f}")
    print(f"    ä¸­ä½æ•°: {cplx.median():.2f}")
    print(f"  åˆ†ä½æ•°:")
    for q in [0.25, 0.5, 0.7, 0.8, 0.9, 0.95]:
        val = cplx.quantile(q)
        print(f"    P{int(q*100)}: {val:.2f}")
    
    # VRUäº¤äº’åˆ†æ
    print("\nã€VRUäº¤äº’æ£€æµ‹ (Has VRU Interaction)ã€‘")
    vru_count = df['has_vru_interaction'].sum()
    print(f"  æœ‰VRUäº¤äº’: {vru_count} ({vru_count/len(df)*100:.1f}%)")
    print(f"  æ— VRUäº¤äº’: {len(df)-vru_count} ({(len(df)-vru_count)/len(df)*100:.1f}%)")
    
    # çœŸå€¼æ ‡ç­¾åˆ†æ
    print("\nã€äººå·¥æ ‡æ³¨æ ‡ç­¾ (Human Judgement)ã€‘")
    label_counts = df['human_judgement'].value_counts()
    for label in sorted(label_counts.index):
        count = label_counts[label]
        print(f"  Label {label}: {count} ({count/len(df)*100:.1f}%)")
    
    # åŸºäºçœŸå€¼çš„æŒ‡æ ‡åˆ†å¸ƒ
    print("\nã€æŒ‰äººå·¥æ ‡æ³¨åˆ†å±‚çš„æŒ‡æ ‡ã€‘")
    for label in [0, 1]:
        subset = df[df['human_judgement'] == label]
        if len(subset) == 0:
            continue
        print(f"\n  Label={label} (n={len(subset)}):")
        print(f"    åŠ¨æ€å˜åŒ–: {subset['dynamic_change'].mean():.4f} Â± {subset['dynamic_change'].std():.4f}")
        print(f"    åœºæ™¯å¤æ‚åº¦: {subset['scene_complexity'].mean():.2f} Â± {subset['scene_complexity'].std():.2f}")
        print(f"    æœ‰VRU: {subset['has_vru_interaction'].sum() / len(subset) * 100:.1f}%")
    
    return df


def suggest_thresholds(df):
    """åŸºäºåˆ†ä½æ•°ç»™å‡ºé˜ˆå€¼å»ºè®®"""
    
    print("\n" + "="*70)
    print("ç¬¬ä¸‰é˜¶æ®µ: åŸºäºåˆ†ä½æ•°çš„é˜ˆå€¼å»ºè®®")
    print("="*70)
    
    dyn = df['dynamic_change']
    cplx = df['scene_complexity']
    
    print("\nã€ä¿å®ˆç­–ç•¥ (é«˜ç²¾åº¦ï¼Œä½å¬å›)ã€‘")
    print("  ç›®çš„: åªç­›é€‰é«˜è´¨é‡æ ·æœ¬ï¼Œå®¹è®¸æ¼æ‰éƒ¨åˆ†")
    dynamic_thresh_cons = dyn.quantile(0.8)
    complexity_thresh_cons = cplx.quantile(0.75)
    print(f"    Dynamic >= {dynamic_thresh_cons:.4f} (P80)")
    print(f"    Complexity >= {complexity_thresh_cons:.1f} (P75)")
    
    print("\nã€å¹³è¡¡ç­–ç•¥ (ä¸­ç­‰ç²¾åº¦ï¼Œä¸­ç­‰å¬å›)ã€‘")
    print("  ç›®çš„: å¹³è¡¡å¬å›ä¸ç²¾åº¦ï¼Œæ˜¯æ¨èæ–¹æ¡ˆ")
    dynamic_thresh_bal = dyn.quantile(0.6)
    complexity_thresh_bal = cplx.quantile(0.5)
    print(f"    Dynamic >= {dynamic_thresh_bal:.4f} (P60)")
    print(f"    Complexity >= {complexity_thresh_bal:.1f} (P50)")
    
    print("\nã€æ¿€è¿›ç­–ç•¥ (ä½ç²¾åº¦ï¼Œé«˜å¬å›)ã€‘")
    print("  ç›®çš„: å°½é‡ä¿ç•™æ‰€æœ‰æ½œåœ¨æ ·æœ¬ï¼Œæ¥å—å™ªå£°")
    dynamic_thresh_aggr = dyn.quantile(0.4)
    complexity_thresh_aggr = cplx.quantile(0.25)
    print(f"    Dynamic >= {dynamic_thresh_aggr:.4f} (P40)")
    print(f"    Complexity >= {complexity_thresh_aggr:.1f} (P25)")
    
    suggestions = {
        'conservative': {
            'dynamic_change_threshold': dynamic_thresh_cons,
            'complexity_threshold': complexity_thresh_cons,
        },
        'balanced': {
            'dynamic_change_threshold': dynamic_thresh_bal,
            'complexity_threshold': complexity_thresh_bal,
        },
        'aggressive': {
            'dynamic_change_threshold': dynamic_thresh_aggr,
            'complexity_threshold': complexity_thresh_aggr,
        }
    }
    
    return suggestions


# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†: é˜ˆå€¼æ‰«æä¸æ€§èƒ½è¯„ä¼°
# ============================================================================

def threshold_sweep(df):
    """é˜ˆå€¼æ‰«æï¼Œè®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
    
    print("\n" + "="*70)
    print("ç¬¬å››é˜¶æ®µ: é˜ˆå€¼æ‰«æä¸æ€§èƒ½è¯„ä¼°")
    print("="*70)
    
    # ç”Ÿæˆæ‰«æèŒƒå›´
    dynamic_thresholds = np.arange(0.0, 1.1, 0.1)
    complexity_thresholds = range(3, 15, 1)
    
    results = []
    
    print(f"\næ‰«æèŒƒå›´: {len(dynamic_thresholds)} Ã— {len(complexity_thresholds)} = {len(dynamic_thresholds)*len(complexity_thresholds)} ä¸ªç»„åˆ")
    print("è®¡ç®—ä¸­...")
    
    for dyn_th in dynamic_thresholds:
        for cplx_th in complexity_thresholds:
            # åº”ç”¨é˜ˆå€¼: è‡³å°‘æ»¡è¶³1ä¸ªæ¡ä»¶
            predicted = ((df['dynamic_change'] >= dyn_th) | (df['scene_complexity'] >= cplx_th)).astype(int)
            
            # è®¡ç®—æŒ‡æ ‡
            tp = ((predicted == 1) & (df['human_judgement'] == 1)).sum()
            fp = ((predicted == 1) & (df['human_judgement'] == 0)).sum()
            fn = ((predicted == 0) & (df['human_judgement'] == 1)).sum()
            tn = ((predicted == 0) & (df['human_judgement'] == 0)).sum()
            
            # è®¡ç®—æ€§èƒ½
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            f1 = 2 * recall * precision / (recall + precision) if (recall + precision) > 0 else 0
            
            results.append({
                'dynamic_threshold': dyn_th,
                'complexity_threshold': cplx_th,
                'tp': tp,
                'fp': fp,
                'fn': fn,
                'tn': tn,
                'recall': recall,
                'precision': precision,
                'f1': f1,
                'predicted_positive': tp + fp,
                'actual_positive': tp + fn
            })
    
    results_df = pd.DataFrame(results)
    
    # æ‰¾æœ€ä¼˜é˜ˆå€¼
    print("\nã€æœ€ä¼˜é˜ˆå€¼ (åŸºäºF1)ã€‘")
    best_f1_idx = results_df['f1'].idxmax()
    best_f1_row = results_df.loc[best_f1_idx]
    
    print(f"  Dynamic Threshold: {best_f1_row['dynamic_threshold']:.1f}")
    print(f"  Complexity Threshold: {int(best_f1_row['complexity_threshold'])}")
    print(f"  æ€§èƒ½æŒ‡æ ‡:")
    print(f"    Recall: {best_f1_row['recall']:.4f}")
    print(f"    Precision: {best_f1_row['precision']:.4f}")
    print(f"    F1: {best_f1_row['f1']:.4f}")
    print(f"    TP/FP/FN: {int(best_f1_row['tp'])}/{int(best_f1_row['fp'])}/{int(best_f1_row['fn'])}")
    
    # æ˜¾ç¤ºTop-10
    print("\nã€Top-10 æœ€ä¼˜é˜ˆå€¼ç»„åˆ (æŒ‰F1)ã€‘")
    top_k = results_df.nlargest(10, 'f1')[['dynamic_threshold', 'complexity_threshold', 'recall', 'precision', 'f1']]
    for idx, row in top_k.iterrows():
        print(f"  Dyn={row['dynamic_threshold']:.1f}, Cplx={int(row['complexity_threshold'])}: "
              f"R={row['recall']:.3f}, P={row['precision']:.3f}, F1={row['f1']:.3f}")
    
    return results_df, best_f1_row


# ============================================================================
# ç¬¬å››éƒ¨åˆ†: ç»“æœå¯¹æ¯”ä¸å¯¼å‡º
# ============================================================================

def compare_thresholds(df, current_config, new_config):
    """å¯¹æ¯”æ–°æ—§é˜ˆå€¼çš„ç­›é€‰ç»“æœ"""
    
    print("\n" + "="*70)
    print("ç¬¬äº”é˜¶æ®µ: æ–°æ—§é˜ˆå€¼å¯¹æ¯”")
    print("="*70)
    
    # åº”ç”¨æ—§é˜ˆå€¼
    old_pred = ((df['dynamic_change'] >= current_config['dynamic']) | 
                (df['scene_complexity'] >= current_config['complexity'])).astype(int)
    
    # åº”ç”¨æ–°é˜ˆå€¼
    new_pred = ((df['dynamic_change'] >= new_config['dynamic']) | 
                (df['scene_complexity'] >= new_config['complexity'])).astype(int)
    
    # ç»Ÿè®¡
    old_pass = (old_pred == 1).sum()
    new_pass = (new_pred == 1).sum()
    old_correct = ((old_pred == 1) & (df['human_judgement'] == 1)).sum()
    new_correct = ((new_pred == 1) & (df['human_judgement'] == 1)).sum()
    
    print(f"\nã€å½“å‰é˜ˆå€¼ã€‘")
    print(f"  Dynamic >= {current_config['dynamic']:.1f}, Complexity >= {current_config['complexity']}")
    print(f"  ç­›é€‰æ•°: {old_pass} ({old_pass/len(df)*100:.1f}%)")
    print(f"  äººå·¥é€šè¿‡çš„: {old_correct}/{old_pass if old_pass > 0 else 1} "
          f"({old_correct/old_pass*100 if old_pass > 0 else 0:.1f}%)")
    
    print(f"\nã€å»ºè®®æ–°é˜ˆå€¼ã€‘")
    print(f"  Dynamic >= {new_config['dynamic']:.1f}, Complexity >= {new_config['complexity']:.0f}")
    print(f"  ç­›é€‰æ•°: {new_pass} ({new_pass/len(df)*100:.1f}%)")
    print(f"  äººå·¥é€šè¿‡çš„: {new_correct}/{new_pass if new_pass > 0 else 1} "
          f"({new_correct/new_pass*100 if new_pass > 0 else 0:.1f}%)")
    
    print(f"\nã€å˜åŒ–åˆ†æã€‘")
    print(f"  ç­›é€‰æ•°å˜åŒ–: {new_pass - old_pass:+d} ({(new_pass-old_pass)/old_pass*100:+.1f}%)")
    print(f"  ç²¾åº¦å˜åŒ–: {new_correct/new_pass if new_pass > 0 else 0:.1f}% "
          f"(vs {old_correct/old_pass if old_pass > 0 else 0:.1f}%)")
    
    # ç»†èŠ‚åˆ†æ
    improved = ((old_pred == 0) & (new_pred == 1) & (df['human_judgement'] == 1))
    worsened = ((old_pred == 1) & (new_pred == 0) & (df['human_judgement'] == 1))
    extra_fp = ((old_pred == 0) & (new_pred == 1) & (df['human_judgement'] == 0))
    
    print(f"\nã€ç»†èŠ‚åˆ†æã€‘")
    print(f"  æ–°å¢å‘ç°(True Positive): {improved.sum()}")
    print(f"  è¯¯åˆ é™¤(False Negative): {worsened.sum()}")
    print(f"  æ–°å¢è¯¯æŠ¥(False Positive): {extra_fp.sum()}")
    
    return {
        'old': {
            'predictions': old_pred,
            'pass_count': old_pass,
            'correct_count': old_correct
        },
        'new': {
            'predictions': new_pred,
            'pass_count': new_pass,
            'correct_count': new_correct
        }
    }


def export_results(df, results_df, best_config, comparison, output_dir):
    """å¯¼å‡ºåˆ†æç»“æœ"""
    
    print("\n" + "="*70)
    print("ç¬¬å…­é˜¶æ®µ: å¯¼å‡ºç»“æœ")
    print("="*70)
    
    os.makedirs(output_dir, exist_ok=True)
    
    # å¯¼å‡ºåˆ†å¸ƒåˆ†æ
    dist_report = {
        'dynamic_change': {
            'min': float(df['dynamic_change'].min()),
            'max': float(df['dynamic_change'].max()),
            'mean': float(df['dynamic_change'].mean()),
            'median': float(df['dynamic_change'].median()),
            'std': float(df['dynamic_change'].std()),
            'quantiles': {
                f'p{int(q*100)}': float(df['dynamic_change'].quantile(q))
                for q in [0.25, 0.5, 0.7, 0.8, 0.9, 0.95]
            }
        },
        'scene_complexity': {
            'min': int(df['scene_complexity'].min()),
            'max': int(df['scene_complexity'].max()),
            'mean': float(df['scene_complexity'].mean()),
            'median': float(df['scene_complexity'].median()),
            'std': float(df['scene_complexity'].std()),
            'quantiles': {
                f'p{int(q*100)}': float(df['scene_complexity'].quantile(q))
                for q in [0.25, 0.5, 0.7, 0.8, 0.9, 0.95]
            }
        }
    }
    
    with open(os.path.join(output_dir, '01_distribution_analysis.json'), 'w') as f:
        json.dump(dist_report, f, indent=2)
    print(f"âœ“ å·²å¯¼å‡º: 01_distribution_analysis.json")
    
    # å¯¼å‡ºé˜ˆå€¼æ‰«æç»“æœ
    results_df.to_csv(os.path.join(output_dir, '02_threshold_sweep_results.csv'), index=False)
    print(f"âœ“ å·²å¯¼å‡º: 02_threshold_sweep_results.csv ({len(results_df)} è¡Œ)")
    
    # å¯¼å‡ºæœ€ä¼˜é˜ˆå€¼é…ç½®
    best_config_dict = {
        'dynamic_change_threshold': float(best_config['dynamic_threshold']),
        'scene_complexity_threshold': float(best_config['complexity_threshold']),
        'recall': float(best_config['recall']),
        'precision': float(best_config['precision']),
        'f1': float(best_config['f1']),
        'true_positive': int(best_config['tp']),
        'false_positive': int(best_config['fp']),
        'false_negative': int(best_config['fn'])
    }
    
    with open(os.path.join(output_dir, '03_optimal_config.json'), 'w') as f:
        json.dump(best_config_dict, f, indent=2)
    print(f"âœ“ å·²å¯¼å‡º: 03_optimal_config.json")
    
    # å¯¼å‡ºæ–°é˜ˆå€¼çš„ç­›é€‰ç»“æœ
    new_pred_videos = df[comparison['new']['predictions'] == 1].copy()
    new_pred_videos.to_csv(os.path.join(output_dir, '04_new_threshold_filtered_videos.csv'), index=False)
    print(f"âœ“ å·²å¯¼å‡º: 04_new_threshold_filtered_videos.csv ({len(new_pred_videos)} æ¡)")
    
    # å¯¼å‡ºå¯¹æ¯”æŠ¥å‘Š
    with open(os.path.join(output_dir, '05_comparison_report.txt'), 'w') as f:
        f.write("ç­›é€‰é˜ˆå€¼ä¼˜åŒ–æŠ¥å‘Š\n")
        f.write("="*70 + "\n\n")
        
        f.write("ã€å½“å‰é…ç½®ã€‘\n")
        f.write(f"Dynamic >= 1.0, Complexity >= 6\n")
        f.write(f"ç­›é€‰æ•°: {comparison['old']['pass_count']}\n")
        f.write(f"å‡†ç¡®ç‡: {comparison['old']['correct_count']}/{comparison['old']['pass_count']} "
                f"({comparison['old']['correct_count']/comparison['old']['pass_count']*100:.1f}%)\n\n")
        
        f.write("ã€æœ€ä¼˜é…ç½® (åŸºäºF1)ã€‘\n")
        f.write(f"Dynamic >= {best_config['dynamic_threshold']:.1f}, "
                f"Complexity >= {int(best_config['complexity_threshold'])}\n")
        f.write(f"ç­›é€‰æ•°: {comparison['new']['pass_count']}\n")
        f.write(f"å‡†ç¡®ç‡: {comparison['new']['correct_count']}/{comparison['new']['pass_count']} "
                f"({comparison['new']['correct_count']/comparison['new']['pass_count']*100:.1f}%)\n")
        f.write(f"Recall: {best_config['recall']:.4f}\n")
        f.write(f"Precision: {best_config['precision']:.4f}\n")
        f.write(f"F1: {best_config['f1']:.4f}\n\n")
        
        f.write("ã€æ€§èƒ½å¯¹æ¯”ã€‘\n")
        f.write(f"ç­›é€‰æ•°å˜åŒ–: {comparison['new']['pass_count'] - comparison['old']['pass_count']:+d}\n")
        f.write(f"ç²¾åº¦å˜åŒ–: {comparison['new']['correct_count']/comparison['new']['pass_count']*100:.1f}% "
                f"(vs {comparison['old']['correct_count']/comparison['old']['pass_count']*100:.1f}%)\n")
    
    print(f"âœ“ å·²å¯¼å‡º: 05_comparison_report.txt")
    
    print(f"\nâœ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³: {output_dir}")


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

def main():
    print("\n" + "â–ˆ"*70)
    print("â–ˆ" + " "*68 + "â–ˆ")
    print("â–ˆ" + "  ç­›é€‰é˜ˆå€¼åˆ†æä¸ä¼˜åŒ–ï¼ˆå…¨å±€å½’ä¸€åŒ–ï¼‰".center(68) + "â–ˆ")
    print("â–ˆ" + " "*68 + "â–ˆ")
    print("â–ˆ"*70)
    
    # Step 1: è®¡ç®—å…¨é‡æŒ‡æ ‡ï¼ˆè¿”å›dfå’Œå…¨å±€å‚è€ƒå€¼ï¼‰
    result = compute_all_metrics()
    if result is None or result[0] is None:
        return
    df, global_max_dist = result
    
    # Step 2: åˆ†æåˆ†å¸ƒ
    analyze_distribution(df)
    
    # Step 3: ç»™å‡ºå»ºè®®
    suggestions = suggest_thresholds(df)
    
    # Step 4: é˜ˆå€¼æ‰«æ
    results_df, best_f1_row = threshold_sweep(df)
    
    # Step 5: å¯¹æ¯”
    current_config = {'dynamic': 1.0, 'complexity': 6}
    new_config = {
        'dynamic': best_f1_row['dynamic_threshold'],
        'complexity': best_f1_row['complexity_threshold']
    }
    
    comparison = compare_thresholds(df, current_config, new_config)
    
    # Step 6: å¯¼å‡º
    export_results(df, results_df, best_f1_row, comparison, ANALYSIS_OUTPUT)
    
    print("\n" + "="*70)
    print("âœ“ åˆ†æå®Œæˆï¼")
    print("="*70)


if __name__ == '__main__':
    main()
