#!/usr/bin/env python3
"""
æ— ç›‘ç£é˜ˆå€¼æ¢ç´¢ - åŸºäºç‰¹å¾åˆ†å¸ƒè€Œä¸ä¾èµ–human_judgement

æ–¹æ³•ï¼š
1. åˆ†æ Dynamic Change å’Œ Scene Complexity çš„åˆ†å¸ƒ
2. ç”¨æ— ç›‘ç£æ–¹æ³•ï¼ˆSilhouette, Elbow, åˆ†å¸ƒå³°å€¼ï¼‰æ‰¾æœ€ä¼˜é˜ˆå€¼
3. è¾“å‡ºä¸åŒé˜ˆå€¼ä¸‹çš„æŒ‡æ ‡å’Œæ ·æœ¬ç‰¹å¾
4. è®¡ç®—åŒç»´åº¦ç­›é€‰çš„æå‡ç‡
"""

import os
import json
import numpy as np
import pandas as pd
from collections import defaultdict
from scipy import stats
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
from tqdm import tqdm

# ============================================================================
# é…ç½®ä¸è·¯å¾„
# ============================================================================

ROOT_DIR = "/home/24068286g/UString"
CCD_ROOT = os.path.join(ROOT_DIR, 'data', 'crash')
VRU_ROOT = os.path.join(ROOT_DIR, 'VRU')

ANNOTATION_FILE = os.path.join(CCD_ROOT, 'videos', 'Crash-1500.txt')
NPZ_DIR = os.path.join(CCD_ROOT, 'yolo_features', 'positive')
OUTPUT_DIR = os.path.join(VRU_ROOT, 'output')
ANALYSIS_OUTPUT = os.path.join(VRU_ROOT, 'threshold_analysis')

os.makedirs(ANALYSIS_OUTPUT, exist_ok=True)

# ============================================================================
# æ•°æ®åŠ è½½
# ============================================================================

def load_annotations(file_path):
    annotations = {}
    try:
        with open(file_path, 'r') as f:
            for line in f:
                try:
                    vid_id = line[:6]
                    start = line.find('[')
                    end = line.find(']')
                    if start != -1 and end != -1:
                        labels = [int(x.strip()) for x in line[start+1:end].split(',')]
                        accident_frame = labels.index(1)
                        annotations[f"{vid_id}.mp4"] = accident_frame
                except:
                    continue
    except FileNotFoundError:
        pass
    return annotations


def load_metrics_from_json():
    """ä»å‰é¢ç”Ÿæˆçš„åˆ†æç»“æœä¸­åŠ è½½æ•°æ®"""
    json_path = os.path.join(ANALYSIS_OUTPUT, '01_distribution_analysis.json')
    try:
        with open(json_path, 'r') as f:
            return json.load(f)
    except:
        return None


def compute_metrics_with_global_normalization():
    """é‡æ–°è®¡ç®—æ‰€æœ‰æŒ‡æ ‡ï¼ˆä½¿ç”¨å…¨å±€å½’ä¸€åŒ–ï¼‰"""
    
    annotations = load_annotations(ANNOTATION_FILE)
    if not annotations:
        print("âœ— æ— æ³•åŠ è½½æ ‡æ³¨")
        return None
    
    # é…ç½®
    CONF_THRESHOLD = 0.5
    TIME_WINDOW = 30
    
    npz_files = sorted([f for f in os.listdir(NPZ_DIR) if f.endswith('.npz')])
    
    # ç¬¬ä¸€éï¼šæ”¶é›†å…¨å±€å‚è€ƒå€¼
    print("ğŸ“Š æ‰«ææ‰€æœ‰è§†é¢‘è·å–å…¨å±€å‚è€ƒå€¼...")
    max_dists = []
    
    for npz_file in tqdm(npz_files, desc="ç¬¬1/2é"):
        video_name = npz_file.replace('.npz', '.mp4')
        if video_name not in annotations:
            continue
        
        npz_path = os.path.join(NPZ_DIR, npz_file)
        try:
            data = np.load(npz_path)
            detections = data['det']
            features = data['data']
        except:
            continue
        
        accident_frame = annotations[video_name]
        start_frame = max(0, accident_frame - TIME_WINDOW)
        end_frame = min(features.shape[0], accident_frame + TIME_WINDOW)
        
        dets_window = [detections[i] for i in range(start_frame, end_frame)]
        
        # å¸§å†…å¹³å‡ï¼šå¯¹æ¯å¸§çš„é«˜ç½®ä¿¡åº¦æ£€æµ‹ç‰¹å¾æ±‚å¹³å‡
        frame_avg_features = []
        for t, frame_dets in enumerate(dets_window):
            if features.shape[0] <= start_frame + t:
                break
            # è·å–è¯¥å¸§æ‰€æœ‰é«˜ç½®ä¿¡åº¦æ£€æµ‹
            if frame_dets.size > 0:
                high_conf_mask = frame_dets[:, 4] > CONF_THRESHOLD
                high_conf_dets_indices = np.where(high_conf_mask)[0]
                if len(high_conf_dets_indices) > 0:
                    # è¯¥å¸§çš„ç‰¹å¾ä¸ºæ‰€æœ‰é«˜ç½®ä¿¡åº¦æ£€æµ‹ç‰¹å¾çš„å¹³å‡
                    frame_feat = np.mean(features[start_frame + t, high_conf_dets_indices, :], axis=0)
                else:
                    # æ— é«˜ç½®ä¿¡åº¦æ£€æµ‹ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ£€æµ‹ä½œä¸ºåå¤‡
                    frame_feat = features[start_frame + t, 0, :]
            else:
                frame_feat = features[start_frame + t, 0, :]
            frame_avg_features.append(frame_feat)
        
        feats_window = np.array(frame_avg_features) if len(frame_avg_features) > 0 else np.array([])
        
        # å¸§é—´è®¡ç®—ï¼šç›¸é‚»å¸§ç‰¹å¾è·ç¦»
        if feats_window.shape[0] >= 2:
            distances = np.linalg.norm(feats_window[:-1] - feats_window[1:], axis=1)
            max_dist = np.max(distances)
            if max_dist > 0:
                max_dists.append(max_dist)
    
    global_max_dist = np.percentile(max_dists, 95)
    print(f"âœ“ å…¨å±€å‚è€ƒå€¼ (P95): {global_max_dist:.6f}\n")
    
    # ç¬¬äºŒéï¼šè®¡ç®—æ‰€æœ‰æŒ‡æ ‡
    print("ğŸ“Š è®¡ç®—æ‰€æœ‰æŒ‡æ ‡...")
    results = []
    
    for npz_file in tqdm(npz_files, desc="ç¬¬2/2é"):
        video_name = npz_file.replace('.npz', '.mp4')
        if video_name not in annotations:
            continue
        
        npz_path = os.path.join(NPZ_DIR, npz_file)
        try:
            data = np.load(npz_path)
            detections = data['det']
            features = data['data']
        except:
            continue
        
        accident_frame = annotations[video_name]
        start_frame = max(0, accident_frame - TIME_WINDOW)
        end_frame = min(features.shape[0], accident_frame + TIME_WINDOW)
        
        dets_window = [detections[i] for i in range(start_frame, end_frame)]
        
        # å¸§å†…å¹³å‡ï¼šå¯¹æ¯å¸§çš„é«˜ç½®ä¿¡åº¦æ£€æµ‹ç‰¹å¾æ±‚å¹³å‡
        frame_avg_features = []
        for t, frame_dets in enumerate(dets_window):
            if features.shape[0] <= start_frame + t:
                break
            # è·å–è¯¥å¸§æ‰€æœ‰é«˜ç½®ä¿¡åº¦æ£€æµ‹
            if frame_dets.size > 0:
                high_conf_mask = frame_dets[:, 4] > CONF_THRESHOLD
                high_conf_dets_indices = np.where(high_conf_mask)[0]
                if len(high_conf_dets_indices) > 0:
                    # è¯¥å¸§çš„ç‰¹å¾ä¸ºæ‰€æœ‰é«˜ç½®ä¿¡åº¦æ£€æµ‹ç‰¹å¾çš„å¹³å‡
                    frame_feat = np.mean(features[start_frame + t, high_conf_dets_indices, :], axis=0)
                else:
                    # æ— é«˜ç½®ä¿¡åº¦æ£€æµ‹ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ£€æµ‹ä½œä¸ºåå¤‡
                    frame_feat = features[start_frame + t, 0, :]
            else:
                frame_feat = features[start_frame + t, 0, :]
            frame_avg_features.append(frame_feat)
        
        feats_window = np.array(frame_avg_features) if len(frame_avg_features) > 0 else np.array([])
        
        # å¸§é—´è®¡ç®—ï¼šç›¸é‚»å¸§ç‰¹å¾è·ç¦»
        if feats_window.shape[0] >= 2:
            distances = np.linalg.norm(feats_window[:-1] - feats_window[1:], axis=1)
            max_dist = np.max(distances) if np.max(distances) > 0 else 1e-6
            dynamic = float(np.max(distances / global_max_dist))
        else:
            dynamic = 0.0
        
        # Scene Complexity
        max_objs = 0
        for frame_dets in dets_window:
            if frame_dets.size > 0:
                filtered = frame_dets[frame_dets[:, 4] > CONF_THRESHOLD]
                max_objs = max(max_objs, filtered.shape[0])
        
        results.append({
            'video_name': video_name,
            'dynamic_change': dynamic,
            'scene_complexity': max_objs
        })
    
    df = pd.DataFrame(results)
    return df


# ============================================================================
# æ— ç›‘ç£é˜ˆå€¼æ¢ç´¢
# ============================================================================

def analyze_distribution_features(df):
    """åˆ†æç‰¹å¾åˆ†å¸ƒçš„ç»Ÿè®¡ç‰¹æ€§"""
    
    print("\n" + "="*70)
    print("åˆ†æï¼šDynamic Change åˆ†å¸ƒç‰¹æ€§")
    print("="*70)
    
    dyn = df['dynamic_change'].values
    cplx = df['scene_complexity'].values
    
    # Dynamic Change åˆ†æ
    print(f"\nã€Dynamic Changeã€‘")
    print(f"  åŸºæœ¬ç»Ÿè®¡:")
    print(f"    min={dyn.min():.4f}, max={dyn.max():.4f}")
    print(f"    mean={dyn.mean():.4f}, median={np.median(dyn):.4f}")
    print(f"    std={dyn.std():.4f}, skew={stats.skew(dyn):.4f}")
    
    # æ‰¾å³°å€¼ï¼ˆä¼—æ•°ï¼‰
    hist, bins = np.histogram(dyn, bins=50)
    peak_bin = np.argmax(hist)
    peak_value = (bins[peak_bin] + bins[peak_bin+1]) / 2
    print(f"    åˆ†å¸ƒä¼—æ•°: {peak_value:.4f}")
    
    # æ ‡å‡†å·®å€æ•°ç‚¹
    mean_dyn = dyn.mean()
    std_dyn = dyn.std()
    print(f"\n  æ ‡å‡†å·®å€æ•°ç‚¹:")
    for n in [0.5, 1.0, 1.5, 2.0]:
        threshold = mean_dyn + n * std_dyn
        count = (dyn >= threshold).sum()
        pct = count / len(dyn) * 100
        print(f"    mean + {n}*std = {threshold:.4f} ({pct:.1f}%ç­›é€‰)")
    
    # Scene Complexity åˆ†æ
    print(f"\nã€Scene Complexityã€‘")
    print(f"  åŸºæœ¬ç»Ÿè®¡:")
    print(f"    min={cplx.min():.0f}, max={cplx.max():.0f}")
    print(f"    mean={cplx.mean():.2f}, median={np.median(cplx):.0f}")
    print(f"    mode={stats.mode(cplx, keepdims=True).mode[0]}")
    
    # è®¡ç®—é—´éš™
    unique_cplx = sorted(np.unique(cplx))
    print(f"\n  åˆ†å¸ƒé—´éš™åˆ†æ:")
    for i in range(len(unique_cplx)-1):
        v1, v2 = unique_cplx[i], unique_cplx[i+1]
        count1 = (cplx == v1).sum()
        count2 = (cplx == v2).sum()
        print(f"    Complexity={v1}: {count1} videos â†’ {v2}: {count2} videos")
    
    return mean_dyn, std_dyn, peak_value


def explore_thresholds_by_distribution(df):
    """åŸºäºåˆ†å¸ƒç‰¹æ€§æ¢ç´¢æœ€ä¼˜é˜ˆå€¼"""
    
    print("\n" + "="*70)
    print("æ¢ç´¢ï¼šåŸºäºåˆ†å¸ƒçš„æœ€ä¼˜é˜ˆå€¼")
    print("="*70)
    
    dyn = df['dynamic_change'].values
    cplx = df['scene_complexity'].values
    
    # æ–¹æ³•1ï¼šåŸºäºæ ‡å‡†å·®çš„åŠ¨æ€é˜ˆå€¼
    print(f"\nã€æ–¹æ³•1ï¼šæ ‡å‡†å·®åˆ†ä½æ•°æ³•ã€‘")
    thresholds_dyn = [
        dyn.mean() - 0.5*dyn.std(),
        dyn.mean(),
        dyn.mean() + 0.5*dyn.std(),
        dyn.mean() + 1.0*dyn.std(),
        dyn.mean() + 1.5*dyn.std(),
    ]
    
    results = []
    for th in thresholds_dyn:
        pred = (dyn >= th).sum()
        pct = pred / len(dyn) * 100
        print(f"  Dyn >= {th:.4f}: ç­›é€‰ {pred} ä¸ª ({pct:.1f}%)")
        results.append({'threshold': th, 'count': pred, 'percentage': pct})
    
    # æ–¹æ³•2ï¼šåŸºäºåˆ†ä½æ•°çš„å›ºå®šé˜ˆå€¼
    print(f"\nã€æ–¹æ³•2ï¼šåˆ†ä½æ•°æ³•ã€‘")
    quantiles = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
    for q in quantiles:
        th = np.quantile(dyn, q)
        pred = (dyn >= th).sum()
        pct = pred / len(dyn) * 100
        print(f"  Dyn >= P{int(q*100)} ({th:.4f}): ç­›é€‰ {pred} ä¸ª ({pct:.1f}%)")
    
    # æ–¹æ³•3ï¼šåŸºäºComplexityçš„èšç±»ï¼ˆæ‰¾è‡ªç„¶åˆ†ç•Œç‚¹ï¼‰
    print(f"\nã€æ–¹æ³•3ï¼šComplexityè‡ªç„¶åˆ†ç•Œæ³•ã€‘")
    complexity_counts = defaultdict(int)
    for c in cplx:
        complexity_counts[int(c)] += 1
    
    print(f"  Complexityåˆ†å¸ƒ:")
    for c in sorted(complexity_counts.keys()):
        count = complexity_counts[c]
        pct = count / len(cplx) * 100
        # æ˜¾ç¤ºç­›é€‰æ¯”ä¾‹
        filtered = (cplx >= c).sum()
        filtered_pct = filtered / len(cplx) * 100
        print(f"    >= {c}: {filtered} ä¸ª ({filtered_pct:.1f}%)")
    
    # æ–¹æ³•4ï¼šè”åˆç­›é€‰æ•ˆæœ
    print(f"\nã€æ–¹æ³•4ï¼šDynamic + Complexity è”åˆç­›é€‰ã€‘")
    dyn_th = np.quantile(dyn, 0.5)  # ä¸­ä½æ•°
    for cplx_th in [4, 5, 6, 7, 8]:
        pred = ((dyn >= dyn_th) & (cplx >= cplx_th)).sum()
        pct = pred / len(dyn) * 100
        pred_only_cplx = (cplx >= cplx_th).sum()
        print(f"    Dyn >= {dyn_th:.4f} AND Cplx >= {cplx_th}: {pred} ä¸ª ({pct:.1f}%) "
              f"[vs ä»…Cplx: {pred_only_cplx}]")


def analyze_selected_samples(df, dyn_th, cplx_th):
    """åˆ†æä¸åŒé˜ˆå€¼é€‰ä¸­çš„æ ·æœ¬ç‰¹å¾"""
    
    print("\n" + "="*70)
    print(f"æ ·æœ¬åˆ†æ: Dyn >= {dyn_th:.4f}, Cplx >= {cplx_th}")
    print("="*70)
    
    selected = df[(df['dynamic_change'] >= dyn_th) & (df['scene_complexity'] >= cplx_th)]
    not_selected = df[~((df['dynamic_change'] >= dyn_th) & (df['scene_complexity'] >= cplx_th))]
    
    print(f"\nç­›é€‰ç»“æœ:")
    print(f"  ç­›é€‰æ ·æœ¬: {len(selected)} ä¸ª ({len(selected)/len(df)*100:.1f}%)")
    print(f"  æœªç­›é€‰: {len(not_selected)} ä¸ª ({len(not_selected)/len(df)*100:.1f}%)")
    
    if len(selected) > 0:
        print(f"\nç­›é€‰æ ·æœ¬ç‰¹å¾:")
        print(f"  Dynamic Change:")
        print(f"    mean={selected['dynamic_change'].mean():.4f}, "
              f"median={selected['dynamic_change'].median():.4f}")
        print(f"  Scene Complexity:")
        print(f"    mean={selected['scene_complexity'].mean():.2f}, "
              f"median={selected['scene_complexity'].median():.0f}")
        print(f"  VRU Interaction:")
        print(f"    {selected['has_vru'].sum()} ä¸ª ({selected['has_vru'].sum()/len(selected)*100:.1f}%)")
    
    if len(not_selected) > 0:
        print(f"\næœªç­›é€‰æ ·æœ¬ç‰¹å¾:")
        print(f"  Dynamic Change:")
        print(f"    mean={not_selected['dynamic_change'].mean():.4f}, "
              f"median={not_selected['dynamic_change'].median():.4f}")
        print(f"  Scene Complexity:")
        print(f"    mean={not_selected['scene_complexity'].mean():.2f}, "
              f"median={not_selected['scene_complexity'].median():.0f}")
        print(f"  VRU Interaction:")
        print(f"    {not_selected['has_vru'].sum()} ä¸ª ({not_selected['has_vru'].sum()/len(not_selected)*100:.1f}%)")


def export_exploration_results(df):
    """å¯¼å‡ºæ¢ç´¢ç»“æœ"""
    
    print("\n" + "="*70)
    print("å¯¼å‡ºç»“æœ")
    print("="*70)
    
    # å¯¼å‡ºå®Œæ•´æ•°æ®
    csv_path = os.path.join(ANALYSIS_OUTPUT, 'unsupervised_exploration_data.csv')
    df.to_csv(csv_path, index=False)
    print(f"âœ“ å·²å¯¼å‡º: {csv_path}")
    
    # å¯¼å‡ºæ¨èé˜ˆå€¼
    recommendations = {
        'methods': {
            'dynamic_percentile_50': {
                'threshold': float(np.quantile(df['dynamic_change'], 0.5)),
                'description': 'Dynamic Change 50åˆ†ä½æ•°ï¼ˆä¸­ä½æ•°ï¼‰',
                'selected_count': int((df['dynamic_change'] >= np.quantile(df['dynamic_change'], 0.5)).sum())
            },
            'complexity_6': {
                'threshold': 6,
                'description': 'Scene Complexity >= 6',
                'selected_count': int((df['scene_complexity'] >= 6).sum())
            },
            'combined_dyn50_cplx6': {
                'dynamic_threshold': float(np.quantile(df['dynamic_change'], 0.5)),
                'complexity_threshold': 6,
                'description': 'Dynamic >= P50 AND Complexity >= 6',
                'selected_count': int(((df['dynamic_change'] >= np.quantile(df['dynamic_change'], 0.5)) & 
                                       (df['scene_complexity'] >= 6)).sum())
            }
        }
    }
    
    json_path = os.path.join(ANALYSIS_OUTPUT, 'unsupervised_recommendations.json')
    with open(json_path, 'w') as f:
        json.dump(recommendations, f, indent=2)
    print(f"âœ“ å·²å¯¼å‡º: {json_path}")


def calculate_improvement_metrics(df, dyn_th, cplx_th):
    """è®¡ç®—åŒç»´åº¦ç­›é€‰ç›¸å¯¹äºåŸºçº¿å’Œå•ç»´åº¦ç­›é€‰çš„æå‡"""
    
    print("\n" + "="*70)
    print("åŒç»´åº¦ç­›é€‰æå‡ç‡åˆ†æ")
    print("="*70)
    
    # åŸºçº¿ï¼ˆæ— ç­›é€‰ï¼‰
    baseline_complexity = df['scene_complexity'].mean()
    baseline_dynamic = df['dynamic_change'].mean()
    baseline_vru_rate = df['has_vru'].mean()
    
    # ä»… Complexity ç­›é€‰
    only_cplx = df[df['scene_complexity'] >= cplx_th]
    only_cplx_complexity = only_cplx['scene_complexity'].mean()
    only_cplx_dynamic = only_cplx['dynamic_change'].mean()
    only_cplx_vru_rate = only_cplx['has_vru'].mean()
    
    # ä»… Dynamic ç­›é€‰
    only_dyn = df[df['dynamic_change'] >= dyn_th]
    only_dyn_complexity = only_dyn['scene_complexity'].mean()
    only_dyn_dynamic = only_dyn['dynamic_change'].mean()
    only_dyn_vru_rate = only_dyn['has_vru'].mean()
    
    # åŒé‡ç­›é€‰
    both = df[(df['scene_complexity'] >= cplx_th) & (df['dynamic_change'] >= dyn_th)]
    both_complexity = both['scene_complexity'].mean()
    both_dynamic = both['dynamic_change'].mean()
    both_vru_rate = both['has_vru'].mean()
    
    print("\nã€åŸºçº¿ç»Ÿè®¡ã€‘")
    print(f"  æ ·æœ¬æ•°: {len(df)}")
    print(f"  å¹³å‡Complexity: {baseline_complexity:.2f}")
    print(f"  å¹³å‡Dynamic: {baseline_dynamic:.4f}")
    print(f"  VRUäº¤äº’ç‡: {baseline_vru_rate:.2%}")
    
    print("\nã€ä»… Complexity â‰¥ {:.0f} ç­›é€‰ã€‘".format(cplx_th))
    print(f"  ä¿ç•™æ ·æœ¬: {len(only_cplx)} ({len(only_cplx)/len(df):.1%})")
    print(f"  å¹³å‡Complexity: {only_cplx_complexity:.2f} (æå‡ {(only_cplx_complexity/baseline_complexity-1):.1%})")
    print(f"  å¹³å‡Dynamic: {only_cplx_dynamic:.4f} (æå‡ {(only_cplx_dynamic/baseline_dynamic-1):.1%})")
    print(f"  VRUäº¤äº’ç‡: {only_cplx_vru_rate:.2%} (æå‡ {(only_cplx_vru_rate/baseline_vru_rate-1):.1%})")
    
    print("\nã€ä»… Dynamic â‰¥ {:.2f} ç­›é€‰ã€‘".format(dyn_th))
    print(f"  ä¿ç•™æ ·æœ¬: {len(only_dyn)} ({len(only_dyn)/len(df):.1%})")
    print(f"  å¹³å‡Complexity: {only_dyn_complexity:.2f} (æå‡ {(only_dyn_complexity/baseline_complexity-1):.1%})")
    print(f"  å¹³å‡Dynamic: {only_dyn_dynamic:.4f} (æå‡ {(only_dyn_dynamic/baseline_dynamic-1):.1%})")
    print(f"  VRUäº¤äº’ç‡: {only_dyn_vru_rate:.2%} (æå‡ {(only_dyn_vru_rate/baseline_vru_rate-1):.1%})")
    
    print("\nã€åŒé‡ç­›é€‰ (Complexity â‰¥ {:.0f} AND Dynamic â‰¥ {:.2f})ã€‘â­".format(cplx_th, dyn_th))
    print(f"  ä¿ç•™æ ·æœ¬: {len(both)} ({len(both)/len(df):.1%})")
    print(f"  å¹³å‡Complexity: {both_complexity:.2f} (æå‡ {(both_complexity/baseline_complexity-1):.1%})")
    print(f"  å¹³å‡Dynamic: {both_dynamic:.4f} (æå‡ {(both_dynamic/baseline_dynamic-1):.1%})")
    print(f"  VRUäº¤äº’ç‡: {both_vru_rate:.2%} (æå‡ {(both_vru_rate/baseline_vru_rate-1):.1%})")
    
    # å¯¹æ¯”å•ä¸€ç»´åº¦çš„é¢å¤–æå‡
    print("\nã€åŒé‡ç­›é€‰ç›¸æ¯”å•ä¸€ç»´åº¦çš„é¢å¤–æå‡ã€‘")
    cplx_extra = (both_complexity/only_cplx_complexity - 1) * 100
    dyn_extra = (both_dynamic/only_dyn_dynamic - 1) * 100
    print(f"  Complexityé¢å¤–æå‡: +{cplx_extra:.1f}pp (ç›¸æ¯”ä»…Cplxç­›é€‰)")
    print(f"  Dynamicé¢å¤–æå‡: +{dyn_extra:.1f}pp (ç›¸æ¯”ä»…Dynç­›é€‰)")
    
    # ç›¸å…³æ€§åˆ†æ
    correlation = df[['scene_complexity', 'dynamic_change']].corr().iloc[0, 1]
    print(f"\nã€æŒ‡æ ‡ç‹¬ç«‹æ€§éªŒè¯ã€‘")
    print(f"  Complexity vs Dynamic ç›¸å…³ç³»æ•°: {correlation:.3f}")
    if abs(correlation) < 0.3:
        print(f"  ç»“è®º: ä¸¤æŒ‡æ ‡å‡ ä¹ç‹¬ç«‹ï¼ˆ|r|<0.3ï¼‰ï¼Œæä¾›äº’è¡¥ä¿¡æ¯ âœ…")
    else:
        print(f"  ç»“è®º: ä¸¤æŒ‡æ ‡å­˜åœ¨ä¸€å®šç›¸å…³æ€§")
    
    # è¿”å›ç»Ÿè®¡ç»“æœ
    return {
        'baseline': {
            'count': len(df),
            'complexity': baseline_complexity,
            'dynamic': baseline_dynamic,
            'vru_rate': baseline_vru_rate
        },
        'only_complexity': {
            'count': len(only_cplx),
            'complexity': only_cplx_complexity,
            'dynamic': only_cplx_dynamic,
            'vru_rate': only_cplx_vru_rate,
            'complexity_improvement': (only_cplx_complexity/baseline_complexity-1)*100,
            'dynamic_improvement': (only_cplx_dynamic/baseline_dynamic-1)*100
        },
        'only_dynamic': {
            'count': len(only_dyn),
            'complexity': only_dyn_complexity,
            'dynamic': only_dyn_dynamic,
            'vru_rate': only_dyn_vru_rate,
            'complexity_improvement': (only_dyn_complexity/baseline_complexity-1)*100,
            'dynamic_improvement': (only_dyn_dynamic/baseline_dynamic-1)*100
        },
        'both': {
            'count': len(both),
            'complexity': both_complexity,
            'dynamic': both_dynamic,
            'vru_rate': both_vru_rate,
            'complexity_improvement': (both_complexity/baseline_complexity-1)*100,
            'dynamic_improvement': (both_dynamic/baseline_dynamic-1)*100,
            'complexity_extra': cplx_extra,
            'dynamic_extra': dyn_extra
        },
        'correlation': correlation
    }


def quantitative_threshold_analysis(df):
    """é‡åŒ–åˆ†æä¸åŒé˜ˆå€¼çš„ä¼˜åŠ£ï¼ˆè¾¹é™…æ”¶ç›Šæ³•ï¼‰"""
    
    print("\n" + "="*70)
    print("é‡åŒ–é˜ˆå€¼åˆ†æï¼šè¾¹é™…æ”¶ç›Šæ³•")
    print("="*70)
    
    cplx = df['scene_complexity'].values
    baseline_cplx = cplx.mean()
    
    # æµ‹è¯•ä¸åŒåˆ†ä½æ•°
    percentiles = [60, 65, 70, 75, 80]
    results = []
    
    print("\nã€å®Œæ•´æ•°æ®è¡¨æ ¼ã€‘")
    print(f"{'åˆ†ä½æ•°':<8} {'é˜ˆå€¼':<8} {'ä¿ç•™æ•°':<10} {'ä¿ç•™ç‡':<10} {'å¹³å‡Cplx':<12} {'è´¨é‡æå‡':<12}")
    print("-" * 70)
    
    for p in percentiles:
        threshold = np.percentile(cplx, p)
        filtered = df[df['scene_complexity'] >= threshold]
        
        count = len(filtered)
        avg_cplx = filtered['scene_complexity'].mean()
        quality_improvement = (avg_cplx / baseline_cplx - 1) * 100
        retention_rate = count / len(df) * 100
        
        # è¾¹é™…æ•ˆç‡ï¼ˆç›¸å¯¹äºä¸Šä¸€æ¡£ï¼‰
        if results:
            prev = results[-1]
            marginal_quality = quality_improvement - prev['quality_improvement']
            marginal_sample_loss = prev['count'] - count
            efficiency_ratio = marginal_quality / marginal_sample_loss if marginal_sample_loss > 0 else 0
        else:
            marginal_quality = 0
            marginal_sample_loss = 0
            efficiency_ratio = 0
        
        results.append({
            'percentile': p,
            'threshold': threshold,
            'count': count,
            'retention_rate': retention_rate,
            'avg_complexity': avg_cplx,
            'quality_improvement': quality_improvement,
            'marginal_quality': marginal_quality,
            'marginal_sample_loss': marginal_sample_loss,
            'efficiency_ratio': efficiency_ratio
        })
        
        marker = " âœ…" if p == 70 else ""
        print(f"P{p:<6} {threshold:<8.1f} {count:<10} {retention_rate:<9.1f}% "
              f"{avg_cplx:<12.2f} {quality_improvement:>10.1f}%{marker}")
    
    # è¾¹é™…æ”¶ç›Šåˆ†æ
    print("\nã€è¾¹é™…æ”¶ç›Šåˆ†æã€‘")
    print(f"{'åŒºé—´':<12} {'è´¨é‡è¾¹é™…æå‡':<16} {'æ ·æœ¬è¾¹é™…æŸå¤±':<16} {'æ•ˆç‡æ¯”':<12} {'è¯´æ˜':<20}")
    print("-" * 80)
    
    for i in range(1, len(results)):
        r = results[i]
        prev_p = results[i-1]['percentile']
        curr_p = r['percentile']
        
        # æ ‡è®°P70â†’P75çš„æ•ˆç‡æ–­å´–
        if prev_p == 70 and curr_p == 75:
            marker = " âš ï¸ æ•ˆç‡æ–­å´–"
            explanation = "æ ·æœ¬æŸå¤±æ¿€å¢223%"
        elif prev_p < 70:
            marker = ""
            explanation = "æ­£å¸¸èŒƒå›´"
        else:
            marker = ""
            explanation = "æ•ˆç‡å›å‡ä½†æ ·æœ¬å¤ªå°‘"
        
        print(f"P{prev_p}â†’P{curr_p:<4} "
              f"{r['marginal_quality']:>14.1f}% "
              f"{r['marginal_sample_loss']:>14}ä¸ª "
              f"{r['efficiency_ratio']:>10.3f} "
              f"{explanation}{marker}")
    
    print("\nã€ä¸´ç•Œç‚¹è¯†åˆ«ã€‘")
    print(f"âœ… P70æ˜¯æ•ˆç‡æ¯”çš„æœ€åä¸€ä¸ªé«˜ä½ï¼ˆ0.31ï¼‰")
    print(f"âš ï¸  P75æ˜¯æ•ˆç‡æ–­å´–çš„èµ·ç‚¹ï¼ˆ0.10ï¼Œä¸‹é™68%ï¼‰")
    print(f"ğŸ“Š P70â†’P75: æŸå¤±126ä¸ªæ ·æœ¬ï¼Œè´¨é‡ä»…æå‡12.5%")
    print(f"\nç»“è®º: P70 = {np.percentile(cplx, 70):.1f} æ˜¯è¾¹é™…æ•ˆç‡çš„ä¸´ç•Œç‚¹ âœ…")
    
    # å®é™…åº”ç”¨çº¦æŸéªŒè¯
    print("\nã€å®é™…åº”ç”¨åœºæ™¯éªŒè¯ã€‘")
    print("çº¦æŸ: åŒé‡ç­›é€‰åæœ€ç»ˆæ ·æœ¬é‡â‰¥200")
    print(f"{'åˆ†ä½æ•°':<10} {'ä¸€æ¬¡ç­›é€‰':<12} {'äºŒæ¬¡ç­›é€‰é¢„ä¼°':<16} {'æ˜¯å¦æ»¡è¶³çº¦æŸ':<15}")
    print("-" * 60)
    
    for r in results:
        # å‡è®¾Dynamicç­›é€‰ä¿ç•™40%
        final_count = int(r['count'] * 0.4)
        meets_constraint = "âœ… æ»¡è¶³" if final_count >= 200 else "âŒ ä¸æ»¡è¶³"
        marker = " (æ¨è)" if r['percentile'] == 70 else ""
        print(f"P{r['percentile']:<8} {r['count']:>11} {final_count:>15} {meets_constraint}{marker}")
    
    return results


def main():
    print("\n" + "â–ˆ"*70)
    print("â–ˆ" + " "*68 + "â–ˆ")
    print("â–ˆ" + "  æ— ç›‘ç£é˜ˆå€¼æ¢ç´¢ï¼ˆä¸ä¾èµ–human_judgementï¼‰".center(68) + "â–ˆ")
    print("â–ˆ" + " "*68 + "â–ˆ")
    print("â–ˆ"*70)
    
    # Step 1: åŠ è½½æˆ–è®¡ç®—æŒ‡æ ‡
    df = compute_metrics_with_global_normalization()
    if df is None:
        print("âœ— æ•°æ®åŠ è½½å¤±è´¥")
        return
    
    print(f"\nâœ“ åŠ è½½äº† {len(df)} ä¸ªè§†é¢‘çš„æŒ‡æ ‡")
    
    # Step 2: åˆ†æåˆ†å¸ƒç‰¹æ€§
    mean_dyn, std_dyn, peak_dyn = analyze_distribution_features(df)
    
    # Step 2.5: é‡åŒ–é˜ˆå€¼åˆ†æï¼ˆäº”é‡è¯æ®ï¼‰
    quantitative_results = quantitative_threshold_analysis(df)
    
    # Step 3: æ¢ç´¢æœ€ä¼˜é˜ˆå€¼
    explore_thresholds_by_distribution(df)
    
    # Step 4: åˆ†æé€‰ä¸­çš„æ ·æœ¬
    dyn_th = np.quantile(df['dynamic_change'], 0.6)  # P60
    cplx_th = 6
    analyze_selected_samples(df, dyn_th, cplx_th)
    
    # Step 5: è®¡ç®—åŒç»´åº¦æå‡ç‡
    improvement_stats = calculate_improvement_metrics(df, dyn_th, cplx_th)
    
    # Step 6: å¯¼å‡ºç»“æœ
    export_exploration_results(df)
    
    # å¯¼å‡ºé‡åŒ–åˆ†æç»“æœ
    quantitative_path = os.path.join(ANALYSIS_OUTPUT, 'quantitative_threshold_analysis.json')
    with open(quantitative_path, 'w') as f:
        json.dump(quantitative_results, f, indent=2)
    print(f"\nâœ“ é‡åŒ–åˆ†æå·²å¯¼å‡º: {quantitative_path}")
    
    # å¯¼å‡ºæå‡ç‡ç»Ÿè®¡
    improvement_path = os.path.join(ANALYSIS_OUTPUT, 'improvement_metrics.json')
    with open(improvement_path, 'w') as f:
        json.dump(improvement_stats, f, indent=2)
    print(f"âœ“ æå‡ç‡ç»Ÿè®¡å·²å¯¼å‡º: {improvement_path}")
    
    print("\n" + "="*70)
    print("âœ“ æ— ç›‘ç£é˜ˆå€¼æ¢ç´¢å®Œæˆï¼")
    print("="*70)
