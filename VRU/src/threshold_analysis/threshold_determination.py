#!/usr/bin/env python3
"""
åŸºäºç»Ÿè®¡åˆ†å¸ƒçš„é˜ˆå€¼è‡ªåŠ¨ç¡®å®šæ–¹æ¡ˆ

èƒŒæ™¯ï¼šä¸åŒçš„ç»Ÿè®¡æ–¹æ³•ä¼šå¯¼è‡´ä¸åŒçš„ç­›é€‰ç»“æœï¼Œæœ¬è„šæœ¬æä¾›å¤šç§
"ç§‘å­¦åˆç†"çš„é˜ˆå€¼ç¡®å®šæ–¹æ³•ï¼Œå¯æ ¹æ®å®é™…å·¥ä½œé‡éœ€æ±‚é€‰æ‹©ã€‚

æ–¹æ³•æ¦‚è§ˆï¼š
1. Elbowæ³•ï¼šæ‰¾åˆ†å¸ƒçš„è‡ªç„¶æ–­ç‚¹
2. æ ‡å‡†å·®æ³•ï¼šmean + n*stdï¼ˆnä¸ºè‡ªç”±åº¦ï¼‰
3. åˆ†ä½æ•°æ³•ï¼šP95/P90/P75ç­‰å…³é”®ç‚¹
4. å¤šæŒ‡æ ‡åŠ æƒï¼šç»¼åˆComplexity+Dynamic
5. å³°åº¦åˆ†æï¼šåŸºäºåˆ†å¸ƒçš„"è‚¥å°¾"ç‰¹æ€§
"""

import os
import json
import numpy as np
import pandas as pd
from scipy import stats
from tqdm import tqdm

# ============================================================================
# é…ç½®
# ============================================================================

ROOT_DIR = "/home/24068286g/UString"
CCD_ROOT = os.path.join(ROOT_DIR, 'data', 'crash')
NPZ_DIR = os.path.join(CCD_ROOT, 'yolo_features', 'positive')
ANNOTATION_FILE = os.path.join(CCD_ROOT, 'videos', 'Crash-1500.txt')

# ============================================================================
# å·¥å…·å‡½æ•°ï¼šåŠ è½½æ•°æ®
# ============================================================================

def load_annotations(file_path):
    """åŠ è½½äº‹æ•…å¸§æ ‡æ³¨"""
    annotations = {}
    try:
        with open(file_path, 'r') as f:
            for line in f:
                try:
                    vid_id = line[:6]
                    start = line.find('[')
                    end = line.find(']')
                    if start != -1 and end != -1:
                        labels = [int(x.strip()) for x in line[start+1:end].split(',')]
                        accident_frame = labels.index(1)
                        annotations[f"{vid_id}.mp4"] = accident_frame
                except:
                    continue
    except FileNotFoundError:
        pass
    return annotations


def compute_metrics(time_window=30, conf_threshold=0.5):
    """è®¡ç®—æ‰€æœ‰è§†é¢‘çš„æŒ‡æ ‡"""
    
    annotations = load_annotations(ANNOTATION_FILE)
    if not annotations:
        print("âœ— æ— æ³•åŠ è½½æ ‡æ³¨")
        return None
    
    VRU_IDS = {0, 1, 3}
    CAR_IDS = {2, 5, 7}
    
    npz_files = sorted([f for f in os.listdir(NPZ_DIR) if f.endswith('.npz')])
    
    # ç¬¬ä¸€éï¼šè·å–å…¨å±€å‚è€ƒå€¼
    print("ğŸ“Š æ”¶é›†å…¨å±€å‚è€ƒå€¼...")
    max_dists = []
    
    for npz_file in tqdm(npz_files, desc="ç¬¬1/2é"):
        video_name = npz_file.replace('.npz', '.mp4')
        if video_name not in annotations:
            continue
        
        npz_path = os.path.join(NPZ_DIR, npz_file)
        try:
            data = np.load(npz_path)
            features = data['data']
        except:
            continue
        
        accident_frame = annotations[video_name]
        start_frame = max(0, accident_frame - time_window)
        end_frame = min(features.shape[0], accident_frame + time_window)
        feats_window = features[start_frame:end_frame, 0, :]
        
        if feats_window.shape[0] >= 2:
            distances = np.linalg.norm(feats_window[:-1] - feats_window[1:], axis=1)
            max_dist = np.max(distances)
            if max_dist > 0:
                max_dists.append(max_dist)
    
    global_ref = np.percentile(max_dists, 95)
    print(f"âœ“ å…¨å±€å‚è€ƒå€¼ (P95): {global_ref:.6f}\n")
    
    # ç¬¬äºŒéï¼šè®¡ç®—æŒ‡æ ‡
    print("ğŸ“Š è®¡ç®—æ‰€æœ‰æŒ‡æ ‡...")
    results = []
    
    for npz_file in tqdm(npz_files, desc="ç¬¬2/2é"):
        video_name = npz_file.replace('.npz', '.mp4')
        if video_name not in annotations:
            continue
        
        npz_path = os.path.join(NPZ_DIR, npz_file)
        try:
            data = np.load(npz_path)
            detections = data['det']
            features = data['data']
        except:
            continue
        
        accident_frame = annotations[video_name]
        start_frame = max(0, accident_frame - time_window)
        end_frame = min(features.shape[0], accident_frame + time_window)
        
        dets_window = [detections[i] for i in range(start_frame, end_frame)]
        feats_window = features[start_frame:end_frame, 0, :]
        
        # Dynamic Change
        if feats_window.shape[0] >= 2:
            distances = np.linalg.norm(feats_window[:-1] - feats_window[1:], axis=1)
            max_dist = np.max(distances) if np.max(distances) > 0 else 1e-6
            dynamic = float(np.max(distances / global_ref))
        else:
            dynamic = 0.0
        
        # Scene Complexity
        max_objs = 0
        for frame_dets in dets_window:
            if frame_dets.size > 0:
                filtered = frame_dets[frame_dets[:, 4] > conf_threshold]
                max_objs = max(max_objs, filtered.shape[0])
        
        # VRU Interaction
        has_vru = False
        for frame_dets in dets_window:
            if frame_dets.size > 0:
                filtered = frame_dets[frame_dets[:, 4] > conf_threshold]
                classes = set(int(obj[5]) for obj in filtered)
                if (classes & VRU_IDS) and (classes & CAR_IDS):
                    has_vru = True
                    break
        
        results.append({
            'video_name': video_name,
            'dynamic_change': dynamic,
            'scene_complexity': max_objs,
            'has_vru': int(has_vru)
        })
    
    df = pd.DataFrame(results)
    return df


# ============================================================================
# é˜ˆå€¼ç¡®å®šæ–¹æ³•
# ============================================================================

class ThresholdDeterminer:
    """å¤šç§é˜ˆå€¼ç¡®å®šæ–¹æ³•çš„é›†åˆ"""
    
    def __init__(self, df):
        self.df = df
        self.dyn = df['dynamic_change'].values
        self.cplx = df['scene_complexity'].values
        self.total = len(df)
    
    def method_1_stddev(self, metric='complexity', n_std=1.0, direction='above'):
        """
        æ ‡å‡†å·®æ³•ï¼šmean Â± n*std
        
        Args:
            metric: 'complexity' æˆ– 'dynamic'
            n_std: æ ‡å‡†å·®å€æ•°ï¼ˆ1.0 = mean+1*stdï¼Œæ›´ä¸¥æ ¼ï¼‰
            direction: 'above' (mean+n*std) æˆ– 'below' (mean-n*std)
        """
        data = self.cplx if metric == 'complexity' else self.dyn
        mean = data.mean()
        std = data.std()
        
        threshold = mean + n_std * std if direction == 'above' else mean - n_std * std
        count = (data >= threshold).sum() if direction == 'above' else (data <= threshold).sum()
        pct = count / self.total * 100
        
        return {
            'method': f'æ ‡å‡†å·®æ³• ({metric}) mean {direction} {n_std}*std',
            'threshold': float(threshold),
            'selected_count': int(count),
            'percentage': float(pct),
            'mean': float(mean),
            'std': float(std),
        }
    
    def method_2_percentile(self, metric='complexity', percentile=75):
        """
        åˆ†ä½æ•°æ³•ï¼šå–ä¸Šä½åˆ†ä½æ•°
        
        å®è´¨ï¼šåªé€‰æ‹©æœ€å¥½çš„å‰(100-percentile)%çš„æ ·æœ¬
        """
        data = self.cplx if metric == 'complexity' else self.dyn
        threshold = np.percentile(data, percentile)
        count = (data >= threshold).sum()
        pct = count / self.total * 100
        
        return {
            'method': f'åˆ†ä½æ•°æ³• ({metric}) P{percentile}',
            'threshold': float(threshold),
            'selected_count': int(count),
            'percentage': float(pct),
            'description': f'é€‰æ‹©å‰{100-percentile:.0f}%çš„æœ€é«˜è´¨é‡æ ·æœ¬',
        }
    
    def method_3_elbow(self, metric='complexity'):
        """
        Elbowæ³•ï¼šå¯»æ‰¾åˆ†å¸ƒçš„è‡ªç„¶æ–­ç‚¹ï¼ˆäºŒé˜¶å¯¼æ•°æœ€å¤§ï¼‰
        """
        data = self.cplx if metric == 'complexity' else self.dyn
        
        # æ’åºå¹¶è®¡ç®—å·®åˆ†
        sorted_data = np.sort(data)[::-1]  # ä»å¤§åˆ°å°
        diff1 = np.diff(sorted_data)
        diff2 = np.diff(diff1)  # äºŒé˜¶å¯¼æ•°
        
        # æ‰¾æœ€å¤§æ›²ç‡ç‚¹ï¼ˆäºŒé˜¶å¯¼æ•°æœ€å¤§ï¼‰
        elbow_idx = np.argmax(np.abs(diff2)) + 1
        threshold = sorted_data[elbow_idx]
        count = (data >= threshold).sum()
        pct = count / self.total * 100
        
        return {
            'method': f'Elbowæ³• ({metric})',
            'threshold': float(threshold),
            'selected_count': int(count),
            'percentage': float(pct),
            'elbow_position': int(elbow_idx),
            'description': 'å¯»æ‰¾åˆ†å¸ƒçš„è‡ªç„¶æ–­ç‚¹',
        }
    
    def method_4_combined_weighted(self, cplx_weight=0.7, dyn_weight=0.3):
        """
        åŠ æƒç»¼åˆæ³•ï¼šç»“åˆComplexityå’ŒDynamic
        
        ç»¼åˆè¯„åˆ† = cplx_normalized * cplx_weight + dyn_normalized * dyn_weight
        é€‰æ‹©è¯„åˆ†åœ¨ä¸Šåˆ†ä½æ•°çš„æ ·æœ¬
        """
        # å½’ä¸€åŒ–
        cplx_norm = (self.cplx - self.cplx.min()) / (self.cplx.max() - self.cplx.min() + 1e-6)
        dyn_norm = (self.dyn - self.dyn.min()) / (self.dyn.max() - self.dyn.min() + 1e-6)
        
        # åŠ æƒè¯„åˆ†
        scores = cplx_norm * cplx_weight + dyn_norm * dyn_weight
        
        # é€‰æ‹©å‰25%æœ€å¥½çš„
        threshold = np.percentile(scores, 75)
        selected = scores >= threshold
        count = selected.sum()
        pct = count / self.total * 100
        
        return {
            'method': f'åŠ æƒç»¼åˆæ³• (Complexity {cplx_weight}, Dynamic {dyn_weight})',
            'threshold': float(threshold),
            'selected_count': int(count),
            'percentage': float(pct),
            'selected_videos': self.df[selected]['video_name'].tolist(),
        }
    
    def method_5_distribution_shape(self, metric='complexity'):
        """
        åŸºäºåˆ†å¸ƒå½¢çŠ¶çš„è‡ªé€‚åº”æ³•
        
        ä½¿ç”¨å³°åº¦(kurtosis)å’Œååº¦(skewness)æ¥ç¡®å®šæœ€ä¼˜åˆ‡åˆ†ç‚¹
        """
        data = self.cplx if metric == 'complexity' else self.dyn
        
        mean = data.mean()
        std = data.std()
        skewness = stats.skew(data)
        kurtosis = stats.kurtosis(data)
        
        # å¦‚æœåˆ†å¸ƒæ˜¯æ­£æ€çš„ï¼ˆskew~0, kurt~0ï¼‰ï¼Œç”¨æ ‡å‡†å·®
        # å¦‚æœå³åï¼ˆskew>0ï¼‰ï¼Œåº”è¯¥ç”¨æ›´é«˜çš„é˜ˆå€¼æ¥é¿å…æç«¯å€¼å½±å“
        if abs(skewness) < 0.5:
            # è¿‘ä¼¼æ­£æ€ â†’ mean + 1*std
            threshold = mean + 1.0 * std
        else:
            # éæ­£æ€ â†’ ç”¨åˆ†ä½æ•°
            threshold = np.percentile(data, 75)
        
        count = (data >= threshold).sum()
        pct = count / self.total * 100
        
        return {
            'method': f'åˆ†å¸ƒè‡ªé€‚åº”æ³• ({metric})',
            'threshold': float(threshold),
            'selected_count': int(count),
            'percentage': float(pct),
            'distribution_analysis': {
                'mean': float(mean),
                'std': float(std),
                'skewness': float(skewness),
                'kurtosis': float(kurtosis),
                'shape': 'è¿‘ä¼¼æ­£æ€' if abs(skewness) < 0.5 else 'éæ­£æ€åˆ†å¸ƒ',
            }
        }
    
    def generate_report(self, target_reduction_rate=0.3):
        """
        ç”Ÿæˆå®Œæ•´çš„é˜ˆå€¼å»ºè®®æŠ¥å‘Š
        
        Args:
            target_reduction_rate: ç›®æ ‡å‰Šå‡ç‡ï¼ˆå¦‚0.3 = å‡å°‘30%ï¼‰
        """
        
        print("\n" + "="*80)
        print("ã€å¤šæ–¹æ³•é˜ˆå€¼ç¡®å®šæŠ¥å‘Šã€‘")
        print("="*80)
        print(f"æ€»è§†é¢‘æ•°: {self.total}")
        print(f"ç›®æ ‡å‰Šå‡ç‡: {target_reduction_rate*100:.0f}% (ä¿ç•™ {int(self.total*(1-target_reduction_rate))} ä¸ª)")
        
        results = {}
        
        print("\n" + "-"*80)
        print("æ–¹æ³•1: æ ‡å‡†å·®æ³•ï¼ˆæ¨èç”¨äºè¿‘ä¼¼æ­£æ€åˆ†å¸ƒï¼‰")
        print("-"*80)
        
        for n_std in [0.5, 1.0, 1.5, 2.0]:
            result = self.method_1_stddev('complexity', n_std)
            results[f'stddev_cplx_{n_std}'] = result
            print(f"  mean + {n_std}*std = {result['threshold']:.2f}")
            print(f"    â†’ é€‰æ‹© {result['selected_count']} ä¸ª ({result['percentage']:.1f}%)")
        
        print("\n" + "-"*80)
        print("æ–¹æ³•2: åˆ†ä½æ•°æ³•ï¼ˆç›®æ ‡ä¿ç•™æŸä¸ªç™¾åˆ†æ¯”ï¼‰")
        print("-"*80)
        
        for percentile in [50, 60, 70, 75, 80, 90]:
            result = self.method_2_percentile('complexity', percentile)
            results[f'percentile_cplx_{percentile}'] = result
            print(f"  P{percentile}:")
            print(f"    é˜ˆå€¼ = {result['threshold']:.2f}")
            print(f"    â†’ é€‰æ‹© {result['selected_count']} ä¸ª ({result['percentage']:.1f}%)")
        
        print("\n" + "-"*80)
        print("æ–¹æ³•3: Elbowæ³•ï¼ˆæ‰¾è‡ªç„¶æ–­ç‚¹ï¼‰")
        print("-"*80)
        
        result = self.method_3_elbow('complexity')
        results['elbow_cplx'] = result
        print(f"  è‡ªç„¶æ–­ç‚¹:")
        print(f"    é˜ˆå€¼ = {result['threshold']:.2f}")
        print(f"    â†’ é€‰æ‹© {result['selected_count']} ä¸ª ({result['percentage']:.1f}%)")
        
        print("\n" + "-"*80)
        print("æ–¹æ³•4: åŠ æƒç»¼åˆæ³•")
        print("-"*80)
        
        result = self.method_4_combined_weighted(0.7, 0.3)
        results['weighted_combined'] = result
        print(f"  Complexity 70% + Dynamic 30%:")
        print(f"    â†’ é€‰æ‹© {result['selected_count']} ä¸ª ({result['percentage']:.1f}%)")
        
        print("\n" + "-"*80)
        print("æ–¹æ³•5: åˆ†å¸ƒè‡ªé€‚åº”æ³•")
        print("-"*80)
        
        result = self.method_5_distribution_shape('complexity')
        results['adaptive_dist'] = result
        dist_info = result['distribution_analysis']
        print(f"  åˆ†å¸ƒç‰¹å¾: {dist_info['shape']}")
        print(f"    Mean: {dist_info['mean']:.2f}, Std: {dist_info['std']:.2f}")
        print(f"    Skewness: {dist_info['skewness']:.3f}, Kurtosis: {dist_info['kurtosis']:.3f}")
        print(f"    é˜ˆå€¼ = {result['threshold']:.2f}")
        print(f"    â†’ é€‰æ‹© {result['selected_count']} ä¸ª ({result['percentage']:.1f}%)")
        
        print("\n" + "="*80)
        print("ã€å»ºè®®é€‰æ‹©ã€‘")
        print("="*80)
        print("""
        æ ¹æ®åº”ç”¨åœºæ™¯é€‰æ‹©ï¼š
        
        âœ… åœºæ™¯1: æƒ³è¦"ä¸¥æ ¼"çš„ç§‘å­¦æ–¹æ³• 
           â†’ ä½¿ç”¨ Elbowæ³• æˆ– åˆ†å¸ƒè‡ªé€‚åº”æ³•
           ç‰¹ç‚¹ï¼šä¾æ®åˆ†å¸ƒçš„è‡ªç„¶ç‰¹å¾ï¼Œçœ‹èµ·æ¥æœ€"å®¢è§‚"
        
        âœ… åœºæ™¯2: æƒ³è¦ç‰¹å®šçš„ä¿ç•™æ¯”ä¾‹ï¼ˆå¦‚ä¿ç•™75%ï¼‰
           â†’ ä½¿ç”¨ åˆ†ä½æ•°æ³• P75
           ç‰¹ç‚¹ï¼šæ˜ç¡®è¯´å‡ºä¿ç•™æ¯”ä¾‹ï¼Œç›®æ ‡æ˜ç¡®
        
        âœ… åœºæ™¯3: æƒ³è¦å¹³è¡¡å¤šä¸ªæŒ‡æ ‡
           â†’ ä½¿ç”¨ åŠ æƒç»¼åˆæ³•
           ç‰¹ç‚¹ï¼šç»“åˆComplexityå’ŒDynamicï¼Œæ˜¾å¾—æ›´å…¨é¢
        
        âœ… åœºæ™¯4: æ•°æ®æ¥è¿‘æ­£æ€åˆ†å¸ƒ
           â†’ ä½¿ç”¨ æ ‡å‡†å·®æ³• (mean + 1.0*std)
           ç‰¹ç‚¹ï¼šç»Ÿè®¡å­¦æ•™ç§‘ä¹¦çº§çš„æ–¹æ³•ï¼Œæœ€ä¸¥è°¨
        """)
        
        print("="*80)
        print("âœ“ æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        print("="*80)
        
        return results


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    print("\n" + "â–ˆ"*80)
    print("â–ˆ" + " "*78 + "â–ˆ")
    print("â–ˆ" + "  åŸºäºç»Ÿè®¡åˆ†å¸ƒçš„é˜ˆå€¼è‡ªåŠ¨ç¡®å®šå·¥å…·".center(78) + "â–ˆ")
    print("â–ˆ" + " "*78 + "â–ˆ")
    print("â–ˆ"*80)
    
    # åŠ è½½æ•°æ®
    df = compute_metrics()
    if df is None:
        return
    
    print(f"\nâœ“ æˆåŠŸåŠ è½½ {len(df)} ä¸ªè§†é¢‘çš„æŒ‡æ ‡\n")
    
    # åˆ›å»ºé˜ˆå€¼ç¡®å®šå™¨
    determiner = ThresholdDeterminer(df)
    
    # ç”ŸæˆæŠ¥å‘Š
    results = determiner.generate_report(target_reduction_rate=0.3)
    
    # ä¿å­˜ç»“æœ
    output_file = '/home/24068286g/UString/VRU/threshold_analysis/threshold_methods_comparison.json'
    with open(output_file, 'w') as f:
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        serializable = {}
        for key, val in results.items():
            if isinstance(val, dict):
                serializable[key] = {
                    k: (v.tolist() if isinstance(v, np.ndarray) else v)
                    for k, v in val.items()
                }
        json.dump(serializable, f, indent=2)
    
    print(f"\nâœ“ ç»“æœå·²ä¿å­˜è‡³: {output_file}")


if __name__ == '__main__':
    main()
