{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f72ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 1: Install and Import Dependencies\n",
    "# ============================================================================\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_if_needed(package, import_name=None):\n",
    "    \"\"\"Install package if not available\"\"\"\n",
    "    import_name = import_name or package\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
    "\n",
    "# Install required packages\n",
    "install_if_needed(\"sentence-transformers\", \"sentence_transformers\")\n",
    "install_if_needed(\"pandas\")\n",
    "install_if_needed(\"numpy\")\n",
    "install_if_needed(\"matplotlib\")\n",
    "install_if_needed(\"seaborn\")\n",
    "install_if_needed(\"scipy\")\n",
    "\n",
    "print(\"‚úÖ All dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcca26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 2: Import Libraries\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39d55fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 3: Load BERT Model (Using lightweight all-MiniLM-L6-v2 for speed)\n",
    "# ============================================================================\n",
    "print(\"Loading Sentence-BERT model...\")\n",
    "print(\"Using 'all-MiniLM-L6-v2' - fast and efficient (80MB, 5x faster than BERT-base)\")\n",
    "\n",
    "# all-MiniLM-L6-v2 is optimized for semantic similarity tasks\n",
    "# It's ~5x faster than BERT-base while maintaining good quality\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {model.get_sentence_embedding_dimension()}-dim embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbeeb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 4: Define Data Paths\n",
    "# ============================================================================\n",
    "DATA_DIR = Path(\"/home/24068286g/UString/VRU/src/option_generate/data\")\n",
    "\n",
    "# Three versions of VQA datasets\n",
    "csv_files = {\n",
    "    \"3_options\": DATA_DIR / \"QA_pair_v1_3options.csv\",\n",
    "    \"4_options\": DATA_DIR / \"QA_pair_v2_4options.csv\",\n",
    "    \"5_options\": DATA_DIR / \"QA_pair_v3_5options.csv\"\n",
    "}\n",
    "\n",
    "# Check file existence\n",
    "for name, path in csv_files.items():\n",
    "    status = \"‚úÖ\" if path.exists() else \"‚ùå\"\n",
    "    print(f\"{status} {name}: {path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987bfcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 5: Load and Parse CSV Data\n",
    "# ============================================================================\n",
    "def load_qa_data(csv_path: Path, num_options: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load QA data from CSV and extract questions with their options.\n",
    "    \n",
    "    Returns DataFrame with columns:\n",
    "    - video_number, q_id, question, correct_answer, wrong_options (list)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    records = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        video_number = row['video_number']\n",
    "        \n",
    "        # Each row can have up to 6 questions (q1-q6)\n",
    "        for q_idx in range(1, 7):\n",
    "            q_text = row.get(f'q{q_idx}_text', None)\n",
    "            if pd.isna(q_text) or not q_text:\n",
    "                continue\n",
    "            \n",
    "            correct_answer = row.get(f'q{q_idx}_ans_correct', None)\n",
    "            if pd.isna(correct_answer):\n",
    "                continue\n",
    "            \n",
    "            # Collect wrong options\n",
    "            wrong_options = []\n",
    "            for w_idx in range(1, num_options):  # num_options-1 wrong answers\n",
    "                wrong_key = f'q{q_idx}_ans_wrong{w_idx}'\n",
    "                wrong_ans = row.get(wrong_key, None)\n",
    "                if not pd.isna(wrong_ans) and wrong_ans:\n",
    "                    wrong_options.append(str(wrong_ans).strip())\n",
    "            \n",
    "            if wrong_options:  # Only add if we have wrong options\n",
    "                records.append({\n",
    "                    'video_number': video_number,\n",
    "                    'q_id': q_idx,\n",
    "                    'question': str(q_text).strip(),\n",
    "                    'correct_answer': str(correct_answer).strip(),\n",
    "                    'wrong_options': wrong_options,\n",
    "                    'num_options': len(wrong_options) + 1  # Total options including correct\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Load all three datasets\n",
    "datasets = {}\n",
    "for name, path in csv_files.items():\n",
    "    if path.exists():\n",
    "        num_opts = int(name.split('_')[0])\n",
    "        datasets[name] = load_qa_data(path, num_opts)\n",
    "        print(f\"üìä {name}: {len(datasets[name])} questions loaded\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total datasets loaded: {len(datasets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d458211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 6: BERT-based Similarity Functions\n",
    "# ============================================================================\n",
    "def compute_bert_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two texts using BERT embeddings.\n",
    "    Returns value in [0, 1] where 1 means identical.\n",
    "    \"\"\"\n",
    "    embeddings = model.encode([text1, text2], convert_to_numpy=True)\n",
    "    similarity = 1 - cosine(embeddings[0], embeddings[1])\n",
    "    return max(0, similarity)  # Ensure non-negative\n",
    "\n",
    "def compute_batch_embeddings(texts: List[str]) -> np.ndarray:\n",
    "    \"\"\"Compute embeddings for a batch of texts (much faster)\"\"\"\n",
    "    return model.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n",
    "\n",
    "def cosine_similarity_matrix(embeddings: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute pairwise cosine similarity matrix\"\"\"\n",
    "    # Normalize embeddings\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    normalized = embeddings / norms\n",
    "    # Compute similarity matrix\n",
    "    return np.dot(normalized, normalized.T)\n",
    "\n",
    "# Test the functions\n",
    "test_sim = compute_bert_similarity(\n",
    "    \"The car crashed into the pedestrian\",\n",
    "    \"A vehicle hit a person walking\"\n",
    ")\n",
    "print(f\"Test similarity (semantically similar texts): {test_sim:.4f}\")\n",
    "\n",
    "test_sim2 = compute_bert_similarity(\n",
    "    \"The car crashed into the pedestrian\",\n",
    "    \"The weather is sunny today\"\n",
    ")\n",
    "print(f\"Test similarity (unrelated texts): {test_sim2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae4b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 7: Quality Metrics Computation\n",
    "# ============================================================================\n",
    "def analyze_question_quality(row: pd.Series) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze quality metrics for a single question.\n",
    "    \n",
    "    Returns:\n",
    "    - option_diversity: Average pairwise distance among wrong options (higher = more diverse)\n",
    "    - distractor_quality: Average distance from correct answer (ideal: medium, not too similar/different)\n",
    "    - question_relevance: Average relevance of options to the question\n",
    "    - separability: Distance between correct answer and closest wrong option\n",
    "    \"\"\"\n",
    "    question = row['question']\n",
    "    correct = row['correct_answer']\n",
    "    wrong_opts = row['wrong_options']\n",
    "    \n",
    "    # Get all embeddings in one batch for efficiency\n",
    "    all_texts = [question, correct] + wrong_opts\n",
    "    embeddings = compute_batch_embeddings(all_texts)\n",
    "    \n",
    "    q_emb = embeddings[0]\n",
    "    correct_emb = embeddings[1]\n",
    "    wrong_embs = embeddings[2:]\n",
    "    \n",
    "    # 1. Option Diversity: Pairwise similarity among wrong options\n",
    "    if len(wrong_embs) > 1:\n",
    "        wrong_sim_matrix = cosine_similarity_matrix(wrong_embs)\n",
    "        # Get upper triangle (excluding diagonal)\n",
    "        upper_tri = wrong_sim_matrix[np.triu_indices(len(wrong_embs), k=1)]\n",
    "        # Diversity = 1 - average similarity\n",
    "        option_diversity = 1 - np.mean(upper_tri)\n",
    "    else:\n",
    "        option_diversity = 0.5  # Default for single wrong option\n",
    "    \n",
    "    # 2. Distractor Quality: Similarity to correct answer\n",
    "    correct_norm = correct_emb / np.linalg.norm(correct_emb)\n",
    "    wrong_norms = wrong_embs / np.linalg.norm(wrong_embs, axis=1, keepdims=True)\n",
    "    distractor_sims = np.dot(wrong_norms, correct_norm)\n",
    "    avg_distractor_sim = np.mean(distractor_sims)\n",
    "    # Ideal distractor: not too similar (>0.8) or too different (<0.2)\n",
    "    # Best range: 0.3-0.6\n",
    "    distractor_quality = 1 - abs(avg_distractor_sim - 0.45) * 2  # Peak at 0.45\n",
    "    \n",
    "    # 3. Question Relevance: How relevant are options to the question\n",
    "    q_norm = q_emb / np.linalg.norm(q_emb)\n",
    "    all_opt_embs = np.vstack([correct_emb, wrong_embs])\n",
    "    all_opt_norms = all_opt_embs / np.linalg.norm(all_opt_embs, axis=1, keepdims=True)\n",
    "    relevance_scores = np.dot(all_opt_norms, q_norm)\n",
    "    question_relevance = np.mean(relevance_scores)\n",
    "    \n",
    "    # 4. Separability: Distance between correct and closest wrong option\n",
    "    min_distractor_sim = np.max(distractor_sims)  # Closest wrong option\n",
    "    separability = 1 - min_distractor_sim  # Higher = more separable\n",
    "    \n",
    "    return {\n",
    "        'option_diversity': float(option_diversity),\n",
    "        'distractor_quality': float(distractor_quality),\n",
    "        'question_relevance': float(question_relevance),\n",
    "        'separability': float(separability),\n",
    "        'avg_distractor_similarity': float(avg_distractor_sim),\n",
    "        'min_distractor_distance': float(1 - min_distractor_sim)\n",
    "    }\n",
    "\n",
    "# Test with first question from 3_options dataset\n",
    "if '3_options' in datasets and len(datasets['3_options']) > 0:\n",
    "    test_row = datasets['3_options'].iloc[0]\n",
    "    print(f\"Question: {test_row['question'][:80]}...\")\n",
    "    print(f\"Correct: {test_row['correct_answer'][:50]}...\")\n",
    "    print(f\"Wrong options: {len(test_row['wrong_options'])}\")\n",
    "    \n",
    "    metrics = analyze_question_quality(test_row)\n",
    "    print(\"\\nüìä Quality Metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59026717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 8: Batch Analysis for All Datasets\n",
    "# ============================================================================\n",
    "from tqdm import tqdm\n",
    "\n",
    "def analyze_dataset(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze all questions in a dataset and return metrics DataFrame.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîÑ Analyzing {name} ({len(df)} questions)...\")\n",
    "    \n",
    "    results = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {name}\"):\n",
    "        try:\n",
    "            metrics = analyze_question_quality(row)\n",
    "            metrics['video_number'] = row['video_number']\n",
    "            metrics['q_id'] = row['q_id']\n",
    "            metrics['num_options'] = row['num_options']\n",
    "            metrics['dataset'] = name\n",
    "            results.append(metrics)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Analyze all datasets\n",
    "all_results = []\n",
    "for name, df in datasets.items():\n",
    "    result_df = analyze_dataset(df, name)\n",
    "    all_results.append(result_df)\n",
    "    print(f\"‚úÖ {name}: {len(result_df)} questions analyzed\")\n",
    "\n",
    "# Combine all results\n",
    "combined_results = pd.concat(all_results, ignore_index=True)\n",
    "print(f\"\\nüìä Total analyzed: {len(combined_results)} question instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27ab312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 9: Statistical Comparison\n",
    "# ============================================================================\n",
    "def compute_statistics(combined_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute summary statistics for each dataset version.\n",
    "    \"\"\"\n",
    "    metrics = ['option_diversity', 'distractor_quality', 'question_relevance', \n",
    "               'separability', 'avg_distractor_similarity']\n",
    "    \n",
    "    stats_data = []\n",
    "    \n",
    "    for dataset_name in combined_df['dataset'].unique():\n",
    "        subset = combined_df[combined_df['dataset'] == dataset_name]\n",
    "        \n",
    "        row = {'dataset': dataset_name}\n",
    "        for metric in metrics:\n",
    "            values = subset[metric]\n",
    "            row[f'{metric}_mean'] = values.mean()\n",
    "            row[f'{metric}_std'] = values.std()\n",
    "            row[f'{metric}_median'] = values.median()\n",
    "        \n",
    "        row['count'] = len(subset)\n",
    "        stats_data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(stats_data)\n",
    "\n",
    "summary_stats = compute_statistics(combined_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä BERT-based Quality Analysis Summary\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display key metrics\n",
    "display_cols = ['dataset', 'count', \n",
    "                'option_diversity_mean', 'distractor_quality_mean', \n",
    "                'question_relevance_mean', 'separability_mean']\n",
    "\n",
    "print(\"\\nüìà Mean Quality Scores by Dataset:\")\n",
    "print(summary_stats[display_cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efe9f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 10: Statistical Significance Tests\n",
    "# ============================================================================\n",
    "def perform_statistical_tests(combined_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Perform statistical tests to compare dataset versions.\n",
    "    Uses Kruskal-Wallis H-test (non-parametric) and pairwise Mann-Whitney U tests.\n",
    "    \"\"\"\n",
    "    metrics = ['option_diversity', 'distractor_quality', 'question_relevance', 'separability']\n",
    "    datasets_list = sorted(combined_df['dataset'].unique())\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for metric in metrics:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìä Statistical Tests for: {metric}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Prepare data groups\n",
    "        groups = [combined_df[combined_df['dataset'] == d][metric].values for d in datasets_list]\n",
    "        \n",
    "        # Kruskal-Wallis H-test (non-parametric ANOVA)\n",
    "        h_stat, p_value = stats.kruskal(*groups)\n",
    "        print(f\"\\nKruskal-Wallis H-test:\")\n",
    "        print(f\"  H-statistic: {h_stat:.4f}\")\n",
    "        print(f\"  p-value: {p_value:.6f}\")\n",
    "        print(f\"  Significant (p<0.05): {'‚úÖ YES' if p_value < 0.05 else '‚ùå NO'}\")\n",
    "        \n",
    "        # Pairwise Mann-Whitney U tests\n",
    "        print(f\"\\nPairwise Mann-Whitney U tests:\")\n",
    "        pairwise_results = {}\n",
    "        for i, d1 in enumerate(datasets_list):\n",
    "            for j, d2 in enumerate(datasets_list):\n",
    "                if i < j:\n",
    "                    g1 = combined_df[combined_df['dataset'] == d1][metric].values\n",
    "                    g2 = combined_df[combined_df['dataset'] == d2][metric].values\n",
    "                    u_stat, p_val = stats.mannwhitneyu(g1, g2, alternative='two-sided')\n",
    "                    \n",
    "                    # Effect size (rank-biserial correlation)\n",
    "                    n1, n2 = len(g1), len(g2)\n",
    "                    effect_size = 1 - (2*u_stat)/(n1*n2)\n",
    "                    \n",
    "                    sig = '‚úÖ' if p_val < 0.05 else ''\n",
    "                    print(f\"  {d1} vs {d2}: U={u_stat:.0f}, p={p_val:.4f} {sig}, effect={effect_size:.3f}\")\n",
    "                    pairwise_results[f\"{d1}_vs_{d2}\"] = {\n",
    "                        'u_statistic': u_stat,\n",
    "                        'p_value': p_val,\n",
    "                        'effect_size': effect_size,\n",
    "                        'significant': p_val < 0.05\n",
    "                    }\n",
    "        \n",
    "        results[metric] = {\n",
    "            'kruskal_wallis': {'h_statistic': h_stat, 'p_value': p_value},\n",
    "            'pairwise': pairwise_results\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "stat_test_results = perform_statistical_tests(combined_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be49b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 11: Visualization - Box Plots\n",
    "# ============================================================================\n",
    "def plot_quality_comparison(combined_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization of quality metrics.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('VQA Dataset Quality Comparison (BERT-based Analysis)', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    metrics = [\n",
    "        ('option_diversity', 'Option Diversity\\n(Higher = More diverse distractors)', axes[0, 0]),\n",
    "        ('distractor_quality', 'Distractor Quality\\n(Optimal: ~0.45 similarity)', axes[0, 1]),\n",
    "        ('question_relevance', 'Question-Option Relevance\\n(Higher = More topically related)', axes[1, 0]),\n",
    "        ('separability', 'Answer Separability\\n(Higher = Easier to distinguish)', axes[1, 1])\n",
    "    ]\n",
    "    \n",
    "    colors = {'3_options': '#3498db', '4_options': '#2ecc71', '5_options': '#e74c3c'}\n",
    "    \n",
    "    for metric, title, ax in metrics:\n",
    "        # Box plot\n",
    "        bp = ax.boxplot(\n",
    "            [combined_df[combined_df['dataset'] == d][metric].values \n",
    "             for d in sorted(combined_df['dataset'].unique())],\n",
    "            labels=['3 Options', '4 Options', '5 Options'],\n",
    "            patch_artist=True\n",
    "        )\n",
    "        \n",
    "        # Color the boxes\n",
    "        for patch, color in zip(bp['boxes'], colors.values()):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        \n",
    "        ax.set_title(title, fontsize=11, fontweight='bold')\n",
    "        ax.set_ylabel('Score', fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add mean markers\n",
    "        means = [combined_df[combined_df['dataset'] == d][metric].mean() \n",
    "                 for d in sorted(combined_df['dataset'].unique())]\n",
    "        ax.scatter([1, 2, 3], means, color='red', marker='D', s=50, zorder=3, label='Mean')\n",
    "        \n",
    "        # Add mean values as text\n",
    "        for i, m in enumerate(means):\n",
    "            ax.annotate(f'{m:.3f}', (i+1, m), textcoords=\"offset points\", \n",
    "                       xytext=(10, 5), fontsize=9, color='red')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    output_path = DATA_DIR / \"bert_quality_analysis_boxplot.png\"\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\n‚úÖ Figure saved: {output_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_quality_comparison(combined_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c434d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 12: Visualization - Distribution Plots\n",
    "# ============================================================================\n",
    "def plot_distributions(combined_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Plot kernel density estimates for each metric.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Quality Metric Distributions by Option Count (BERT-based)', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    metrics = [\n",
    "        ('option_diversity', 'Option Diversity', axes[0, 0]),\n",
    "        ('distractor_quality', 'Distractor Quality', axes[0, 1]),\n",
    "        ('question_relevance', 'Question-Option Relevance', axes[1, 0]),\n",
    "        ('avg_distractor_similarity', 'Avg Distractor-Correct Similarity', axes[1, 1])\n",
    "    ]\n",
    "    \n",
    "    colors = {'3_options': '#3498db', '4_options': '#2ecc71', '5_options': '#e74c3c'}\n",
    "    labels = {'3_options': '3 Options', '4_options': '4 Options', '5_options': '5 Options'}\n",
    "    \n",
    "    for metric, title, ax in metrics:\n",
    "        for dataset_name in sorted(combined_df['dataset'].unique()):\n",
    "            data = combined_df[combined_df['dataset'] == dataset_name][metric]\n",
    "            sns.kdeplot(data, ax=ax, label=labels[dataset_name], \n",
    "                       color=colors[dataset_name], linewidth=2, fill=True, alpha=0.3)\n",
    "        \n",
    "        ax.set_title(title, fontsize=11, fontweight='bold')\n",
    "        ax.set_xlabel('Score', fontsize=10)\n",
    "        ax.set_ylabel('Density', fontsize=10)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    output_path = DATA_DIR / \"bert_quality_analysis_kde.png\"\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\n‚úÖ Figure saved: {output_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_distributions(combined_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d728295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 13: Radar Chart Comparison\n",
    "# ============================================================================\n",
    "def plot_radar_comparison(summary_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create radar chart comparing all datasets across metrics.\n",
    "    \"\"\"\n",
    "    metrics = ['option_diversity_mean', 'distractor_quality_mean', \n",
    "               'question_relevance_mean', 'separability_mean']\n",
    "    metric_labels = ['Option\\nDiversity', 'Distractor\\nQuality', \n",
    "                    'Question\\nRelevance', 'Separability']\n",
    "    \n",
    "    # Number of metrics\n",
    "    N = len(metrics)\n",
    "    \n",
    "    # Compute angle for each metric\n",
    "    angles = np.linspace(0, 2*np.pi, N, endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Complete the loop\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    colors = {'3_options': '#3498db', '4_options': '#2ecc71', '5_options': '#e74c3c'}\n",
    "    labels = {'3_options': '3 Options', '4_options': '4 Options', '5_options': '5 Options'}\n",
    "    \n",
    "    for _, row in summary_df.iterrows():\n",
    "        dataset = row['dataset']\n",
    "        values = [row[m] for m in metrics]\n",
    "        values += values[:1]  # Complete the loop\n",
    "        \n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=labels[dataset], \n",
    "                color=colors[dataset])\n",
    "        ax.fill(angles, values, alpha=0.25, color=colors[dataset])\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metric_labels, fontsize=10)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('VQA Dataset Quality Comparison\\n(BERT-based Metrics)', \n",
    "                 fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    output_path = DATA_DIR / \"bert_quality_analysis_radar.png\"\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\n‚úÖ Figure saved: {output_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_radar_comparison(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb8dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 14: Composite Quality Score\n",
    "# ============================================================================\n",
    "def compute_composite_score(combined_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute a composite quality score combining all metrics.\n",
    "    \n",
    "    Weights:\n",
    "    - Option Diversity: 0.25 (important for challenging distractors)\n",
    "    - Distractor Quality: 0.30 (most important - right difficulty level)\n",
    "    - Question Relevance: 0.20 (options should be topically relevant)\n",
    "    - Separability: 0.25 (correct answer should be distinguishable)\n",
    "    \"\"\"\n",
    "    weights = {\n",
    "        'option_diversity': 0.25,\n",
    "        'distractor_quality': 0.30,\n",
    "        'question_relevance': 0.20,\n",
    "        'separability': 0.25\n",
    "    }\n",
    "    \n",
    "    combined_df['composite_score'] = (\n",
    "        combined_df['option_diversity'] * weights['option_diversity'] +\n",
    "        combined_df['distractor_quality'] * weights['distractor_quality'] +\n",
    "        combined_df['question_relevance'] * weights['question_relevance'] +\n",
    "        combined_df['separability'] * weights['separability']\n",
    "    )\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "combined_results = compute_composite_score(combined_results)\n",
    "\n",
    "# Summary by dataset\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä COMPOSITE QUALITY SCORE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nWeights: Diversity=0.25, Distractor Quality=0.30, Relevance=0.20, Separability=0.25\")\n",
    "print(\"\\n\")\n",
    "\n",
    "composite_summary = combined_results.groupby('dataset')['composite_score'].agg(['mean', 'std', 'median'])\n",
    "composite_summary = composite_summary.round(4)\n",
    "print(composite_summary)\n",
    "\n",
    "# Best dataset\n",
    "best_dataset = composite_summary['mean'].idxmax()\n",
    "print(f\"\\nüèÜ Best Overall Quality: {best_dataset} (mean composite score: {composite_summary.loc[best_dataset, 'mean']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d082f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 15: Final Comparison Bar Chart\n",
    "# ============================================================================\n",
    "def plot_final_comparison(combined_df: pd.DataFrame, summary_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create final comparison bar chart with composite scores.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Left: Composite score comparison\n",
    "    ax = axes[0]\n",
    "    composite_means = combined_df.groupby('dataset')['composite_score'].mean().sort_index()\n",
    "    composite_stds = combined_df.groupby('dataset')['composite_score'].std().sort_index()\n",
    "    \n",
    "    colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "    x_labels = ['3 Options', '4 Options', '5 Options']\n",
    "    \n",
    "    bars = ax.bar(x_labels, composite_means.values, yerr=composite_stds.values,\n",
    "                  color=colors, alpha=0.8, capsize=5, edgecolor='black')\n",
    "    \n",
    "    ax.set_ylabel('Composite Quality Score', fontsize=11)\n",
    "    ax.set_title('Overall VQA Quality by Option Count\\n(BERT-based Analysis)', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, composite_means.values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{val:.3f}', ha='center', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Right: Individual metrics comparison\n",
    "    ax = axes[1]\n",
    "    metrics = ['option_diversity_mean', 'distractor_quality_mean', \n",
    "               'question_relevance_mean', 'separability_mean']\n",
    "    metric_short = ['Diversity', 'Distractor\\nQuality', 'Relevance', 'Separability']\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, (dataset, color) in enumerate(zip(['3_options', '4_options', '5_options'], colors)):\n",
    "        row = summary_df[summary_df['dataset'] == dataset].iloc[0]\n",
    "        values = [row[m] for m in metrics]\n",
    "        ax.bar(x + (i-1)*width, values, width, label=f'{dataset.split(\"_\")[0]} Options', \n",
    "               color=color, alpha=0.8)\n",
    "    \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metric_short, fontsize=10)\n",
    "    ax.set_ylabel('Score', fontsize=11)\n",
    "    ax.set_title('Individual Metric Comparison', fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    output_path = DATA_DIR / \"bert_quality_analysis_final.png\"\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\n‚úÖ Figure saved: {output_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_final_comparison(combined_results, summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe84737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 16: Save Results to CSV\n",
    "# ============================================================================\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Save detailed results\n",
    "detail_path = DATA_DIR / f\"bert_analysis_detailed_{timestamp}.csv\"\n",
    "combined_results.to_csv(detail_path, index=False)\n",
    "print(f\"‚úÖ Detailed results saved: {detail_path}\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_path = DATA_DIR / f\"bert_analysis_summary_{timestamp}.csv\"\n",
    "summary_stats.to_csv(summary_path, index=False)\n",
    "print(f\"‚úÖ Summary statistics saved: {summary_path}\")\n",
    "\n",
    "# Create a comprehensive report\n",
    "report_path = DATA_DIR / f\"bert_analysis_report_{timestamp}.txt\"\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(\"=\" * 80 + \"\\n\")\n",
    "    f.write(\"VQA Dataset Quality Analysis Report (BERT-based)\\n\")\n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Method\\n\")\n",
    "    f.write(\"Using Sentence-BERT (all-MiniLM-L6-v2) for semantic similarity analysis.\\n\")\n",
    "    f.write(\"This provides deeper semantic understanding compared to TF-IDF.\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Metrics Explained\\n\")\n",
    "    f.write(\"1. Option Diversity: Semantic variety among wrong options (higher = better)\\n\")\n",
    "    f.write(\"2. Distractor Quality: Optimal similarity to correct answer (~0.45 is ideal)\\n\")\n",
    "    f.write(\"3. Question Relevance: How topically related options are to the question\\n\")\n",
    "    f.write(\"4. Separability: How distinguishable the correct answer is\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Summary Statistics\\n\")\n",
    "    f.write(summary_stats.to_string() + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Composite Score Rankings\\n\")\n",
    "    composite_summary = combined_results.groupby('dataset')['composite_score'].agg(['mean', 'std'])\n",
    "    f.write(composite_summary.sort_values('mean', ascending=False).to_string() + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"## Conclusion\\n\")\n",
    "    best = composite_summary['mean'].idxmax()\n",
    "    f.write(f\"Based on BERT-based semantic analysis, '{best}' shows the best overall quality.\\n\")\n",
    "\n",
    "print(f\"‚úÖ Report saved: {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c195d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 17: Summary and Conclusions\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã ANALYSIS COMPLETE - BERT-based VQA Quality Assessment\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä Key Findings:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Find best for each metric\n",
    "for metric in ['option_diversity_mean', 'distractor_quality_mean', \n",
    "               'question_relevance_mean', 'separability_mean']:\n",
    "    best = summary_stats.loc[summary_stats[metric].idxmax(), 'dataset']\n",
    "    value = summary_stats[metric].max()\n",
    "    metric_name = metric.replace('_mean', '').replace('_', ' ').title()\n",
    "    print(f\"‚Ä¢ Best {metric_name}: {best} ({value:.4f})\")\n",
    "\n",
    "print(\"\\nüèÜ Overall Quality Ranking (Composite Score):\")\n",
    "print(\"-\" * 40)\n",
    "composite_ranking = combined_results.groupby('dataset')['composite_score'].mean().sort_values(ascending=False)\n",
    "for i, (dataset, score) in enumerate(composite_ranking.items(), 1):\n",
    "    medal = ['ü•á', 'ü•à', 'ü•â'][i-1] if i <= 3 else f'{i}.'\n",
    "    print(f\"{medal} {dataset}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"‚Ä¢ Higher option diversity = more challenging distractors\")\n",
    "print(\"‚Ä¢ Optimal distractor quality (0.45) = similar enough to be confusing, different enough to be wrong\")\n",
    "print(\"‚Ä¢ Higher question relevance = options are on-topic\")\n",
    "print(\"‚Ä¢ Higher separability = correct answer is more distinguishable\")\n",
    "\n",
    "print(\"\\n‚úÖ BERT-based analysis provides deeper semantic understanding than TF-IDF\")\n",
    "print(\"   by capturing contextual meaning and handling synonyms/paraphrases better.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb177e0",
   "metadata": {},
   "source": [
    "# Part B: Video-Text Similarity Analysis using CLIP\n",
    "\n",
    "## Ë∑®Ê®°ÊÄÅÁõ∏‰ººÂ∫¶ËØÑ‰º∞\n",
    "\n",
    "‰ΩøÁî® OpenAI ÁöÑ CLIP Ê®°ÂûãËÆ°ÁÆóËßÜÈ¢ëÂ∏ß‰∏éÈÄâÈ°πÊñáÊú¨‰πãÈó¥ÁöÑËØ≠‰πâÁõ∏‰ººÂ∫¶„ÄÇ\n",
    "\n",
    "**ËØÑ‰º∞ÊåáÊ†áÔºö**\n",
    "1. **Correct Answer Alignment** - Ê≠£Á°ÆÁ≠îÊ°à‰∏éËßÜÈ¢ëÂÜÖÂÆπÁöÑÂåπÈÖçÁ®ãÂ∫¶\n",
    "2. **Distractor Plausibility** - ÈîôËØØÈÄâÈ°π‰∏éËßÜÈ¢ëÁöÑÂêàÁêÜÁõ∏ÂÖ≥ÊÄß\n",
    "3. **Answer Discriminability** - Ê≠£Á°ÆÁ≠îÊ°àÊòØÂê¶ÊØîÈîôËØØÈÄâÈ°πÊõ¥ÂåπÈÖçËßÜÈ¢ë"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2a36bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 18: Install CLIP Dependencies\n",
    "# ============================================================================\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_clip_deps():\n",
    "    \"\"\"Install CLIP and related dependencies\"\"\"\n",
    "    packages = [\n",
    "        (\"transformers\", \"transformers\"),\n",
    "        (\"Pillow\", \"PIL\"),\n",
    "        (\"opencv-python\", \"cv2\"),\n",
    "    ]\n",
    "    \n",
    "    for package, import_name in packages:\n",
    "        try:\n",
    "            __import__(import_name)\n",
    "        except ImportError:\n",
    "            print(f\"Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
    "    \n",
    "    # Install CLIP from transformers (no need for separate clip package)\n",
    "    print(\"‚úÖ CLIP dependencies ready\")\n",
    "\n",
    "install_clip_deps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b567553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Part 19: Load CLIP Model\n",
    "# ============================================================================\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Use GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load CLIP model (ViT-B/32 is fast and efficient)\n",
    "print(\"Loading CLIP model (openai/clip-vit-base-patch32)...\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "print(f\"‚úÖ CLIP model loaded on {device}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
